# Open Questions

<!-- Format: - YYYY-MM-DD | [OPEN/RESOLVED] | Question | Context/Related papers -->

- 2026-02-23 | [OPEN] | Weis et al. の mixed pool メカニズムは IPD 以外の Social Dilemma（Stag Hunt, Chicken, Public Goods Game）でも協調を誘導するか？ | papers/multi-agent-cooperation/weis-2026/
- 2026-02-23 | [OPEN] | LLM ベースのエージェント（Transformer）で in-context co-player inference による協調は出現するか？GRU から Transformer へのスケーリングの効果は？ | papers/multi-agent-cooperation/weis-2026/
- 2026-02-23 | [OPEN] | A2C の Step 3 不安定性の原因は何か？PPI の安定性は Self-Supervised 訓練・累積データ・パラメータ再初期化のどれに起因するか？ | papers/multi-agent-cooperation/weis-2026/
- 2026-02-23 | [OPEN] | TTD-DR の Diffusion ベース反復改善は、計算コストに見合う品質向上をもたらすか？Denoising 回数と品質のトレードオフの最適点は？ | surveys/deep-research/
- 2026-02-23 | [OPEN] | RL で獲得される Deep Research エージェントの創発的行動（自己反省、クロスバリデーション）は、RLVR の限界（NeurIPS 2025: 推論能力拡張ではなくサンプリング効率向上に留まる）と矛盾しないか？ | surveys/deep-research/
- 2026-02-23 | [OPEN] | Long-Horizon 探索（ASearcher: 100+ターン、400k+トークン）の終了条件をどう設計すべきか？不要な探索を回避しつつ網羅性を確保する制御手法は？ | surveys/deep-research/
- 2026-02-23 | [OPEN] | Deep Research エージェントのレポート品質の統一的評価基準は確立可能か？事実性・引用正確性・構造の論理性・情報網羅性をどう統合するか？ | surveys/deep-research/
- 2026-02-23 | [OPEN] | Tongyi DeepResearch の 3段階訓練（Agentic CPT → SFT → RL）のうち、性能向上への寄与が最も大きいステージはどれか？ | surveys/deep-research/
- 2026-02-23 | [OPEN] | マルチモーダル Deep Research（WebWatcher 的アプローチ）は、テキスト中心のアプローチと比較してどの程度実用的な優位性を持つか？ | surveys/deep-research/
- 2026-02-23 | [OPEN] | Contract-first decomposition は主観的・創造的タスク（デザイン、文章作成等）でも収束するか？「検証可能な粒度」まで分解すると意味のある単位でなくなる可能性の検証が必要 | papers/agents/tomasev-2026/
- 2026-02-23 | [OPEN] | zk-SNARKs + Smart Contract + TEE を統合した delegation システムの計算オーバーヘッドはリアルタイム delegation に許容可能か？proof-of-concept による定量評価が必要 | papers/agents/tomasev-2026/
- 2026-02-23 | [OPEN] | Weis et al. の in-context co-player inference は、Tomašev et al. の Trust Calibration メカニズムの実装として機能しうるか？エージェント間の信頼を interaction history から in-context で推定する可能性 | papers/agents/tomasev-2026/, papers/multi-agent-cooperation/weis-2026/
