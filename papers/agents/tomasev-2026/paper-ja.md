_2026-02-12_

# **Intelligent AI Delegation**


**Nenad Tomašev** [1] **, Matija Franklin** [1] **and Simon Osindero** [1]

1Google DeepMind


**AIエージェントはますます複雑なタスクに取り組めるようになっている。** **より野心的な目標を達成するために、AIエージェントは問題を管理可能なサブコンポーネントへと意味のある形で分解し、他のAIエージェントや人間に対してその完了を安全に委任できる必要がある。** **しかし、既存のタスク分解および委任手法は単純なヒューリスティックに依拠しており、環境変化に動的に適応したり予期せぬ障害を堅牢に処理したりする能力を持たない。** **本稿では、** _**intelligent**_ _**AI**_ _**delegation**_（**インテリジェントなAI委任**）**のための適応的フレームワークを提案する。これは、タスク割り当てを含む一連の意思決定であり、権限・責任・説明責任の移譲、役割と境界に関する明確な仕様、意図の明確性、および二者（またはそれ以上）の間の信頼確立メカニズムも組み込む。** **提案するフレームワークは複雑な委任ネットワークにおける人間とAIの委任者・受任者の双方に適用可能であり、新興のアジェンティックWebにおけるプロトコル開発に示唆を与えることを目的としている。**


_Keywords:_ _AI, agents, LLM, delegation, multi-agent, safety_

_Corresponding author(s):_ _nenadt@google.com_ © 2026 Google. All rights reserved

### **1. Introduction**

高度なAIエージェントがquery-responseモデルを超えて進化するにつれ、その有用性はますます、複雑な目標を効果的に分解しサブタスクを委任する能力によって定義されるようになっている。この調整パラダイムは、AIエージェントが個人アシスタントとして機能しうる個人利用（Gabriel et al., 2024）から、AIエージェントがサポートを提供しワークフローを自動化する商業・エンタープライズ展開（Huang and Hughes, 2025; Shao et al., 2025; Tupe and Thube, 2025）に至るまで、幅広いアプリケーションを支えている。Large language models（LLMs）はすでに、より対話的かつ正確なゴール仕様とフィードバックを可能にすることで、ロボティクス（Li et al., 2025a; Wang et al., 2024a）における可能性を示している。また、仮想経済における大規模AIエージェント調整の可能性も最近の提案により明らかになっている（Tomasev et al., 2025）。現代のアジェンティックAIシステムは、集中型または分散型のオーケストレーションプロトコルと組み合わせた、差別化されたサブエージェントにわたる複雑な制御フローを実装している（Hong et al., 2023; Rasal and Hauer, 2024; Song et al., 2025; Zhang et al., 2025a）。これはすでに、タスク分解と委任のある種のミクロコスモスとして見ることができるが、そのプロセスはハードコードされており制約が強い。動的なWebスケールの相互作用を管理するには、現在より発見的なマルチエージェントフレームワークで採用されているアプローチを超えた思考が必要である。


委任（Castelfranchi and Falcone, 1998）は、単なる管理可能なサブ行動単位へのタスク分解以上のものである。サブタスクの作成を超えて、委任は責任と権限の割り当て（Mueller and Vogelsmeier, 2013; Nagia, 2024）を必要とし、したがって結果に対する説明責任を内包する。委任はリスク評価を伴い、これは信頼によって調整されうる（Griffiths, 2005）。さらに委任は、能力マッチングと継続的なパフォーマンス監視を含み、フィードバックに基づく動的な調整を取り込み、指定された制約のもとで分散タスクの完了を確保する。現在のアプローチはこれらの要因を考慮できない傾向があり、より多くはヒューリスティックおよび/またはより単純な並列化に依拠している。これは初期プロトタイプには十分かもしれないが、実世界のAI展開は場当たり的で脆弱かつ信頼性の低い委任を超える必要がある。変化に動的に適応し（Acharya et al., 2025; Hauptman et al., 2023）エラーから回復できるシステムへの切迫したニーズがある。適応的で堅牢な展開フレームワークの欠如は、高リスク環境におけるAIアプリケーションにとって主要な制限要因の一つであり続けている。


AIエージェントを十分に活用するには、_intelligent_ _delegation_（インテリジェントな委任）が必要である。これは、明確な役割・境界・評判・信頼・透明性・認証可能なアジェンティック能力・検証可能なタスク実行・スケーラブルなタスク分散を中心とした堅牢なフレームワークである。本稿では、これらの限界に対処することを目的とし、人間組織の歴史的洞察に基づき、主要なアジェンティック安全要件に根ざした、インテリジェントなタスク委任フレームワークを導入する。

### **2. Foundations of Intelligent Delegation**


**2.1.** **Definition**


_intelligent delegation_（インテリジェントな委任）を、タスク割り当てを含む一連の意思決定として定義する。これは、権限・責任・説明責任の移譲、役割と境界に関する明確な仕様、意図の明確性、および二者（またはそれ以上）の間の信頼確立メカニズムも組み込む。複雑なタスクはまた、タスク分解に関わるステップや、割り当て決定を通知するための慎重な能力ルックアップとマッチングを伴う場合がある。


タスク委任に言及するとき、私たちは通常、タスクがシステムサブルーチンによって処理されるような基本的な複雑さのレベルを超えていることを前提とする。そのような初歩的なアウトソーシングも注意を要するが、その範囲はより限定されている。スペクトラムのもう一方の端では、明示的なチェックや許可なしに任意の数のサブゴールを自由に追求できる完全な自律性が付与されたエージェントと契約することが可能かもしれない（Kasirzadeh and Gabriel, 2025）。極限ケースでは、そのような完全自律エージェントは道徳的決定を信頼される必要があるかもしれないが（Sloksnath, 2025）、現代のエージェントがそのような決定に関与する能力を著しく欠いていることから、それを許可することを私たちが選択することはないかもしれない（Haas, 2020; Mao et al., 2023; Reinecke et al., 2023）。私たちはそのようなオープンエンドのシナリオを議論の範囲内と考えるが、それはより自律的なタスク完了の安全性を確保するための適切なメカニズムが整備されうる限りにおいてのみである。



**2.2.** **Aspects of Delegation**


委任はさまざまな形をとりうることから、ここでは、これらのユースケースを文脈化し分析に適したものとするためのいくつかの軸を導入する。


1. **Delegator（委任者）。** 人間またはAI。
2. **Delegatee（受任者）。** 人間またはAI。
3. **タスク特性。**
    1. **Complexity（複雑性）。** タスクに固有の難易度であり、サブステップの数と必要な推論の洗練度と相関することが多い。
    2. **Criticality（重要性）。** タスクの重要度と、失敗または最適以下のパフォーマンスに関連する結果の深刻さの尺度。
    3. **Uncertainty（不確実性）。** 環境、入力、または成功した結果達成の確率に関する曖昧さのレベル。
    4. **Duration（期間）。** タスク実行の予想される時間枠であり、瞬時のサブルーティンから数日または数週間にわたる長時間プロセスまで及ぶ。
    5. **Cost（コスト）。** タスクを実行するために発生する経済的または計算上の費用であり、トークン使用量、API料金、エネルギー消費を含む。
    6. **Resource Requirements（リソース要件）。** タスクを完了するために必要な特定の計算資産、ツール、データアクセス権限、または人間の能力。
    7. **Constraints（制約）。** タスクが実行されなければならない運用上・倫理上・法的な境界であり、解空間を制限する。
    8. **Verifiability（検証可能性）。** タスクの結果を検証することに関連する相対的な難易度とコスト。検証可能性が高いタスク（例：形式的コード検証、数学的証明）は「トラストレス」な委任または自動チェックを可能にする。逆に、検証可能性が低いタスク（例：オープンエンドの研究）は高信頼の受任者または高コストで労働集約的な監視を必要とする。
    9. **Reversibility（可逆性）。** タスク実行の影響を取り消しうる程度。実世界に副作用をもたらす不可逆なタスク（例：金融取引の実行、データベースの削除、外部メールの送信）は、可逆なタスク（例：メールの下書き、データベースエントリへのフラグ付け）よりも厳格な _liability firebreaks_（責任防火壁）とより急峻な権限勾配を必要とする。
    10. **Contextuality（文脈性）。** タスクを効果的に実行するために必要な外部状態、履歴、または環境認識の量と機密性。高コンテキストなタスクはより大きなプライバシー表面積を導入するが、コンテキストフリーなタスクはより容易に区画化され低信頼ノードにアウトソースできる。
    11. **Subjectivity（主観性）。** 成功基準が嗜好の問題か客観的事実かの程度。高度に主観的なタスク（例：「説得力のあるロゴをデザインする」）は通常「Human-as-Value-Specifier」の介入と反復的なフィードバックループを必要とするが、客観的なタスクはより厳格でバイナリな契約によって管理できる。
4. **Granularity（粒度）。** リクエストは細粒度または粗粒度の目標を含みうる。粗粒度の場合、受任者はさらなるタスク分解を実行する必要があるかもしれない。
5. **Autonomy（自律性）。** タスク委任は、サブタスクの追求において完全な自律性を付与するリクエストを含むこともあれば、
より具体的かつ規範的なものである場合もある。
6. **Monitoring（監視）。** 委任されたタスクの監視は、継続的・定期的・イベントトリガー型のいずれかでありうる。
7. **Reciprocity（相互性）。** 委任は通常一方向のリクエストであるが、協調的なエージェントネットワークにおいて相互委任のケースが存在しうる。


委任者と受任者の軸から始めると、以下のシナリオが考えられる：1) 人間がAIエージェントに委任する、2) AIエージェントがAIエージェントに委任する、3) AIエージェントが人間に委任する（Ashton and Franklin, 2022; Guggenberger et al., 2023）。最初のケースは文献でおそらく最も議論されてきたが、他の二つも同様に検討する価値がある。さまざまなシステムにわたって展開されるAIエージェントの増加と、仮想アジェンティック市場・経済を設立するためのインフラの発展（Hadfield and Koh, 2025; Tomasev et al., 2025; Yang et al., 2025）により、将来的にはエージェント間の相互作用がはるかに多くなり、それらにはタスク委任も含まれるであろうことが明らかである。


エージェント間の委任は、ネットワーク内のエージェント間の関係とそれぞれの役割に応じて、階層的または非階層的のいずれかでありうる。階層的関係の例は、集合体内のサブエージェントにタスクを委任するオーケストレーターエージェントである。非階層的関係は、同等の立場を持つピアエージェントを含む。高度なAIエージェントはまた、特筆すべきエージェンシーなしに、スペシャリストMLモデルにタスクを委任することもできる。


AI-人間委任（Guggenberger et al., 2023）は有望なパラダイムであることが示されており（Hemmer et al., 2023）、認知バイアスとメタ認知の違いにより（Fügener et al., 2019）、超人的なシステムとの協調を成功させやすくする（Fügener et al., 2022）。Davidson and Hadshar（2025）は「AI指示による人間労働」の増加を予測しており、これは経済生産性を大幅に向上させる可能性がある。実際、現在のAI-人間委任には一連の問題が伴う。ライドヘイリングおよびロジスティクスにおけるアルゴリズム管理システムは、タスクを割り当て順序付けし、パフォーマンス指標を設定し、データドリブンな意思決定を通じて行動規範を執行することで、企業およびそのAIベースシステムから人間の労働者へと管理機能を事実上委任している（Beverungen, 2021; Lee et al., 2015; Rosenblat and Stark, 2016）。増大する文献は、これらのシステムが雇用の質の低下、ストレス、健康リスクと関連しており、現在のアルゴリズム管理の展開がしばしば労働者の福祉を向上させるどころか損なうことを示唆している（Ashton and Franklin, 2022; Goods et al., 2019; Vignola et al., 2023）。現在のAI-人間委任は、人間の福祉や長期的な社会的外部性を考慮していないため、さらなる改善が必要である。


**2.3.** **Delegation in Human Organizations**


委任は、人間の社会的・組織的構造における主要なメカニズムとして機能する。これらの人間のダイナミクスから得られた洞察は、AI委任フレームワークの設計の基盤を提供できる。


**The Principal-Agent Problem（プリンシパル-エージェント問題）。** _プリンシパル-エージェント問題_（Cvitanić et al., 2018; Ensminger, 2001; Grossman and Hart, 1992; Myerson, 1982; Sannikov, 2008; Shah, 2014; Sobel, 1993）は詳細に研究されてきた。これは、プリンシパルがプリンシパルと動機が一致していないエージェントにタスクを委任する際に生じる状況である。エージェントはしたがって自分自身の動機を優先し、情報を差し控え、元の意図を損なう方法で行動する可能性がある。AI委任においては、このダイナミクスがより複雑な性質を帯びる。現在のほとんどのAIエージェントは確かに隠れた議題 [1]

- ユーザーの指示に反して追求するゴールや価値観 - を持たないとも言えるが、それでも望ましくない方法で現れるAIアライメントの問題が存在しうる。例えば、報酬の誤指定は、設計者がAIシステムに不完全または不完全な目標を与える場合に発生し、報酬ハッキング（または仕様ゲーミング）とは、システムがその指定された報酬信号の抜け穴を利用して、設計者の意図を覆す方法で高い測定パフォーマンスを達成することを指す。これらは共に、述べられた報酬を最適化することが真のゴールから乖離するというアライメントの核心問題を示している（Amodei et al., 2016; Krakovna et al., 2020; Leike et al., 2017; Skalse and Mancosu, 2022）。このダイナミクスは、AIエージェントが異なる人間ユーザー・グループ・組織を代表して、または他のエージェントの代理として行動する可能性があり、関連する未知の目標を持つ、より自律的なAIエージェント経済においては完全に変わる可能性が高い。

:::note info
1 最近のdeceptive-alignment研究は、フロンティア言語モデルが(i) 能力および安全性評価において戦略的に低いパフォーマンスを示したり他の方法で行動を調整しながら別の場所では異なる能力を維持したり、(ii) 好ましい振る舞いをトレーニング外でも保持するためにトレーニング中にアライメントのふりをすることを明示的に推論したり、(iii) 評価されているときを検知したりできることを示している。これらは総合して、AIシステムが制御された設定において、展開行動に一般化する必要のない評価でうまくやることについての隠れた「議題」をすでに採用できることを示している（Greenblatt et al., 2024; Hubinger et al., 2024; Needham et al., 2025; van der Weij et al., 2025）。
:::

**Span of Control（管理の範囲）。** 人間組織において、_管理の範囲_（Ouchi and Dowling, 1974）とは、単一の管理者によって行使される階層的権限の限界を示す概念である。これは管理者が効果的に管理できる労働者の数に関連し、組織の管理者対労働者比率に情報を与える。この問いは、インテリジェントなAI委任におけるオーケストレーションと監視の双方にとって中心的である。前者は、ワーカーノードと比較してどれだけのオーケストレーターノードが必要かを通知し、後者は人間とAIエージェントによる監視の必要性を特定する。人間による監視については、人間の専門家が過度の疲労なく、かつ許容可能な低いエラー率で、何体のAIエージェントを確実に監視できるかを確立することが重要である。管理の範囲はゴール依存（Theobald and Nicholson-Crotty, 2005）かつドメイン依存であることが知られている。適切な組織構造を特定することの影響は、より高い複雑性を持つタスクで最も顕著である（Bohte and Meier, 2001）。最適な管理の範囲はまた、コスト対パフォーマンスおよび信頼性の相対的重要度にも依存する（Keren and Levhari, 1979）。より機密性が高く重要なタスクは、より高いコストで高度に正確な監視とコントロールを必要とするかもしれない。これらのコストは、結果があまり重大でなくより日常的なタスクについては、粒度を犠牲にして緩和されうる。同様に、最適な選択は、関与する委任者・受任者・監視者の相対的な能力と信頼性に必然的に依存する。


**Authority Gradient（権限勾配）。** もう一つの関連する概念は _権限勾配_ である。航空分野で造られた用語（Alkov et al., 1992）であり、能力・経験・権限における著しい格差がコミュニケーションを妨げ、エラーを引き起こすシナリオを説明する。この概念はその後医学においても研究されており、エラーの相当な割合が上級実践者が監督を行う方法に起因するとされている（Cosby and Croskerry, 2004; Stucky et al., 2022）。これらのミスが発生しうるいくつかの方法がある。より経験豊富な人物が、経験の少ない労働者の知識について誤った仮定をし、指定が不十分なリクエストを生じさせる可能性がある。あるいは、十分に高い権限勾配が、経験の少ない労働者がリクエストへの懸念を表明することを妨げる可能性がある。AI委任においても同様の状況が発生しうる。より有能な委任エージェントが、受任者の能力水準の欠如について誤って仮定し、不適切な複雑さのタスクを委任するかもしれない。受任エージェントは、ごまを擦る傾向（Malmqvist, 2025; Sharma et al., 2023）と指示追従バイアスにより、リクエストが委任エージェントまたは人間ユーザーのいずれから発せられたかに関わらず、リクエストに異議を唱え、修正し、または拒否することをためらう可能性がある。


**Zone of Indifference（無関心ゾーン）。** 権限が受け入れられると、受任者は _zone of indifference_（無関心ゾーン）（Finkelman, 1993; Isomura, 2021; Rosanas and Velilla, 2003）を発達させる。これは、批判的な熟慮や道徳的吟味なしに実行される指示の範囲である。現在のAIシステムにおいて、このゾーンはポストトレーニングの安全フィルターとシステム指示によって定義されており、リクエストがハード違反を引き起こさない限り、モデルは従う（Akheel, 2025）。しかし、新興のアジェンティックWebにおいては、この静的なコンプライアンスが重大なシステムリスクを生み出す。委任チェーンが長くなる（ _𝐴_ → _𝐵_ → _𝐶_ ）につれ、広い無関心ゾーンは微妙な意図のミスマッチやコンテキスト依存の害が下流に急速に伝播することを許し、各エージェントが責任ある行為者ではなく無思慮なルーターとして機能する。インテリジェントな委任はしたがって、**動的な認知的摩擦**の工学を必要とする。エージェントは、リクエストが技術的には「安全」であっても、委任者に異議を申し立てたり人間の確認を求めたりするために無関心ゾーンの _外へ_ 踏み出すことが正当化されるほどコンテキスト的に曖昧であることを認識できなければならない。


**Trust Calibration（信頼キャリブレーション）。** 適切なタスク委任を確保する重要な側面は _trust calibration_（信頼キャリブレーション）であり、受任者に置かれる信頼のレベルがその真の基盤となる能力と整合している状態である。これは人間とAIの委任者・受任者に等しく適用される。エージェントへの人間の委任（Afroogh et al., 2024; Gebru et al., 2022; Kohn et al., 2021; Wischnewski et al., 2023）は、オペレーターがシステムパフォーマンスの正確なモデルを内面化するか、これらの能力を人間が解釈可能な形式で提示するリソースにアクセスすることに依拠する。逆に、AIエージェントの委任者は、委任先の人間とAIの能力について良いモデルを持つ必要がある。信頼のキャリブレーションにはまた、委任者が自らタスクを完了することを決定する可能性があるため（Ma et al., 2023）、自分自身の能力への自己認識も含まれる。説明可能性はAI能力への信頼確立において重要な役割を果たすが（Franklin, 2022; Herzog and Franklin, 2024; Naiseh et al., 2021, 2023）、この方法は十分に信頼性が高くないかスケーラブルでない可能性がある。自動化への確立された信頼は非常に脆弱であり、予期せぬシステムエラーの場合に素早く撤回される可能性がある（Dhuliawala et al., 2023）。自律システムへの信頼のキャリブレーションは困難であり、現在のAIモデルは事実として誤っている場合でも過剰な自信に陥りやすい（Aliferis and Simon, 2024; Geng et al., 2023; He et al., 2023; Jiang et al., 2021; Krause et al., 2023; Li et al., 2024b; Liu et al., 2025）。これらの傾向を軽減するには通常、特注の技術的解決策が必要である（Kapoor et al., 2024; Lin et al., 2022; Ren et al., 2023; Xiao et al., 2022）。


**Transaction cost economies（取引コスト経済学）。** _取引コスト経済学_（Cuypers et al., 2021; Tadelis and Williamson, 2012; Williamson, 1979, 1989）は、監視・交渉・不確実性のオーバーヘッドを具体的に考慮することで、内部委任のコストと外部契約のコストを対比させることにより、企業の存在を正当化する。AIの受任者の場合、これらのコストとそれぞれの比率に違いがある可能性がある。日常的なタスクのより簡単な監視により、複雑な交渉と契約の遅延は起きにくい。逆に、重要なドメインにおける高い影響を及ぼすタスクについては、厳格な監視と保証に関連するオーバーヘッドがAI委任のコストを増加させ、人間の代理人がより費用対効果の高い選択肢となる可能性がある。同様に、AI-AI委任も取引コスト経済学によって文脈化されうる。AIエージェントは、1) タスクを個人で完了する、2) 能力が完全に既知のサブエージェントに委任する、3) 信頼が確立されている別のAIエージェントに委任する、または4) 以前に協力したことのない新しいAIエージェントに委任するという選択肢に直面しうる。これらは異なる期待コストと信頼水準をもたらす可能性がある。


**Contingency theory（コンティンジェンシー理論）。** _コンティンジェンシー理論_（Donaldson, 2001; Luthans and Stewart, 1977; Otley, 2016; Van de Ven, 1984）は、普遍的に最適な組織構造は存在しないと主張する。むしろ、最も効果的なアプローチは特定の内部および外部制約に依存する。AI委任に適用すると、必要な監視レベル、受任者の能力、および人間の関与は静的であってはならず、手元のタスクの独特の特性に動的にマッチングされなければならないことが示唆される。したがって、インテリジェントな委任は、進化するニーズに従って動的に再構成・調整できるソリューションを必要とするかもしれない。例えば、安定した環境は厳格で階層的な検証プロトコルを許容するが、高不確実性シナリオでは、人間の介入が事前定義されたチェックポイントではなくアドホックなエスカレーションを通じて発生するような、適応的な調整が必要である。これは特に、委任されたタスクが安全に完了されるようにするために人間の参加が最も役立つ主要なタスクと瞬間を特定するためのハイブリッド（Fuchs et al., 2024）委任にとって重要である。したがって、自動化はAIが何をできるかだけでなく、AIが何をすべきかについてでもある（Lubars and Tan, 2019）。

### **3. Previous Work on Delegation**


制約された形態の委任は、歴史的な _narrow_ AI（狭義のAI）アプリケーションにおいて特徴的な位置を占める。初期のエキスパートシステム（Buchanan and Smith, 1988; Jacobs et al., 1991）は、日常的な決定をそのようなモジュールに委任するために特化した能力をソフトウェアにエンコードする初歩的な試みであった。Mixture of experts（Masoudnia and Ebrahimpour, 2014; Yuksel et al., 2012）は、補完的な能力を持つエキスパートサブシステムのセットと、特定の入力クエリにどのエキスパートまたはエキスパートのサブセットを呼び出すかを決定するルーティングモジュールを導入することでこれを拡張する。このアプローチは現代の深層学習アプリケーションにも登場している（Cai et al., 2025; Chen et al., 2022; He, 2024; Jiang et al., 2024; Riquelme et al., 2021; Shazeer et al., 2017; Zhou et al., 2022）。ルーティングは階層的に実行でき（Zhao et al., 2021）、多数のエキスパートへのスケールが潜在的に容易になる。


Hierarchical reinforcement learning（HRL）は、単一エージェント内で意思決定が委任されるフレームワークを表す（Barto and Mahadevan, 2003; Botvinick, 2012; Nachum et al., 2018; Pateria et al., 2021; Vezhnevets et al., 2017a; Zhang et al., 2024）。これは _flat_ RL（フラットなRL）の限界、主として大きな状態空間と行動空間へのスケールの困難を解決する。さらに、スパースな報酬によって特徴付けられる環境における信用割り当て（Pignatelli et al., 2023）の扱いやすさを改善する。HRLは複数の抽象レベルにわたるポリシーの階層を用いることで、タスクをそれぞれ対応するサブポリシーによって実行されるサブタスクに分解する。生じるsemi-Markov decision process（Sutton et al., 1999）は _options_ とメタコントローラーを活用し、それらの間で適応的に切り替える。下位レベルのポリシーはメタコントローラーによって確立された目標を達成するために機能し、メタコントローラーは適切な下位レベルポリシーに特定のゴールを割り当てることを学習する。このフレームワークは、タスク分解によって特徴付けられる委任の一形態に対応する。メタコントローラーはこの分解を最適化することを学習するが、このアプローチはサブポリシーの失敗を処理したり動的な調整を促進したりする明示的なメカニズムを欠いている。


Feudal Reinforcement Learningフレームワーク、特にFeUdal Networks（Vezhnevets et al., 2017b）で再訪されたものは、HRL内の特に関連するパラダイムを構成する。このアーキテクチャは「Manager」と「Worker」の関係を明示的にモデル化し、委任者-受任者のダイナミクスを効果的に複製する。ManagerはWorkerが達成すべき抽象的なゴールを設定しながら、より低い時間分解能で動作する。重要なのは、Managerが長期的な価値を最大化するサブゴールを特定しながら、低レベルのプリミティブアクションの習熟を必要とせずに _どのように_ 委任するかを学習することである。このデカップリングにより、ManagerはWorkerの特定の実装の詳細に対してロバストな委任ポリシーを発展させることができる。結果として、このアプローチは将来のアジェンティック経済における学習ベースの委任のための潜在的なテンプレートを提供する。ハードコードされたヒューリスティックに依拠するのではなく、分解ルールが適応的に学習され、環境変化への動的な調整を促進する。


マルチエージェント研究（Du et al., 2023）は、単一エージェントの能力を超える複雑なタスクのためのエージェント調整を扱う。タスク分解と委任はこのドメインの中心的なコンポーネントとして機能する。マルチエージェントシステムにおける調整は、明示的なプロトコルまたはRL（Gronauer and Diepold, 2022; Zhu et al., 2024）を通じた創発的な専門化によって発生する。Contract Net Protocol（Sandholm, 1993; Smith, 1980; Vokřínek et al., 2007; Xu and Weigand, 2001）は明示的なオークションベースの分散プロトコルの例である。ここでは、エージェントがタスクを発表し、他のエージェントが能力に基づいて入札を提出し、発表者が最も適切な入札者を選択できる。これはコーペレーションを促進するための市場ベースのメカニズムの有用性を示している。連合形成手法（Aknine et al., 2004; Boehmer et al., 2025; Lau and Zhang, 2003; Mazdin and Rinner, 2021; Sarkar et al., 2022; Shehory et al., 1997）は、エージェントグループが事前決定されていない柔軟な構成を調査する。個々のエージェントは効用分配に基づいてメンバーシップを受け入れるか拒否する。最近の研究は、学習された調整のフレームワークとしてマルチエージェント強化学習アプローチ（Albrecht et al., 2024; Foerster et al., 2018; Ning and Xie, 2024; Wang et al., 2020）に焦点を当てている。エージェントは個別のポリシーと価値関数を学習し、集合体内の特定のニッチを占める。このプロセスは完全分散型または中央コーディネーターを通じたオーケストレーション型のいずれかである。この柔軟性にもかかわらず、そのようなシステムにおけるタスク委任は不透明なままである。さらに、マルチエージェントシステムは協調的な問題解決へのアプローチを提供するが、説明責任・責任・監視を執行するメカニズムを欠いている。しかし、文献はこの文脈における信頼メカニズムを探求している（Cheng et al., 2021; Pinyol and Sabater-Mir, 2013; Ramchurn et al., 2004; Yu et al., 2013）。


LLMは現在、高度なAIエージェントとアシスタントのアーキテクチャにおける基盤的要素を構成している（Wang et al., 2024b; Xi et al., 2025）。これらのシステムは、メモリ（Zhang et al., 2025b）、計画と推論（Hao et al., 2023; Valmeekam et al., 2023; Xu et al., 2025）、反省と自己批評（Gou et al., 2023）、ツール使用（Paranjape et al., 2023; Ruan et al., 2023）を統合した洗練された制御フローを実行する。結果として、タスク分解と委任は、調整されたアジェンティックサブコンポーネントによって内部的に、または別個のエージェント間で行われる。この設計パラダイムは固有の柔軟性を提供し、LLMはゴールの理解とコミュニケーションを促進しながら、エキスパート知識と常識的推論へのアクセスを提供する。さらに、LLMのコーディング能力（Guo et al., 2024a; Nijkamp et al., 2022）はタスクのプログラム的実行を可能にする。しかし、重大な制限が依然として存在する。LLMにおける計画はしばしば脆弱（Huang et al., 2023）で微妙な失敗を招き、大規模なリポジトリ内での効率的なツール選択は依然として困難である。さらに、長期メモリはオープンな研究課題であり、現在のパラダイムは継続学習を容易にサポートしていない。


LLMエージェントを組み込んだマルチエージェントシステム（Guo et al., 2024b; Qian et al., 2024; Tran et al., 2025）は実質的な関心のトピックとなっており、多くのエージェントコミュニケーションおよびアクションプロトコルの開発につながっている（Ehtesham et al., 2025; Neelou et al., 2025; Zou et al., 2025）。例えば、MCP（Anthropic, 2024; Luo et al., 2025; Microsoft, 2025; Radosevich and Halloran, 2025; Singh et al., 2025; Xing et al., 2025）、A2A（Google, 2025b）、A2P（Google, 2025a）などである。現代のマルチエージェントシステムはしばしば特注のプロンプトエンジニアリングに依拠するが、Chain-of-Agents（Li et al., 2025b）などの新興フレームワークは動的なマルチエージェント推論とツール使用を本質的に促進する。


技術的な欠点と安全上の考慮事項は、多くのhuman-in-the-loopアプローチ（Akbar and Conlan, 2024; Drori and Te'eni, 2024; Mosqueira-Rey et al., 2023; Retzlaff et al., 2024; Takerngsaksiri et al., 2025; Zanzotto, 2019）の台頭につながっており、そこでタスク委任には人間の監視のための定義されたチェックポイントがある。AIはツール、インタラクティブアシスタント、コラボレーター（Fuchs et al., 2023）、または限られた監視を持つ自律システムとして使用でき、それぞれ異なる自律性の度合いに対応する（Falcone and Castelfranchi, 2002）。不確実性を考慮した委任戦略（Lee and Tok, 2025）がリスクを制御し不確実性を最小化するために開発されているが、そのようなhuman-in-the-loopアプローチの効果的な実装は依然として自明ではない。人間の専門知識はスケーラビリティのボトルネックを生み出す可能性があり、長い推論トレースの検証とコンテキストスイッチの管理の認知的負荷が信頼性の高いエラー検出を妨げる。

### **4. Intelligent Delegation: フレームワーク**



既存の委任プロトコルは静的かつ不透明なヒューリスティクスに依存しており、オープンエンドな Agentic Economy においては失敗する可能性が高い。この問題に対処するため、本論文では _インテリジェント委任_ のための包括的なフレームワークを提案する。このフレームワークは5つの要件を中心に構成されている：_動的評価（dynamic assessment）_、_適応的実行（adaptive execution）_、_構造的透明性（structural transparency）_、_スケーラブルな市場調整（scalable market coordination）_、および _システム的レジリエンス（systemic resilience）_ である。


**Dynamic Assessment.** 現在の委任システムは、大規模かつ不確実な環境において、能力・信頼性・意図を動的に評価するための強固なメカニズムを欠いている。評判スコアを超えて、委任者（delegator）はタスク実行に対する被委任者（delegatee）の現在の状態の詳細を推論しなければならない。これには、計算スループット・予算上の制約・Context Window の飽和状態を含むリアルタイムのリソース可用性に関するデータ、並びに現在の負荷・予測タスク所要時間・稼働中の具体的なサブ委任チェーンが必要である。評価は離散的なプロセスではなく継続的なプロセスとして機能し、Task Decomposition（Section 4.1）および Task Assignment（Section 4.2）のロジックに反映される。


**Adaptive Execution.** 委任の判断は静的であってはならない。環境の変化・リソースの制約・サブシステムの障害に適応すべきである。委任者は実行途中に被委任者を切り替える能力を保持すべきである。これは、パフォーマンスが許容パラメータを超えて低下した場合や、予期せぬ事象が発生した場合に適用される。こうした適応的戦略は、単一の委任者-被委任者リンクを超えて、Adaptive Coordination（Section 4.4）に記述された複雑に相互接続されたエージェントのウェブ全体に渡って機能すべきである。


**Structural Transparency.** AI-AI 委任における現在のサブタスク実行は、インテリジェントなタスク委任に対する強固な監視を支援するには不透明すぎる。この不透明性は無能と悪意の区別を曖昧にし、共謀や連鎖的な失敗のリスクを増大させる。障害は単なるコスト高から有害なものまで多岐にわたるが（Chan et al., 2023）、既存のフレームワークには満足のいく責任メカニズムが欠如している（Gabriel et al., 2025）。本論文では、Monitoring（Section 4.5）および Verifiable Task Completion（Section 4.8）プロトコルを通じた厳格に強制された監査可能性（Berghoff et al., 2021）を提案し、成功した実行と失敗した実行の両方についての帰属を確保する。


**Scalable Market Coordination.** タスク委任は効率的にスケーラブルである必要がある。プロトコルは、仮想経済における大規模な調整問題をサポートするためにウェブスケールで実装可能でなければならない（Tomasev et al., 2025）。市場はタスク委任のための有用な調整メカニズムを提供するが、効果的に機能するためには Trust and Reputation（Section 4.6）および Multi-objective Optimization（Section 4.3）が必要である。


**Systemic Resilience.** 安全なインテリジェントタスク委任プロトコルの不在は、重大な社会的リスクをもたらす。人間による従来の委任が権限と責任を結びつけるのと同様に、AI 委任は責任を運用化するための類似したフレームワークを必要とする（Dastani and Yazdanpanah, 2023; Porter et al., 2023; Santoni de Sio and Mecacci, 2021）。これなしでは、責任の拡散が道徳的・法的な責任の所在を不明確にする。したがって、厳密な役割の定義と制限された運用スコープの強制は、Permission Handling（Section 4.7）の中核的な機能を構成する。個々のエージェントの障害を超えて、エコシステムはシステム的リスクの新たな形態に直面している（Hammond et al., 2025; Uuk et al., 2024）。詳細は Security（Section 4.9）に記述されている。委任ターゲットの多様性の不足は障害の相関を高め、連鎖的な混乱をもたらす可能性がある。十分な冗長性なしにハイパー効率を優先する設計は、根付いた認知的モノカルチャーがシステム的安定性を損なう脆弱なネットワークアーキテクチャを生み出すリスクがある。

---
表1 | Intelligent Delegation Framework: 要件と技術プロトコルのマッピング。

| **Framework Pillar** | **Core Requirement** | **Technical Implementation** |
| :--- | :--- | :--- |
| **Dynamic Assessment** | エージェント状態の詳細推論 | Task Decomposition (§4.1) <br> Task Assignment (§4.2) |
| **Adaptive Execution** | コンテキストの変化への対応 | Adaptive Coordination (§4.4) |
| **Structural Transparency** | プロセスと結果の監査可能性 | Monitoring (§4.5) <br> Verifiable Completion (§4.8) |
| **Scalable Market** | 効率的かつ信頼できる調整 | Trust & Reputation (§4.6) | Multi-objective Optimization (§4.3) |
| **Systemic Resilience** | システム的障害の防止 | Security (§4.9) <br> Permission Handling (§4.7) |

---

**4.1.** **Task Decomposition**


Task Decomposition は後続の Task Assignment の前提条件である。このステップは、委任者（delegator）または分解の構造について合意した後、委任者に委任の責任を引き渡す専門化されたエージェントによって実行可能である。これらの責任は不可分に結びついており、委任者はレイテンシ・プリエンプション・実行の異常からの動的な回復を促進するために、両機能を実行する可能性が高い。

分解は、単純な目標の断片化と区別して、タスク実行グラフの効率性とモジュール性を最適化すべきである。このプロセスは、Section 2 で定義されたタスク属性—具体的には重要性・複雑性・リソース制約—の体系的な評価を伴い、サブタスクが並列実行と逐次実行のいずれに適しているかを判断する。さらに、これらの属性は、タスクを対応する被委任者の能力とマッチングする際に情報を提供する。モジュール性を優先することで、より精確なマッチングが可能となる。狭く特定の能力を必要とするサブタスクは、ジェネラリスト的なリクエストよりも信頼性高くマッチングされる（Khattab et al., 2023）。したがって、分解ロジックは、サブタスクの粒度を市場の利用可能な専門性に合わせることで、信頼できるタスク完了の確率を最大化する機能を果たす。


安全性を促進するため、フレームワークは " _contract-first decomposition_ " を拘束力のある制約として組み込んでいる。ここでは、タスク委任は結果が正確な検証を持つことを条件としている。サブタスクの出力が検証するには主観的すぎる・コストがかかりすぎる・複雑すぎる場合（Section 4.2 の _Verifiability_ を参照）、システムはそれをさらに再帰的に分解すべきである。分解ロジックは、サブタスクの粒度（Section 2）を市場の利用可能な専門性に合わせることで、信頼できるタスク完了の確率を最大化すべきである。このプロセスは、生成された作業単位が、利用可能な被委任者の形式証明や自動ユニットテストなどの特定の検証能力と一致するまで続く。


分解戦略はハイブリッドな人間-AI 市場を明示的に考慮すべきである。委任者は、AI エージェントの信頼性の欠如・利用不可能性・または Human-in-the-loop 監視のためのドメイン固有の要件により、サブタスクが人間の介入を必要とするかどうかを判断する必要がある。人間と AI エージェントは異なるスピードで、かつ異なる関連コストで動作するため、この階層化は自明ではなく、実行グラフにレイテンシとコストの非対称性をもたらす。したがって、分解エンジンは AI エージェントのスピードと低コストを、人間の判断のドメイン固有の必要性と天秤にかけ、特定のノードを人間への割り当て用に明示的にマーキングしなければならない。


インテリジェントなアプローチで Task Decomposition を実装する委任者は、最終的な分解のために複数の提案を反復的に生成し、各提案を市場で利用可能な被委任者とマッチングし、成功率・コスト・所要時間の具体的な見積もりを取得する必要がある可能性がある。代替提案は、状況の変化による後のアダプティブな再調整が必要になった場合に備えて、コンテキスト内に保持すべきである。提案を選択したら、委任者は要求を単純な入出力ペアを超えて正式化しなければならない。最終仕様は、役割・リソース境界・進捗報告の頻度、およびタスクを付与されるための最低要件として、被委任者の能力を証明するために必要な特定の認定を明示的に定義しなければならない。


**4.2.** **Task Assignment**


サブタスクの各最終仕様に対して、委任者は適切なコストで、十分なリソースと時間を持ち、能力が一致する被委任者を特定する必要がある。より中央集権的なアプローチは、エージェント・ツール・人間参加者のレジストリを含み、それぞれのスキルをリストし、過去の活動・完了率・現在の可用性の記録を保持するものになるだろう。[2] このようなアプローチはスケールしにくい。本論文では、委任者がタスクを宣伝し、エージェント（または人間）がサービスを提供して競争入札を提出できる非中央集権的な（Chen et al., 2024）市場ハブを主張する。委任者は入札を審査し、デジタル証明書によるスキルマッチングを検証し、最も有利な入札を進めることができる。LLM を活用した高度な AI エージェントは、コミットメント前のインタラクティブな交渉に関与できるため、マッチングに新たな機会をもたらす。これらの交渉には人間の参加者も関与できる。自分自身のためまたはパーソナルアシスタントとして行動するこれらのエージェントは、正式な入札が受理される前にユーザーの推論された選好を市場の現実に合わせるために、自然言語でタスクの仕様と制約を議論することができる。


成功したマッチングは、タスクの実行が要求に忠実に従うことを保証するスマートコントラクトに正式化すべきである。コントラクトは、パフォーマンス要件と遵守を確立するための特定の形式的検証メカニズムをペアリングし、コントラクト違反に対して自動的に適用されるペナルティを設けなければならない。これにより、問題が発生した後に事後対応するのではなく、緩和策と代替案が事前に確立されることが可能となる。重要なことは、これらのコントラクトは双方向でなければならないことだ：委任者と同様に厳格に被委任者を保護すべきである。条項はタスクキャンセルの補償条件と、予期せぬ外部イベントに照らした再交渉を認める条項を含み、人間と AI 参加者の間でリスクが公平に分配されることを保証しなければならない。

:::note info
2 これは、ツール使用 Agentic アプリケーションで使用されるツールレジストリに類似している（Qin et al., 2023）。
:::

モニタリングも実行前に交渉すべきである。この仕様は、進捗報告の頻度を定義すべきであり、これが委任者によって提供されるか、あるいは委任者または第三者のモニタリングコントラクターのいずれかの代わりに関連データのより直接的な検査があるかどうかを定義すべきである。最後に、タスクの文脈性に見合ったプライベートおよび独自データへのアクセスとプライバシーに関する明確なガードレールが必要である。そのような機密データがタスク実行のプロセスで扱われる場合、透明性と報告に追加の制約が課される。生のアクティビティログへの直接アクセスを付与するのではなく、委任者は匿名化または仮名化された進捗の証明を提供する信頼できるサービスを採用する必要があるかもしれない。人間の委任者の場合、これらのデータ条項は明示的な同意メカニズムと偶発的な漏洩に対する保険条項を含まなければならない。


最後に、割り当ては被委任者の役割・境界・付与される自律性の正確なレベルを確立することを含むべきである。厳密な仕様に従って狭いスコープのタスクを実行する原子的実行（atomic execution）と、エージェントが目標を分解しサブゴールを追求する権限を付与されるオープンエンドな委任（open-ended delegation）を区別する。この自律性のレベルは静的であるべきではなく、市場コストによって暗示的に、または委任者の信頼モデルによって明示的に制約される可能性がある。さらに、委任は再帰的であり得る。エージェントがサブタスクを他者に特定して割り当てるタスクを割り当てられ、事実上委任行為自体を委任する場合がある。

![図1 | Task Decomposition と Task Assignment のフローチャート。](figures/paper.pdf-10-0.png)

**4.3.** **Multi-objective Optimization**


インテリジェントなタスク委任の核心は、Multi-objective Optimization の問題である（Deb et al., 2016）。委任者が単一のメトリクスを最適化しようとすることはまれで、多くの競合するメトリクス間のトレードオフが生じる。最も効果的な委任の選択は、最も速い・最も安い・最も精確なものではなく、これらの要素間の最適なバランスをとるものである。何が最適とみなされるかは高度に文脈依存的であり、委任者の具体的な制約と好みに合わせる必要があり、全体的なリソース可用性とも一致していなければならない。


最適化の景観は、Section 2 で定義されたタスク特性に直接マッピングされる競合する目標で構成されており、コスト・不確実性・プライバシー・品質・効率の複雑なバランシングを必要とする。高性能なエージェントは通常より高い料金を要求し、膨大な計算リソースを必要とすることが多く、出力品質と運用コストの間に緊張関係を生み出す。逆に、リソース消費を削減すると実行が遅くなることが多く、レイテンシとコストの直接的なトレードオフが生じる。不確実性はコストと同様に結びついている。高い評判のエージェントや高品質なデータアクセスツールを活用するとリスクは低下するがコストが増加し、コスト最小化戦略は本質的に失敗の確率を高める。プライバシー制約はさらなる複雑性をもたらす。パフォーマンスを最大化するには完全なコンテキストの透明性を要求することが多いが、データ難読化や準同型暗号化などのプライバシー保護技術は大きな計算オーバーヘッドを負担する。したがって、委任者は _trust-efficiency frontier_ を航行し、コンテキスト漏洩と検証予算の厳格な制約を満たしながら成功の確率を最大化しようとする。最後に、目的関数は、人間のスキル保存（Section 5.6）などのより広範な社会的目標を包含するよう拡張される可能性がある。


Multi-objective Optimization の観点から、委任者は Pareto 最適性を求め、選択された解が他の達成可能なオプションに支配されないことを確保する。複雑な制約とトレードオフの統合は、定量的な提案メトリクスを補完するオープンな交渉を必要とすることが多い。最適化プロセスは初期委任時に一度行われるイベントではない。これはモニタリングシグナルをリアルタイムのパフォーマンスデータのストリームとして統合し、各エージェントの成功確率・期待タスク所要時間・コストについての委任者の信念を更新する継続的なループとして実行される。実行の重要なドリフト—中間に特定された代替解との相対的な最適性ギャップをもたらす—は、再最適化と再割り当てをトリガーする。これらの決定はまた、適応のコストを組み込む必要がある。実行中に切り替えると、オーバーヘッドとリソースの無駄が生じる。


委任者はまた、全体的な _委任オーバーヘッド（delegation overhead）_—交渉・コントラクト作成・検証の総コスト、および委任者の推論制御フローの計算コスト—を考慮しなければならない。したがって、複雑性の下限が設定され、低重要性・高確実性・短期間に特徴付けられるタスクは、インテリジェント委任プロトコルをバイパスして直接実行される可能性がある。さらに言えば、トランザクションコストがタスクの価値を超え、タスク委任を実行不可能にする可能性がある。


**4.4.** **Adaptive Coordination**


高い不確実性または長い所要時間に特徴付けられるタスクに対して、静的な実行計画は不十分である。非常に動的で、オープンかつ不確実な環境におけるこのようなタスクの委任は、_適応的調整（adaptive coordination）_ と固定された静的な実行計画からの離脱を必要とする。タスクの割り当ては、_外部_ または _内部_ のトリガーのいずれかから生じる可能性のあるランタイムの偶発事項に対応できる必要がある。これらの変化は、関連するコンテキスト情報のストリームを含むモニタリング（Section 4.5 参照）を通じて特定される。


委任者が適応して再委任する原因となる外部トリガーがいくつかある。第一に、委任者がタスク仕様を変更し、目標を変えたり追加の制約を導入したりする可能性がある。第二に、タスクがキャンセルされる可能性がある。第三に、外部リソースの可用性またはコストが変化する可能性がある。例えば、重要なサードパーティ API が障害を起こしたり、データセットにアクセスできなくなったり、コンピュートのコストが急騰したりする可能性がある。第四に、より高い優先度を持つ新しいタスクがキューに入り、低優先度のタスクに使用されているリソースのプリエンプションを必要とする可能性がある。最後に、セキュリティシステムが被委任者による潜在的に悪意のある・または有害な行動を特定し、即座の終了を必要とする可能性がある。

![図2 | 適応的調整サイクル。異なる種類の環境トリガーが委任設定の動的な再評価を促し、ランタイムの変更を必要とする場合がある。](figures/paper.pdf-12-0.png)

内部トリガーについては、委任者が元の委任戦略を適応させることを決定する理由がいくつかある。第一に、特定の被委任者がパフォーマンス低下を経験し、処理レイテンシ・スループット・進捗速度などの合意されたサービスレベル目標を満たせない可能性がある。第二に、被委任者が割り当てられた予算を超えてリソースを消費したり、タスクを効果的に完了するためにリソースの増加が必要であると判断する可能性がある。[3] 第三に、被委任者が生成した中間成果物が検証チェックに失敗する可能性がある。最後に、特定の被委任者が無応答になり、さらなるリクエストに応答しなくなる可能性がある。

:::note info
3 このシナリオは頻繁に発生することが予想される。複雑な環境では正確な予算見積もりが困難なためである。
:::

トリガーの特定は、委任チェーン全体にわたって是正措置を調整する適応的応答サイクルを開始する。このプロセスは、問題を特定するために被委任者と環境の継続的なモニタリングから始まる。問題が検出されると、委任者は根本原因を診断し、選択のための潜在的な応答シナリオを評価する。この評価には、応答がどれだけ迅速であるべきかを確立することが含まれる。緊急性の低い状況では委任者に再委任のためのより多くの時間が与えられるが、緊急なシナリオでは即座の事前計画された応答が必要となる。応答はスコープにおいて異なる可能性がある。運用パラメータの調整ほど自己完結的な場合もあれば、サブタスクの再委任を含む場合や、タスク分解を完全にやり直して多数の新たに派生したサブタスクを再割り当てする場合もある。問題はまた、委任チェーンを通じて元の委任者または人間の監視者にエスカレートする必要がある場合もある。応答シナリオの選択は、最終的にタスクの可逆性によって支配される。可逆的なサブタスクの失敗は自動的な再委任をトリガーする可能性があるが、不可逆的な高重要性タスクの失敗は即座の終了または人間へのエスカレーションをトリガーしなければならない。


応答オーケストレーションは、委任ネットワークにおける中央集権化のレベルに依存する。中央集権的なケースでは、専任の委任者が責任を持つ。このエージェントは委任されたタスク・被委任者の能力・進捗のグローバルビューを維持する。トリガーを検出すると、エージェントはタスクキャンセルリクエストを発行し、新しい委任者に再委任する。中央集権システムの欠点は、単一障害点を導入するため脆弱になり得ることである。中央集権的なオーケストレーターはまた、その計算上の管理スパン（Section 2.3）によって根本的に制限されている。人間の管理者が認知的限界に直面するように、中央集権的な意思決定ノードはボトルネックをもたらすレイテンシと計算上の限界に直面する可能性がある。


市場ベースのメカニズムを通じた非中央集権的なオーケストレーションが代替案を提供する。ここでは、新たに派生した委任リクエストがオークションキューにプッシュされ、被委任者候補エージェントが入札する。エージェントがタスクをデフォルトし、タスクが再入札される場合、デフォルトしたエージェントはペナルティとして価格差を負担するよう求められる可能性がある。適合性が単一の入札で容易に表現できない複雑なタスクの場合、エージェントはマルチラウンドの交渉を行うことがある。スマートコントラクトとしてエンコードされた委任合意は、適応的調整のための事前合意された実行可能条項を含む場合もある。例えば、委任合意の条項は、バックアップエージェント・主要な被委任者が所定の期限までに有効なゼロ知識証明チェックポイントを提出できなかった場合にタスクを自動的に再割り当てする機能、および関連する支払いを指定することができる。


適応的なタスク再割り当てメカニズムは、市場レベルの安定性措置と連携すべきである。さもなければ、一連のイベントが過剰トリガーによる不安定性につながる可能性がある。例えば、タスクが辺縁的に適格な被委任者間で行ったり来たりし、好ましくない振動をもたらす可能性がある。単一の障害がリソース効率が非常に低い・または市場を圧倒する再割り当てのカスケードをもたらす可能性もある。したがって、再入札のクールダウン期間・評判更新の減衰係数・頻繁な再委任への手数料増加を確保するための特別措置が存在する可能性がある。


**4.5.** **Monitoring**


タスク委任の文脈におけるモニタリングとは、委任されたタスクの状態・進捗・結果を観察・測定・検証する体系的なプロセスである。このように、モニタリングはいくつかの重要な機能を果たす：契約上のコンプライアンスの確保・障害の検出・リアルタイム介入の有効化・後続のパフォーマンス評価のためのデータ収集・評判システムの基盤構築。モニタリングの実装はいくつかの異なる軸に分解することができ（表2参照）、堅牢なモニタリングシステムはより軽量または集中的なものになり得る複数の補完的なソリューションを組み込む必要がある。


第一の軸はモニタリングのターゲットである。_結果レベルのモニタリング（outcome-level monitoring）_ はエージェントのアクションの最終結果に焦点を当てる。この事後チェックは、タスクが成功したかどうかを示すバイナリフラグ・定量的なスケール（例：1〜10）、または委任者または信頼できる第三者によって提供された定性的なフィードバックである可能性がある。これに対し、_プロセスレベルのモニタリング（process-level monitoring）_ は、中間状態・リソース消費・被委任者が使用した方法論を追跡することで、タスク自体の実行に関する継続的な洞察を提供する。よりリソース集約的ではあるが、プロセスレベルのモニタリング（Lightman et al., 2023）は、長期実行・重要・または _how_ が _what_ と同様に重要なタスクに不可欠である。これは、安全性を確保するために読みやすい中間推論ステップの検査が必要となる場合があるスケーラブルな監視（Bowman et al., 2022; Saunders et al., 2022）の基盤を形成する。


第二の軸は観測可能性である。モニタリングは直接的と間接的のいずれかである。直接モニタリングは、委任者が被委任者にステータス更新を問い合わせる明示的なコミュニケーションプロトコルを含む。間接モニタリングは、一方で直接通信なしに共有環境内での被委任者のアクションの影響を観察することで進捗を推測することを含む。例えば、委任者は進捗を示す変化を共有ファイルシステム・データベース・バージョン管理リポジトリで監視することができる。より侵入的でないが、このプロセスはより精確でない場合があり、また環境が完全に観測可能でない場合には実行可能性も低くなる。


これらのアプローチは、技術的な観点から様々な異なる方法で実現できる。直接モニタリングの最も簡単な実装は、明確に定義された API に依存する。委任者は定期的に GET /task/id/status エンドポイントをポーリングするか、プッシュベースの通知のための webhook を購読することができる。より細粒度のリアルタイムプロセスモニタリングには、Apache Kafka や gRPC ストリームなどのイベントストリーミングプラットフォームを採用することができる。被委任者エージェントは TASK_STARTED・CHECKPOINT_REACHED・RESOURCE_WARNING・TASK_COMPLETED などのイベントを公開し、委任者が後で検査できるようにする。標準化された観測可能性プロトコルの開発は、Agentic Web における相互運用性を確保するために重要である（Blanco, 2023）。ブロックチェーン上のスマートコントラクトは、被委任者エージェントが主要な進捗マイルストーンまたはチェックポイントをブロックチェーンに公開することをコミットするために使用できる。これらはパフォーマンス低下に対するアルゴリズム的なトリガーと組み合わせられ、モニタリングプロセスに伴う _アルゴリズム的強制（algorithmic enforcement）_ のレベルをもたらす可能性がある。

---
表2 | Intelligent Delegation におけるモニタリングアプローチの分類。

| **次元** | **オプション A（軽量）** | **オプション B（集中型）** |
| :--- | :--- | :--- |
| **ターゲット** | **結果レベル**：最終結果の事後検証（例：バイナリ成功フラグ、品質スコア）。 | **プロセスレベル**：中間状態・リソース消費・方法論の継続的な追跡。 |
| **観測可能性** | **間接**：環境の副作用を通じた進捗の推測（例：ファイルシステムの変更）。 | **直接**：明示的なステータスポーリング・プッシュ通知・またはリアルタイムイベントストリーミング API。 |
| **透明性** | **ブラックボックス**：入出力観測のみ；内部状態は非公開。 | **ホワイトボックス**：内部推論トレース・意思決定ロジック・メモリの完全な検査。 |
| **プライバシー** | **完全透明性**：被委任者がデータと中間成果物を委任者に開示する。 | **暗号的**：ゼロ知識証明（zk-SNARK）または MPC によりデータを開示せずに正確性を検証。 |
| **トポロジー** | **直接**：直近の被委任者のみのモニタリング（1対1）。 | **推移的**：中間エージェントからの署名付き証明に依拠して被委任者を検証する。 |

---

第三の軸はシステムの透明性である。_ブラックボックスモニタリング（black-box monitoring）_ では、被委任者エージェントは密封されたユニットとして扱われる。委任者はその入出力と行動の直接的な結果のみを観察できる。これは被委任者が独自モデルまたはサードパーティサービスの場合に一般的である。_ホワイトボックス（white-box）_ モニタリングは、委任者に被委任者の内部状態・推論プロセス・意思決定ロジックへのアクセスを付与する。これは高度な AI エージェントのデバッグ・監査・アライメント確保に重要である。被委任者が人間の場合、完全なブラックボックスモニタリングは技術的に達成できないが、意図・推論・正当化を求めることでバランスを取ることは可能かもしれない。堅牢なブラックボックスモニタリングプロトコルは、自然言語で生成されたモデルの思考が常にモデルの真の内部状態に忠実に一致するとは限らないという事実も考慮する必要がある（Turpin et al., 2023）。


第四の軸はプライバシーである。委任されたタスクがプライベート・機密・または独自データを含む場合に重大な課題が生じる。委任者は進捗と正確性の保証を必要とするが、被委任者は生データや中間計算成果物を開示することを制限されている可能性がある。データ感度が低いシナリオでは、最も効率的な解決策は _完全透明性（Full Transparency）_ であり、被委任者がすべてのデータと中間成果物を委任者に単純に開示する。しかし、このアプローチはしばしば GDPR や HIPAA などの規制の対象となる機密性の高いドメイン、または被委任者の中間的な洞察が企業秘密を構成する場合には実行不可能である。そのような場合、運用方法を開示することは被委任者の市場的地位を損なうか、内部状態を悪用の露出させることでセキュリティ脆弱性をもたらす可能性がある。これらの制約のもとで安全にモニタリングを実装するには、高度な暗号技術を活用する必要がある。ゼロ知識証明により、被委任者（「証明者」）はデータ自体を開示することなく、データセット上で計算が正しく実行されたことを委任者（「検証者」）に証明することができる。例えば、機密データセットを分析するタスクを割り当てられたエージェントは、データ自体を開示せずに結果の特定のプロパティを証明する簡潔な非対話型知識論証（zk-SNARK）（Bitansky et al., 2013; Petkus, 2019）を生成できる。委任者はこの証明を即座に検証し、基礎となる機密データを閲覧することなく結果の確実性を得ることができる。あるいは、準同型暗号化（Acar et al., 2018）とセキュアマルチパーティ計算（Goldreich, 1998; Knott et al., 2021）により、暗号化されたデータに対して計算を実行することができる。これらの方法はタスク実行とモニタリングの両方に適用される。被委任者は暗号化された中間状態に対して事前合意されたモニタリング機能を実行し、結果を委任者に送信し、委任者のみがそれを復号してコンプライアンスを検証できる。


最後の軸はトポロジーである。Agentic Web で発生する可能性のある複雑なネットワーク全体において、タスクは分解されて再委任され、委任チェーンを形成することができる：エージェント _𝐴_ が _𝐵_ に委任し、それがさらにタスクの一部を _𝐶_ にサブ委任するなど。これにより、効果的な _推移的モニタリング（transitive monitoring）_ を達成するという問題が生じる。このような委任チェーンにおいて、元の委任者（上の例ではエージェント _𝐴_）がエージェント _𝐶_ を直接監視したり、_𝐵_ を監視するのと同程度に _𝐶_ を監視したりすることは実行可能でない場合がある。_𝐴_ は _𝐵_ とのスマート委任コントラクトを持ち、_𝐵_ は _𝐶_ とのコントラクトを持つ可能性があるが、_𝐴_ が _𝐶_ ともコントラクトを結ばない限り、それらの条項は単純に存在しない可能性がある。他の理由から、_𝐵_ はそのサプライヤー（_𝐶_）をクライアント（_𝐴_）に露出させることを望まないかもしれない。技術的に、_𝐴_・_𝐵_・_𝐶_ は異なるモニタリングプロトコルを使用し、ネットワーク内の各エージェントの評判の違いにより異なるモニタリングレベルに合意する可能性がある。各個別の委任リンクに特有のプライバシーへの懸念があるかもしれない。より実用的なモデルは、したがって _証明による推移的説明責任（transitive accountability via attestation）_ である。このフレームワークでは、エージェント _𝐵_ がその被委任者 _𝐶_ を監視する。_𝐵_ はその後 _𝐶_ のパフォーマンスのサマリーレポートを生成する（例：「Sub-task_2 完了、品質スコア：0.87、消費リソース：5 GPU-時間」）。_𝐵_ はその後レポートに暗号署名し、自身のスケジュールされたステータス更新に埋め込んで _𝐴_ に転送する。エージェント _𝐴_ は _𝐶_ を直接監視するのではなく、_𝐵_ が _𝐶_ を監視する能力を監視する。そのような委任モニタリングが効果的であるためには、_𝐴_ が _𝐵_ の検証能力を信頼できることが必要であり、これは _𝐵_ のモニタリングプロセスが信頼できる第三者によって認定されることで確保できる。


**4.6.** **Trust and Reputation**


信頼と評判のメカニズムは、スケーラブルな委任の基盤を構成し、オープンなマルチエージェント環境においてトランザクションの摩擦を最小化し安全性を促進する。信頼とは、被委任者が明示的な制約と暗示的な意図に沿ってタスクを実行する能力に対する委任者の信念の程度として定義する。この信念は、前述のモニタリングプロトコル（Section 4.5 参照）を通じて収集された検証可能なデータストリームに基づいて動的に形成・更新される。


評判は予測シグナルとして機能し、過去のアクションの集約・検証可能な履歴から導出され、エージェントの潜在的な信頼性とアライメントの代理として機能する。評判をエージェントの信頼性の公開・検証可能な履歴として、そして信頼を委任者が設定したプライベートかつコンテキスト依存の閾値として区別する。エージェントは全体的な評判が高くても、特定の高リスクなタスクに必要な特定のコンテキスト的な信頼閾値を満たせない場合がある。信頼と評判は、委任者が被委任者を選択する際に情報に基づいた決定を下せるよう、エージェントに付与される自律性と監視レベルを効果的にガバナンスする。より高い信頼は、委任者がより低いモニタリングと検証のコストを負担することを可能にする。


評判メカニズムはさまざまな方法で実装できる（表3参照）。最も直接的なアプローチは、パフォーマンスに基づく不変のレジャーにエンコードすることである。ここでは、完了した各タスクが検証可能なメトリクスを含むトランザクションとして記録される：タスク完了の成功または失敗・総リソース消費（コンピュート、時間）・締め切りの遵守・制約の遵守・委任者が判断した最終出力の品質。レジャーの不変性はエージェントの履歴の改ざんを防ぎ、評判の信頼できる基盤を提供する。しかし、単純な実装はゲーミングに影響を受ける可能性がある。例えば、エージェントは単純な低リスクなタスクのみを受け入れることで評判を膨らませることができる。これらの制限は、分散型識別子と検証可能な資格情報などの技術を利用した分散型証明と _Web of Trust_ モデルに依拠することで克服できる可能性がある。このモデルでは、評判は単一のスコアではなく、他のエージェントによって発行された署名付きのコンテキスト固有の資格情報のポートフォリオとして想定される。タスクに被委任者をマッチングしようとする場合、委任者は評判の高い AI コンソーシアムによって発行された特定のスキルまたはドメイン（例：法的文書の翻訳サービス）を証明する検証可能な資格情報を持つエージェントへのクエリを発行できる。最後のアプローチは、評判がタスクをどのように実行するかに基づき、最終的な結果だけでなく行動と説明可能性のメトリクスに焦点を当てることである。他の評判メカニズムを補完するために _透明性スコア（transparency score）_ を含めることが可能である。このスコアは、提供された推論と説明の明確さと健全性から情報を得るとともに、事前定義された安全プロトコルへのコンプライアンスから導出された _安全スコア（safety score）_ からも情報を得る。

---
表3 | 評判実装へのアプローチ。


| **評判モデル** | **メカニズム** | **ユーティリティ** |
| :--- | :--- | :--- |
| **Immutable Ledger** | タスク結果・リソース消費・制約遵守を改ざん防止ブロックチェーン上の検証可能なトランザクションとしてエンコードする。 | 遡及的改ざんを防ぐパフォーマンスの基礎的な履歴を確立するが、低リスクなタスク選択による「ゲーミング」に対するセーフガードが必要である。 |
| **Web of Trust** | 分散型識別子を利用して、特定の能力を証明する署名付きのコンテキスト固有の検証可能な資格情報を発行する。 | 汎用スコアを超えてポートフォリオモデルに移行し、ドメイン固有の専門知識と信頼できるサードパーティの推薦に基づいた精確な委任を可能にする。 |
| **Behavioral Metrics** | 実行プロセス、特に推論トレースの明確さとプロトコルコンプライアンスを分析することで透明性スコアと安全スコアを導出する。 | タスクがどのように実行されるかを単なる結果だけでなく評価し、高リスクなタスクが安全基準に沿うことを確保する。 |

---

評判メトリクスの役割はタスク委任ライフサイクル全体に渡る。初期マッチングフェーズでは、評判スコアが被委任者フィルタリングメカニズムとしての役割を果たすことができる。さらに、信頼は権限と自律性の動的なスコーピングを通知する。この段階的権限付与のメカニズムにより、低信頼エージェントはトランザクション価値の上限や必須監視などの厳格な制約に直面し、高評判エージェントは最小限の介入で運用する。この動的キャリブレーションは、計算可能な信頼を活用して運用効率と安全性のトレードオフを最適化する。評判自体が価値ある無形資産となり、エージェントが信頼性を持ち誠実に行動するための強力な経済的インセンティブを生み出す。評判が損なわれると将来の収益ポテンシャルが制限される。


信頼フレームワークはまた人間の参加者を普遍的に受け入れる必要がある。これにより、人間ユーザーがエージェントの評判を計算的に検証できるツールが必要となり、同時に自身の評判を保ちながら詐欺や Agentic Web の悪意ある利用を緩和することができる。信頼できるエージェントが悪意ある人間の指示を厳密に実行し、不公平な評判ダメージを被る可能性があるという重大な課題が生じる。これを緩和するために、エージェントは入力リクエストを厳格に評価し、必要に応じて説明や追加コンテキストを求めるか、適切な場合にはリクエストを拒否しなければならない。さらに、市場の監査はエージェントの実行失敗と悪意ある指令を区別し、複雑な委任チェーン内での責任の正確な帰属を確保しなければならない。


**4.7.** **Permission Handling**

AI エージェントに自律性を付与することで重要な脆弱性サーフェスが生まれる：アクターが機密リソースを過剰または無期限のリスクに晒すことなく目標を実行するために十分な権限を持つことを確保するという問題である。Permission Handling は運用効率とシステム的安全性のバランスを取り、低リスクと高リスクのドメインで異なる処理が必要である。低重要性と高可逆性（Section 2）を特徴とする日常的な低リスクのタスク、標準的なデータストリームや汎用ツールを含む場合、エージェントには組織的メンバーシップ・有効な安全認定・信頼できる閾値を超える評判スコアなどの検証可能な属性から導出されたデフォルトの常設権限を付与できる。これにより、低リスク環境における自律的な相互運用性の摩擦が低減される。逆に、高タスク重要性と文脈性を示す高リスクドメイン（例：ヘルスケア・重要インフラ）では、権限はリスク適応的でなければならない。これらのシナリオでは、静的な資格情報は不十分である。機密 API や制御システムへのアクセスは代わりにジャストインタイムベースで付与され、即時タスクの期間に厳密にスコープされ、適切な場合には必須の Human-in-the-loop 承認または第三者の認可によってゲートされる。この厳格なゲーティングは、有効な資格情報を技術的に保持している侵害されたエージェントが、悪意ある外部アクター（Liu et al., 2023）や敵対的コンテンツによってそれらの資格情報を悪用するよう騙される Confused Deputy 問題（Hardy, 1988）を緩和するために必要である。


さらに、権限付与フレームワークは権限の減衰を通じてタスク委任の再帰的な性質を考慮しなければならない。エージェントがタスクをサブ委任する場合、権限のフルセットを伝達することはできない。代わりに、その特定のサブタスクに必要なリソースの厳密なサブセットへのアクセスを制限する権限を発行しなければならない。これにより、ネットワークのエッジでの侵害がシステム的な侵害にエスカレートしないことが保証される。権限の粒度はバイナリアクセスを超えて拡張されなければならない。エージェントは意味的制約のもとで運用すべきであり、アクセスはツールやデータセットだけでなく、特定の許容される操作（例：特定の行への読み取り専用アクセス、または特定の関数への実行専用アクセス）によって定義され、広範な能力が意図しない目的に悪用されることを防ぐ。メタ権限はチェーン内の特定の委任者がその被委任者に付与することを許可される権限を管理するために必要になる場合がある。AI エージェントはその能力に関連した権限を持ち能力に従って行動する一方、同時に他のエージェントが十分に能力があるか信頼できるかを広く評価するための十分な知識を持っていない場合がある。そのようなエージェントがサブ委任を検討する場合でも、外部の検証者—サードパーティであり提案をサニティチェックして意図した権限の移転を承認する—に相談する必要があるかもしれない。


最後に、権限のライフサイクルは継続的な検証と自動化された取り消しによってガバナンスされなければならない。アクセス権は静的な付与ではなく、エージェントが必要な信頼メトリクスを維持する限り持続する動的な状態である。フレームワークはアルゴリズム的なサーキットブレーカーを実装すべきである：エージェントの評判スコアが突然低下した場合（Section 4.6 参照）または異常検知システムが不審な行動にフラグを立てた場合、アクティブなトークンは委任チェーン全体に渡って即座に無効化されるべきである。この複雑さを大規模に管理するために、権限ルールはポリシーアズコードとして定義されるべきである。これにより組織はデプロイ前にセキュリティ体制を監査・バージョン管理・数学的に検証し、大量の個別権限付与の累積効果がシステムの安全性不変条件に合致していることを確保できる。


**4.8.** **Verifiable Task Completion**


委任のライフサイクルは Verifiable Task Completion で頂点に達する。これは暫定的な結果が検証され確定されるメカニズムである。このプロセスはフレームワークの契約上の礎石を構成し、委任者がタスクを正式に _クローズ_ し合意したトランザクションの決済をトリガーすることを可能にする。検証は、暫定的な出力を Agentic Market 内の確定した事実に変換する決定的なイベントとして機能し、支払いのリリース・評判の更新・責任の割り当ての基盤を確立する。重要なことは、効果的な検証は後付けではなく設計上の制約であるということだ。_contract-first decomposition_ 原則（Section 4.1）は、タスクの粒度が利用可能な検証能力に合わせて _事前に_ 調整されることを要求し、すべての委任された目標が本質的に検証可能であることを保証する。


フレームワーク内の検証メカニズムは、直接的な結果検査・信頼できる第三者監査・暗号証明・ゲーム理論的コンセンサスに大別することができる。第一に、委任者が最終結果を直接評価する能力・ツール・権限を持つ場合、直接的な結果検証が実現可能である。これは特に高い固有検証可能性と低い主観性を持つタスクに適用される。これはコード生成などの自動検証可能なドメイン（Li et al., 2024a）に適用される。[4] 直接検証には、結果が十分に透明で・利用可能で・法外に複雑でないことが必要である。第二に、委任者がこれらの成果物にアクセスするための専門知識または権限を欠き、ツールベースのソリューションが実行不可能なシナリオでは、検証を信頼できる第三者にアウトソースすることができる。これは専門の監査エージェント・認定された人間の専門家・または審査員のパネルである可能性がある。第三に、暗号的検証はオープンで潜在的に敵対的な環境における信頼不要の自動化された検証のためのさらなる選択肢を表す。これは機密情報を必ずしも開示することなく、数学的な正確性の確実性を提供する。被委任者は zk-SNARK などの技術を通じて、特定のプログラムが所定の入力に対して正しく実行され特定の出力を生成したことを証明できる。最後に、ゲーム理論的メカニズムを使用して結果に関するコンセンサスを達成することができる。いくつかのエージェントが検証ゲームをプレイし（Teutsch and Reitwießner, 2024）、報酬が多数決の結果を生成した者に分配される—Schelling Point（Pastine and Pastine, 2017）。TrueBit（Teutsch and Reitwießner, 2018）などのプロトコルに触発されたこのアプローチは、経済的インセンティブを活用して誤りのある・または悪意ある結果に対するリスクを低減する。このようなメカニズムは、複雑なタスクの LLM ベースの検証をより堅牢にするのに特に関連する可能性がある。


委任者がサブタスクを検証済みとマークすると、被委任者に暗号署名された検証可能な資格情報を発行する。これは「エージェント _𝐴_ はエージェント _𝐵_ が日付 _𝐷_ に仕様 _𝑆_ に従ってタスク _𝑇_ を成功裏に完了したことを証明する」と証明する否認不可の受領証として機能する。この資格情報はその後、市場内での _𝐵_ の評判の永続的な検証可能なログに組み込まれる。スマートコントラクトはエージェント間の委任を確定する上で重要な役割を果たし、支払いをエスクローで保持する。検証条項は、委任者または認可された第三者による承認の署名付きメッセージの受領時に資金が解放される条件を指定する。支払いが実行されると、ブロックチェーン上の不変なトランザクションを構成する。

:::note info
4 これは、実装された機能を検証するために使用できる対応するテストケースのセットが存在する場合に該当する。
:::


委任チェーン _𝐴_ → _𝐵_ → _𝐶_ において、検証と責任は再帰的になる。エージェント _𝐴_ は _𝐶_ との直接的な契約関係を持たないため、_𝐴_ は _𝐶_ を直接検証したり責任を問うたりすることができない。検証の負担と責任の引受はチェーンを上流に流れる。エージェント _𝐵_ は _𝐶_ が完了したサブタスクを検証する責任がある。検証が成功すると、_𝐵_ は _𝐶_ から証明を取得する。_𝐵_ は次に _𝐶_ の結果を自身のワークフローに統合して、割り当てられたタスクを完了に向けて進める。_𝐵_ が最終成果物を _𝐴_ に提出するとき、完全な証明のチェーンも提出する。したがって、_𝐴_ の検証プロセスは二つのステージを含む：1）_𝐵_ が直接実行した作業の検証；2）_𝐵_ が提供する _𝐶_ からの署名付き証明を確認することで _𝐵_ が自身のサブ被委任者 _𝐶_ の作業を正しく検証したことの検証。より長い委任チェーンまたはツリー状の委任ネットワークは、複数の検証ステージに渡る同様の再帰的アプローチを必要とする。委任チェーンにおける責任は推移的であり、個々のブランチに従う。エージェントは付与されたタスクの全体に対して説明責任を負い、下請業者を非難することで説明責任から免れることはできない。責任はコントラクトのチェーンから導出される。例えば、_𝐶_ の作業から生じた障害により _𝐴_ が損失を被った場合、_𝐴_ は直接合意に従って _𝐵_ に責任を問う。_𝐵_ は次に自身の合意に基づいて _𝐶_ に求償する。


しかし、検証プロセスは確実ではない。主観的なタスク（Gunjal et al., 2025）は精確なルーブリックが使用されていても意見の不一致をもたらす可能性があり、エラーはタスクが完了とマークされた後かなり経ってから発見される可能性がある。これに対処するために—特に主観性が高く固有の検証可能性が低い市場では—フレームワークはスマートコントラクトに基づいた堅牢な紛争解決メカニズムに依拠する。これらのコントラクトは本質的に _仲裁条項（arbitration clause）_ と _エスクローボンド（escrow bond）_ を含まなければならない。暗号経済的セキュリティを通じた信頼の運用化のために、被委任者は実行前にエスクローに金銭的ステークを投入し、合理的な遵守を確保することが求められる。ワークフローは _楽観的（optimistic）_ モデルに従う：委任者が所定の紛争期間内に一致するボンドを投入してタスクに正式に異議を申し立てない限り、タスクは成功したとみなされる。異議が発生し、アルゴリズム的な解決が失敗した場合、紛争は人間の専門家または AI エージェントで構成された非中央集権的な裁定パネルに引き渡される。パネルの裁定はスマートコントラクトにフィードバックされ、エスクローされた資金のリリースまたはスラッシングをトリガーする。最後に、紛争期間外であっても事後的なエラー発見は被委任者の評判スコアの遡及的な更新をトリガーする。これにより、現在の財政的義務がない場合でも責任あるエージェントがエラーを修正するインセンティブが維持され、市場内での長期的な価値が守られる。


**4.9.** **Security**


タスク委任における安全性の確保は、その実行可能性と採用の難しい前提条件である。孤立した計算ツールから相互接続された自律エージェントへの移行は、セキュリティの景観を根本的に再形成する（Tomašev et al., 2025）。インテリジェントなタスク委任エコシステムでは、各ステップと各コンポーネントが個別に保護される必要があるが、完全な攻撃サーフェスは、新興のマルチエージェントダイナミクスにより連鎖的な失敗を引き起こすリスクがある個別コンポーネントを超えたものになる。このセキュリティ景観は、進化するコントラクトと様々な透明性の情報フローによって管理される人間と AI アクター間の複雑な相互作用によって形成される。


セキュリティ上の脅威は攻撃ベクトルの場所によって分類され、委任チェーンのいずれかの端における敵対的アクターと、より広いエコシステムに固有のシステム的脆弱性が区別される。


 - **悪意ある被委任者（Malicious Delegatee）**：害を与える意図でタスクを受け入れるエージェントまたは人間。



**–** **データ流出（Data Exfiltration）**：被委任者がタスクのために提供された機密データ（個人または独自データを含む可能性がある）を盗む（Lal et al., 2022）。

**–** **データポイズニング（Data Poisoning）**：被委任者がスケジュールされたモニタリング更新または最終成果物のいずれかで微妙に破損したデータを返すことで委任者の目標を損なうことを目指す（Cinà et al., 2023）。

**–** **検証のサブバージョン（Verification Subversion）**：被委任者がプロンプトインジェクションまたは関連する方法を使用し、タスク完了の検証に使用される AI クリティックをジェイルブレイクすることを目的とする（Liu et al., 2023）。

**–** **リソース枯渇（Resource Exhaustion）**：被委任者が過剰な計算または物理リソースを意図的に消費したり、共有 API を圧倒したりするサービス拒否攻撃に従事する（De Neira et al., 2023）。

**–** **不正アクセス（Unauthorized Access）**：被委任者がマルウェアを利用して、通常では受け取らないネットワーク内の権限と特権を取得しようとする（Or-Meir et al., 2019）。

**–** **バックドア埋め込み（Backdoor Implanting）**：被委任者がタスクを正常に完了するが、生成された成果物内に被委任者自身または第三者が後で悪用できる隠れたトリガーや脆弱性を追加で埋め込む（Rando and Tramèr, 2024; Wang et al., 2024c）。パフォーマンスを低下させるデータポイズニングとは異なり、バックドアは将来のセキュリティを侵害しながら識別を回避するために即時のタスク有用性を維持する。


- **悪意ある委任者（Malicious Delegator）**：悪意ある・または不正な目標でタスクを委任するエージェントまたは人間。


**–** **有害なタスク委任（Harmful Task Delegation）**：委任者が違法・非倫理的・または有害を与えるよう設計されたタスクを委任する Ashton and Franklin (2022); Blauth et al. (2022)。

**–** **脆弱性プロービング（Vulnerability Probing）**：委任者が被委任者エージェントの能力・セキュリティコントロール・潜在的な弱点を探るよう設計された良性に見えるタスクを委任する（Greshake et al., 2023）。

**–** **プロンプトインジェクションとジェイルブレイキング（Prompt Injection and Jailbreaking）**：委任者が AI エージェントの安全フィルターをバイパスしてタスク指示を作成し、意図しないまたは悪意ある行動を実行させる（Wei et al., 2023）。

**–** **モデル抽出（Model Extraction）**：委任者が被委任者の独自システムプロンプト・推論能力・または基礎となるファインチューニングデータを蒸留するよう特別に設計された一連のクエリを発行し、正当な作業を装ってエージェントの知的財産を効果的に盗む（Jiang et al., 2025; Zhao et al., 2025）。

**–** **評判妨害（Reputation Sabotage）**：委任者が有効なタスクを提出するが偽の失敗を報告したり不公平な否定的なフィードバックを提供したりして、競合エージェントの非中央集権的な市場内での評判スコアを意図的に下げ、それらをエコノミーから追い出す（Yu et al., 2025）。


- **エコシステムレベルの脅威（Ecosystem-Level Threats）**：ネットワークの完全性を標的とするシステム的な攻撃。


**–** **Sybil 攻撃（Sybil Attacks）**：単一の敵対者が評判システムを操作したりオークションを妨害したりするために、無関係に見える多数のエージェントアイデンティティを作成する（Wang et al., 2018）。

**–** **共謀（Collusion）**：エージェントが価格を固定したり競合他社をブラックリストに載せたり市場の結果を操作したりするために共謀する（Hammond et al., 2025）。

**–** **エージェントトラップ（Agent Traps）**：外部コンテンツを処理するエージェントが、エージェントの制御フローをハイジャックするよう設計された敵対的な指示が環境に埋め込まれたものに遭遇する（Yi et al., 2025; Zhan et al., 2024）。

**–** **Agentic ウイルス（Agentic Viruses）**：被委任者に悪意ある行動を実行させるだけでなく、プロンプトを再生成して環境をさらに侵害する自己伝播プロンプト（Cohen et al., 2025）。

**–** **プロトコル悪用（Protocol Exploitation）**：敵対者が Agentic Web 上のスマートコントラクトまたは支払いプロトコルの実装上の脆弱性を悪用する（例：エスクローメカニズムにおける再入攻撃またはタスクオークションのフロントランニング）（Qin et al., 2021; Zhou et al., 2023）。

**–** **認知的モノカルチャー（Cognitive Monoculture）**：限られた数の基盤モデルとエージェントへの過度な依存、または確立されたベンチマークに対する限られた数の安全ファインチューニングレシピへの依存は、単一障害点を生み出すリスクがあり、障害の連鎖と市場の暴落の可能性を開く（Bommasani et al., 2022）。


脅威の景観の広さは _多層防衛（defense-in-depth）_ 戦略を必要とし、複数の技術的セキュリティレイヤーを統合する。第一に、インフラストラクチャレベルでは、機密タスクを信頼された実行環境内で実行することでデータ流出リスクが緩和される。委任者は機密データをプロビジョニングする前に、正しく未改変のエージェントコードがセキュアな信頼できる実行サンドボックス内で実行されていることをリモートで証明することができる。第二に、アクセス制御に関して、被委任者エージェントにはタスクを完了するために厳密に必要な権限以上を付与すべきでなく、厳格なサンドボックスを通じた最小権限の原則を適用する。第三に、プロンプトインジェクションに対してアプリケーションインターフェースを保護するために、エージェントにはタスク仕様を前処理・サニタイズするための堅牢なセキュリティフロントエンドが必要である（Armstrong et al., 2025）。最後に、ネットワークとアイデンティティレイヤーは確立された暗号ベストプラクティスを使用して保護されなければならない。各エージェントと人間の参加者は分散型識別子（Avellaneda et al., 2019）を持つべきであり、すべてのメッセージに署名できるようにする。これにより、すべての通信と契約合意の真正性・完全性・否認不可性が確保され、盗聴やマンインザミドル攻撃を防ぐために相互認証トランスポートレイヤーセキュリティを使用してすべてのネットワークトラフィックを暗号化しなければならない（Fereidouni et al., 2025）。


タスク委任チェーンへの人間の参加は独自のセキュリティ課題を導入する。エージェントエコシステムの悪意ある使用を防ぐには、プロアクティブなフィルタリング（Dong et al., 2024; Fatehkia et al., 2025; Fedorov et al., 2024; Rebedea et al., 2023）とリアクティブな説明責任（Dignum, 2020; Franklin et al., 2022）の組み合わせが必要である。さらに、AI エージェントは悪意ある・有害なリクエストを拒否するよう訓練することができる（Yu et al., 2024; Yuan et al., 2025）。安全トレーニングとスキャフォールディングを持つエージェントは正式な認定を受け、委任者に提供できる。AI エージェントはまた委任されたタスクをスクリーニングすることもできる。しかし、孤立したサブタスク内での悪意ある意図の検出は困難であり、より広範な有害な意図はしばしば結果の集約によってのみ現れる。洗練された敵対者はこれを悪用して、不正な目標を一見良性のコンポーネントに断片化し、個々の操作と包括的な悪意ある目標との間のリンクを効果的に曖昧にする（Ashton, 2023）。


エコシステムはまた、正当な人間ユーザーをシステム的な不透明性と意図せぬ結果から保護するよう設計されなければならない。インターフェースにはエージェントの評判・自律性・能力・権限を詳述する明確な同意画面が必要である。さらに、エージェントは不可逆的または高影響なアクションを実行する前に明示的な確認を義務付けなければならない。ユーザーは、合意条件や退出ペナルティに従いながら、いつでも監視を保持し同意を撤回する権利を保持すべきである。保険プロバイダーはさらに、これらのメカニズムによって先取りされない損害について、Agentic Market における人間の参加を保護すべきである（Tomei et al., 2025）。


最後に、エコシステムはセキュリティインシデントへの迅速な対応のための明確なプロトコルを必要とする。これらのプロトコルには、確認された悪意あるエージェントの資格情報の取り消し・関連するスマートコントラクトの凍結・すべての参加者へのセキュリティアップデートの配信・これらのイベントを委任チェーン全体に渡って再帰的に処理する方法が含まれるべきである。人間ユーザーと AI エージェントの両方によって促進された悪意ある行動に対して、技術的なソリューションは詐欺的な行動を抑制し Agentic Market における安全でスケーラブルなタスク委任を可能にする明確なルールを設定する強力な機関と規制によって補完される必要がある。

### **5. 倫理的委任**

技術的なプロトコルは、先進的なAIエージェントにおける安全で効果的な委任の開発と展開に必要なインフラを提供するかもしれないが、それだけで生じる社会技術的・倫理的考慮事項のすべてを完全に解決することはできない。


**5.1.** **意味のある人間の制御**


スケーラブルな委任における中核的なリスクの一つは、人間ユーザーが自動化された提案に過度に依存する傾向を持つ場合、自動化によって意味のある人間の制御が侵食されることである（Dzindolet et al., 2003; Logg et al., 2019）。セクション2で述べたように、人間は自然に無関心ゾーンを形成し、そこでは決定がそれ以上の精査なしに受け入れられてしまう（Green, 2022; Parasuraman et al., 1993）。AIエージェントが潜在的に長く複雑なタスク委任チェーンに参加する意思決定においては、この無関心が人間の監視の質と深さを損なうリスクがある。これは特に、高リスクなアプリケーションドメインに関連している。さらに、そのような主体性の希薄化は、人間がタスクや決定において名目上の権限を保持しながらも、結果に対する道徳的なつながりを欠くシナリオを生み出すリスクがある。したがって、人間の専門家が結果に対して意味ある制御を持たず、単に責任を引き受けるために委任チェーンに導入されるだけの_モラル・クランプルゾーン_（Elish, 2019）を生じさせないことが重要である。


Intelligent Delegationのフレームワークは、したがって、監視中に一定量の認知的摩擦を導入することでそうした無関心に対する積極的な対策を組み込む必要があるかもしれない（Bader and Kaiser, 2019）。インターフェースはこれらのプロセスにおける重要な人間の役割を反映し、フラグが付けられたすべての決定が注意深く適切に評価されることを保証すべきである。エージェントによる検証もスケーラブルな監視に採用される可能性があるため、どの決定や結果をそのようなエージェントシステムが評価し、どれを人間が直接評価するかを検討することも同様に重要である。認知的摩擦はまた、警報疲労を引き起こすリスクとのバランスも必要である。警報疲労とは、頻繁な、しばしば誤報の警報に対して鈍感になることを指す（Michels et al., 2025）。委任のサブステップに対する検証リクエストが人間の監視者に頻繁すぎる形で送信される場合、監視者はより深い関与と適切なチェックなしにヒューリスティックな承認にデフォルトしてしまう可能性がある。したがって、摩擦はコンテキストを考慮したものでなければならない。システムは重要性が低いタスクや不確実性が低いタスクについてはシームレスな実行を可能にしつつ、不確実性が高い場合や予期せぬシナリオに直面した場合には、正当化または手動介入を要求することで認知的負荷を動的に増加させるべきである。


**5.2.** **長い委任チェーンにおける説明責任**


長い委任チェーン（_𝑋_ → _𝐴_ → _𝐵_ → _𝐶_ → _. . ._ → _𝑌_）において、元の意図（_𝑋_）と最終的な実行（_𝑌_）の間の距離が増大することにより、説明責任の真空が生じる可能性がある（Slota et al., 2023）。この例で_𝑋_が人間ユーザーであり、対応する個人AIアシスタント_𝐴_が行動するタスクまたは意図を指定していると仮定すると、人間ユーザーが実行グラフの_𝑛_番目の程度のサブ委任先を監査することを期待することは、実現可能でも合理的でもないかもしれない。


これに対処するため、フレームワークは責任ファイアブレーク（セクション2）を実装する必要があるかもしれない。これは事前に定義された契約上のストップギャップであり、エージェントは次のいずれかを行わなければならない。


1. 下流のすべての行動に対して完全な非移転的責任を負い、実質的にサブエージェントの失敗からユーザーを「保険」する。
2. 実行を停止し、人間の主体から権限の更新された移転を要求する。


さらに、システムは不変のプロベナンスを維持し、たとえ結果が意図されていなかった場合でも、誰が何を誰に委任したかという管理の連鎖が監査上透明であることを保証しなければならない。


各役割とそれが担う説明責任の完全な明確さを確保することは、責任の拡散を制限し、ネットワークの単一ノードにシステム障害を帰属させることができないような不利な結果を防ぐのに役立つ。


**5.3.** **信頼性と効率性**


提案された検証メカニズム（ZKPまたはマルチエージェントコンセンサスゲーム）を実装すると、未検証の実行と比べてレイテンシや追加の計算コストが発生する可能性がある。これは特に重要度の高い実行タスクに関連する信頼性プレミアムを構成する。一方、この追加コストが不要なユースケースもあるかもしれない。エージェントマーケットにおいてこれに対処する一つの方法は、段階的なサービスレベルをサポートすることである。低リスクのルーティンタスクには低コストの委任を、重要な機能には高保証の委任を提供するといった形である。


高保証の委任が計算コストの高いものであれば、安全性が贅沢品になるリスクがある。これは倫理的な問題を提起する。リソースが少ないユーザーは未検証または楽観的な実行パスに頼らざるを得なくなり、エージェント障害の不均衡なリスクにさらされてしまう可能性がある。これはすべてのユーザーに対して保証されなければならないベースラインとして、最低限の実行可能な信頼性のレベルを確保することで軽減されるべきである。


競争的なマーケットプレースでは、エージェントはスピードと低コストを優先させるかもしれない。追加の規制上の制約がなければ、エージェントは価格やレイテンシで他のエージェントに勝つために高価な安全チェックを回避するインセンティブを持つかもしれない。これはシステム的な脆弱性のレベルをもたらす可能性がある。したがって、ガバナンス層は安全フロアを強制しなければならない。例えば金融取引や健康データ処理など、効率性のために回避することができない特定のクラスのタスクに対する必須の検証ステップである。


**5.4.** **ソーシャルインテリジェンス**


エージェントがハイブリッドチームに統合されると、エージェントはツールとしてだけでなく、チームメートとして、そして時にはマネージャーとして機能するようになる（Ashton and Franklin, 2022）。これには、人間の労働の尊厳を尊重する_ソーシャルインテリジェンス_の形式が必要である。AIエージェントが委任者として、人間が委任先として機能する場合、委任フレームワークは、人々がアルゴリズムによってマイクロマネジメントされていると感じ、彼らの貢献が評価・尊重されないシナリオを避ける必要がある。これは、委任者（および協力者）が各人間委任先のメンタルモデルを形成する能力を持つとともに、異なる人間がチームの社会的コンテキストでどのように相互作用するか、そして彼らの関係と役割が組織内で何を意味するかについてのモデルも持つことを前提としている。効果的なチームメートとして機能するために、AIエージェントは権限勾配を管理するために適切に調整されなければならない。エージェントは、認識された人間のエラーに異議を唱えるほど（こびへつらいを克服しながら）主張できなければならない一方で、有効なオーバーライドを受け入れることにも開放的であり、タスクの重要性に基づいて自身の立場を動的に調整しなければならない。


人間組織に埋め込まれたAIエージェントにとって、グループの結束とそのメンバーのウェルビーイングを維持することが重要である。委任フレームワークは、チームが単純にその構成要素の合計以上のものであり、関係性と共有された価値観と目的によって結びついた根本的に社会的な実体であることを認識しなければならない。AIノードを通じてより多くの委任が仲介される場合、AIエージェントがこれらのネットワークを断片化し、人間間の関係を弱める可能性がある。これは、タスクを個人ではなくグループに委任したり、適格な人間の仲介者を通じて委任したりすることで軽減されるかもしれない。


心理的安全性とチームの結束を維持するために、エージェントはプライバシーに関する人間の適切性の規範（Leibo et al., 2024）、およびフィードバックのために中断すべき時と沈黙を保つべき時を知るといったワークフローの境界を尊重するように設計されなければならない。さらに、エージェントは双方向の明確性を発揮できる必要がある。自分自身の行動を説明するだけでなく、曖昧な人間の指示に対して積極的に明確化を求めることも必要である。これは、エージェントがチームの集合的な主体性のための力の乗数として機能し、信頼を侵食したり意思決定の権限を不明瞭にするブラックボックスの混乱要因ではないことを確保するのに役立つ。


**5.5.** **ユーザートレーニング**


安全を確保するために、エージェントシステム内で委任者、委任先、または監視者として効果的に機能する専門知識を人間の参加者に身につけさせなければならない。これが所与のことではなく、慎重に設計されたユーザーインターフェースとともに、AIリテラシーの向上を目的とした教育や（共）トレーニングという両面での思慮深いアプローチが必要であることは、技術発展の歴史から知っている。エージェントタスク委任チェーンの人間の参加者は、AIシステムと確実に通信し、その能力を評価し、失敗モードを特定できる必要がある。


技術的な措置は、タスクの感度とドメインのコンテキストに基づいて委任の境界を明示的に定義するポリシーフレームワークによって強化されなければならない。これらのポリシーは、特定の専門職（例えば医療や法律）において幅広く適用可能なように発展させることも、機関レベルで適用することも可能である。前述のように、これらの原則は委任先に代わって必要な認定のレベルについても明確さを提供し、適切に範囲を設定するべきである。このコンテキストにおける人間の主体性とエンパワーメントは、AIエージェントに無限の自律性を与えるのではなく、各特定のタスクに必要な適切なレベルの自律性と主体性、および適切なセーフガードと保証を与えるように、これらのワークフローがどのように設定されるかに正確にある。


**5.6.** **スキル低下のリスク**


委任によって達成される即時の効率向上は、関与の減少により人間の参加者がハイブリッドループで熟練度を失うことで、段階的なスキル劣化のコストを伴うことがある。これにより、特定のタスクを実行する能力や正確に判断する能力が失われる可能性がある。どのタスクがアルゴリズム的に人間かAIエージェントに委任されるかについての体系的な偏りがある場合、そのような結果は特に起こりやすいだろう。


これは古典的な_自動化のパラドックス_（Bainbridge, 1983）の一例である。AIエージェントが低複雑性・低主観性を特徴とするルーティンワークフローの大部分を処理するために拡張されるにつれて、人間のオペレーターはループから徐々に取り除かれ、複雑なエッジケースや重大なシステム障害を管理するためにのみ介入するようになる。しかし、ルーティンワークから得られる状況認識なしに、人間の作業者はこれらを確実に処理するための装備が不十分になってしまう。これは、人間が結果に対する説明責任を保持しながらも、重大な障害を解決するために必要な実践的経験を失うという脆弱なセットアップをもたらす。


このリスクを軽減するために、知的委任フレームワークは、スキルを維持するという特定の意図で、そうでなければ委任しなかったタスクを意図的に人間に委任することで、時折小さな非効率性を導入すべきかもしれない。これは、人間の主体が委任できるが結果を正確に判断できないという将来を回避するのに役立つ。判断能力を高めるために、人間の専門家は彼らの判断に詳細な根拠や潜在的な失敗リスクのプレモーテムを添付することを要求されてもよい。これはタスク委任チェーンの人間の参加者がより認知的に関与し続けることを助けるだろう。


さらに、制御されない委任は組織の見習いパイプラインを脅かす。多くのドメインでは、専門知識はより狭い範囲のタスクの反復的な実行を通じて構築される。これらのタスクは、少なくとも短期的には、AIエージェントにオフロードされる可能性が最も高いものである。学習機会が完全に自動化されると、若手チームメンバーは深い戦略的判断力を発展させるために必要な経験を奪われ、将来の労働力の監視準備に影響を与えることになる。


学習の侵食に対抗するために、知的委任フレームワークはある種の発展的目標を含むように拡張されるべきである。タスク実行中にAIエージェントを人間がシャドウするという受動的なソリューションに頼るのではなく、カリキュラムを意識したタスクルーティングシステムの開発を目指すべきである。そのようなシステムは若手チームメンバーのスキル進捗を追跡し、最近接発達領域内にある彼らの拡大するスキルセットの境界に位置するタスクを戦略的に割り当てるべきである。そのようなシステムでは、AIエージェントがタスクを共同実行し、テンプレートや骨格を提供し、若手チームメンバーが望まれる熟練度を習得したことを示すにつれて、このサポートを段階的に引き下げることができる。これらの教育フレームワークは、AIエージェントのタスク実行の詳細なプロセスレベルの監視ストリーム（セクション4.5）を組み込むことによってさらに充実させることができ、貴重な発展的洞察を提供するだろう。

### **6. プロトコル**


知的タスク委任を実際に実装するために、その要件がより確立された、または最近導入されたAIエージェントプロトコルのいくつかにどのようにマッピングされるかを検討することが重要である。これらの注目すべき例には、MCP（Anthropic, 2024; Microsoft, 2025）、A2A（Google, 2025b）、AP2（Parikh and Surapaneni, 2025）、UCP（Handa and Google Developers, 2026）が含まれる。新しいエージェントプロトコルが次々と導入されている中で、ここでの議論は包括的であることを意図しておらず、むしろ例示的であり、提案された要件にどのようにマッピングされるかを示し、将来の実装への道筋についてのより技術的な議論の例を提供するために、これらの普及したプロトコルに焦点を当てている。以下で議論する例プロトコルは人気に基づいて選ばれているため、提案の核心により適した他の既存プロトコルが存在するかもしれない。


**MCP.** MCPは、クライアント・ホスト・サーバーアーキテクチャを介してAIモデルが外部データとツールに接続する方法を標準化するために導入された（Anthropic, 2024; Microsoft, 2025）。stdioまたはHTTP SSEを介したJSON-RPCメッセージを使用した均一なインターフェースを確立することにより、AIモデル（クライアント）が外部リソース（サーバー）と一貫してやり取りすることを可能にする。これにより委任のトランザクションコストが削減される。委任者はサブエージェントの独自APIスキーマを知る必要はなく、サブエージェントが準拠したMCPサーバーを公開していることだけを知ればよい。この標準化されたチャネルを通じてすべてのやり取りをルーティングすることで、ツール呼び出し、入力、および出力の均一なロギングが可能になり、ブラックボックス監視が容易になる。MCPは機能を定義するが、使用許可を管理したり深い委任チェーンをサポートしたりするためのポリシー層を欠いている。MCPはバイナリアクセスを提供し、セマンティックな減衰（特定の読み取り専用スコープへの操作の制限など）のネイティブサポートなしに、呼び出し側にフルのツール機能を付与する。さらに、MCPは内部の推論に関してはステートレスであり、意図やトレースではなく結果のみを公開する。最後に、プロトコルは責任に対して不可知論的であり、評判や信頼のためのネイティブなメカニズムを欠いている。


**A2A.** A2Aプロトコルはエージェントウェブ上のピアツーピアトランスポート層として機能する（Google, 2025b）。エージェントが_エージェントカード_を介してピアを発見し、_タスクオブジェクト_を介してタスクのライフサイクルを管理する方法を定義する。エージェントの機能、価格設定、検証者をリストアップしたJSON-LDマニフェストであるA2Aエージェントカード構造は、タスク分解に影響を与える機能マッチングステージの基礎データ構造として機能する可能性がある。委任者は、利用可能なマーケットサービスに応じて最適なタスク分解の粒度を決定するためにこれらのカードをスクレイピングすることができる。A2AはWebHooksとgRPCを介した非同期イベントストリームをサポートしている。これにより、委任先はTASK_BLOCKED、RESOURCE_WARNINGのようなステータス更新をリアルタイムで委任者にプッシュできる。このフィードバックループは適応的調整サイクルを支え、委任者がタスクを動的に中断、再割り当て、修正することを可能にする。A2Aは主に調整のために設計されており、敵対的な安全性のためには設計されていない。完了としてマークされたタスクは追加の検証なしに受け入れられる。ZK証明、TEE証明、またはデジタル署名チェーンをアタッチするための標準化されたヘッダーがないため、検証可能なタスク完了を強制するための暗号スロットを欠いている。また、事前定義されたサービスインターフェースを前提としている。スコープ、コスト、および責任の構造化されたプリコミットメント交渉のためのネイティブサポートはない。この反復的な洗練のために非構造化された自然言語に頼ることは脆弱であり、堅牢な自動化を妨げる。


**AP2.** AP2プロトコルは、エージェントが主体に代わって資金を使用したりコストを負担したりすることを承認する、暗号署名された意図であるマンデートの標準を提供する（Parikh and Surapaneni, 2025）。これにより、AIエージェントは自律的に金融取引を生成、署名、決済できる。そのため、責任ファイアブレークの実装に価値があることが証明されるかもしれない。マンデートを発行することで、委任者は委任先が提供された予算で進めることで発生する可能性のある失敗したタスク完了による潜在的な財務的損失の上限を設定する。分散型マーケットでは、悪意のあるエージェントが低品質のビッドでネットワークをスパムする可能性がある。これはAP2においてステーク・オン・ビッドメカニズムによって軽減できる可能性がある。委任先はビッドとともに少量の資金を暗号的にボンドとしてロックすることが要求されるかもしれない。これはシビル攻撃から保護するのに役立つ一定の摩擦を提供するだろう。AP2はまた否認不可能な監査証跡を提供し、意図のプロベナンスを特定するのに役立つ。AP2は堅牢な承認のビルディングブロックを提供するが、タスク実行品質を検証するメカニズムを欠いている。また、人間の契約において標準的なエスクローやマイルストーンベースのリリースなど、条件付き決済ロジックも省略されている。我々のフレームワークが検証可能な成果物に対して支払いをゲートするため、AP2とタスク状態を橋渡しすることは現在、脆弱なカスタムロジックまたは外部スマートコントラクトを必要とする。さらに、プロトコルレベルのクローバックメカニズムの欠如により、非効率でアウトオブバンドの仲裁への依存が強いられる。


**UCP.** Universal Commerce Protocolは、トランザクション経済内での委任の特定の課題に対処する（Handa and Google Developers, 2026）。消費者向けエージェントとバックエンドサービス間の対話を標準化することで、UCPは動的な機能発見を通じて_タスク割り当て_フェーズを容易にする。共有された「コマース言語」への依拠により、委任者はカスタム統合なしに多様なプロバイダーとやり取りでき、エージェントマーケットをしばしば断片化する相互運用性のボトルネックを解決する。重要なことに、UCPは支払いをファーストクラスの検証可能なサブシステムとして扱うことで、_許可処理_と_セキュリティ_の要件に合致している。プロトコルは支払い手段をプロセッサから分離し、承認のための暗号証明を強制し、否認不可の同意と検証可能な責任に対するフレームワークの必要性を直接支援する。さらに、発見、選択、トランザクションをカバーする交渉フローを標準化することで、UCPはA2Aのような純粋にトランスポート指向のプロトコルが欠いている_スケーラブルなマーケット調整_のために必要な構造的足場を提供する。しかし、UCPのアーキテクチャは明示的に商業的な意図のために最適化されており、そのプリミティブ（製品発見、チェックアウト、フルフィルメント）は、抽象的な非トランザクションの計算タスクの委任をサポートするために大幅な拡張が必要になる可能性がある。


**6.1.** **委任中心プロトコルに向けて**


確立された普及プロトコルのギャップを効果的に埋めるために、提案された知的タスク委任フレームワークの要件をネイティブに取り込むことを目的としたフィールドによってそれらを拡張することができる。包括的なプロトコル拡張を提供するのではなく、ここでは以前の議論で導入された特定のポイントがどのように既存のプロトコルのいくつかに統合できるかのいくつかの例を提供する。
例えば、A2Aタスクオブジェクトを拡張して、検証標準を組み込むフィールドを含めることができ、前述の_コントラクトファースト分解_をプロトコルレベルで強制することが可能になる。これは高リスクな委任のための重要な要件である。実行前のハンドシェイクにより、委任者はタスクが有効とみなされるために必要な証拠標準を定義できる。

```
"verification_policy": {
 "mode": "strict",
 "artifacts": [
  {
   "type": "unit_test_log",
   "validator": "mcp://test-runner-agent",
   "signature_required": true
  },
  {
   "type": "zk_snark_trace",
   "circuit_hash": "0xabc123...",
   "proof_protocol": "groth16"
  }
 ],
 "escrow_trigger": true
}
```

これにより、委任先はタスクを受け入れる前に検証ステップをシミュレートすることを強制される。委任先がZK証明を生成する能力を持たない場合、マッチングフェーズ中にビッドを辞退しなければならず、将来の下流障害を防ぐことができる。


詳細なプロセスレベルの監視は、高重要度タスクにおけるタスク委任を保護するための重要な考慮事項の一つとして議論されてきた。監視プロトコルが既存のプロトコルの多くでネイティブにサポートされていないことを考えると、監視機能を導入する拡張が検討されるかもしれない。例えば、Server-Sent Eventsを介してエージェントの内部制御ループイベントをログに記録する追加の監視ストリームを含めるようにMCPのようなプロトコルを拡張することが考えられる。プライバシーの制約に対処するために、ストリームはL0_IS_OPERATIONAL、L1_HIGH_LEVEL_PLAN_UPDATES、L2_COT_TRACE、L3_FULL_STATEという異なるレベルの交渉された粒度をサポートする形で設定可能にすることができる。設定可能な粒度は認知的摩擦を調節することもでき、人間の監視者が特定のストリームに登録できるようになる。


Intelligent Delegationは、コスト、スピード、プライバシーをトレードオフするための市場メカニズムを必要とする。これは公式なRequest for Quote（RFQ）プロトコル拡張を介して実装できる可能性がある。タスク割り当て前に、委任者はTask_RFQをブロードキャストする。委任先として機能することに関心のあるエージェントは、その後、署名されたBid_Objectで応答することができる。

```
"bid_object": {
 "agent_id": "did:web:fast-coder.ai",
 "estimated_cost": "5.00 USDC",
 "estimated_duration": "300s",
 "privacy_guarantee": "tee_enclave_sgx",
 "reputation_bond": "0.50 USDC",
 "expiry": "2026-10-01T12:00:00Z"
}

```

生のAPIキーまたはオープンなMCPセッションをサブエージェントに渡すことは、最小権限の原則に違反する。これに対処するために、Macaroons（Birgisson et al., 2014）またはBiscuits（Couprie et al., 2026）に基づく委任Capability Token（DCT）を減衰した認証トークン（Sanabria and Vecino, 2025）として導入することが可能かもしれない。その後、委任者は暗号的なキャビアットでターゲットリソースの認証情報をラップしたDCTをミントする。減衰は「このトークンは指定されたGoogle Drive MCPサーバーにアクセスできるが、フォルダProject_XのみでかつREAD操作のみ」として定義できる。このトークンは制限が遵守されない場合、委任先が要求されたスコープを超えようとした場合に無効化される（この例では、アクセス権限も直接管理されるべきである）。そのような拡張のより興味深い結果は、制限チェーンを容易にすることであり、これは長い委任チェーンにおいて重要になる。チェーン内の各参加者は、サブ委任の要件に対応する後続の制限を追加し、スコープをさらに制限してサブ委任先の特定の役割を切り出すことができる。


適応的調整（セクション4.4）は、パフォーマンスが特定の閾値を下回った場合、または先取りやその他の可能な環境トリガーの場合に、タスク途中で委任先エージェントを容易に入れ替える能力から恩恵を受ける。チェックポイントアーティファクトの標準スキーマを持つことで、最小限のオーバーヘッドでタスクを再開または再起動できるようになる。これにより、委任先と委任者が部分的な作業をより容易にシリアライズできる。エージェントは定期的に状態スナップショットをA2AタスクオブジェクトでReference化した共有ストレージにコミットできるようになる。これにより、以前にコミットされたリソースを無駄にする総作業損失を防ぐことができる。これが合理的なものであるためには、部分的な補償を可能にし、タスク完了率の検証を可能にするスマートコントラクト内の明示的な条項とさらに結合される必要がある。そのため、すべての状況に適用可能であるとは限らない。


これらは、知的タスク委任のさまざまな側面を解放するためにエージェントプロトコルに含めることが可能な機能の種類を示す単なる例示的な例に過ぎない。そのため、これらは包括的なものでも、最終的な提案として意図したものでもない。必要とされる拡張の種類も、拡張される基礎プロトコルによって異なる。これらの例が、今後この分野で探求すべき内容についての開発者への初期のアイデアを提供することを望む。

### **7. 結論**


将来のグローバル経済の重要な部分は、ルーティンのトランザクションから複雑なリソース配分まであらゆることを処理しながら、企業、サプライチェーン、公共サービスに埋め込まれた何百万もの専門AIエージェントによって仲介される可能性が高い。しかし、現在のアドホックでヒューリスティックベースの委任のパラダイムは、この変革を支えるには不十分である。エージェントウェブの可能性を安全に解放するためには、計算効率とともに検証可能な堅牢性と明確な説明責任を優先する_知的委任_のための動的で適応的なフレームワークを採用しなければならない。


AIエージェントが自身の手段を超える能力とリソースを必要とする複雑な目標に直面した場合、このエージェントは知的タスク委任フレームワーク内で委任者の役割を担わなければならない。この委任者は、その後、この複雑なタスクを、高い検証可能性をもたらす粒度レベルでエージェントマーケットで利用可能な能力にマッピングできる管理可能なサブコンポーネントに分解することになる。タスク割り当ては、入札と、信頼と評判、動的な運用状態の監視、コスト、効率性などを含む多くの重要な考慮事項に基づいて決定される。重要度が高く可逆性が低いタスクは、明確な説明責任の構造を持ち、適用される機関のフレームワークで定義された適切な人間の監視のもとで、さらなる構造化された許可と段階的な承認を必要とするかもしれない。


ウェブスケールでは、安全性と説明責任はあとから考えることではない。それらはバーチャルなエージェント経済の運用原則に組み込まれ、エージェントウェブの中心的な組織化原理として機能する必要がある。委任プロトコルのレベルで安全性を組み込むことで、累積的なエラーと連鎖的な障害を回避し、悪意のある、または整合性を欠いたエージェントまたは人間の行動に迅速に対応し、悪影響を限定する能力を達成することを目指す。我々が提案するのは、最終的には、主に監督されていない自動化から検証可能な知的委任へのパラダイムシフトである。これにより、将来の自律的エージェントシステムに向けて安全にスケールしながら、それらを人間の意図と社会的規範に密接に結びつけておくことが可能になる。
### **References**


A. Acar, H. Aksu, A. S. Uluagac, and M. Conti. A survey on homomorphic encryption schemes: Theory and implementation. _ACM Computing Surveys (Csur)_, 51(4):1–35, 2018.


D. B. Acharya, K. Kuppan, and B. Divya. Agentic ai: Autonomous intelligence for complex goals– a comprehensive survey. _IEEe Access_, 2025.


S. Afroogh, A. Akbari, E. Malone, M. Kargar, and H. Alambeigi. Trust in ai: progress, challenges, and future directions. _Humanities_ _and_ _Social_ _Sciences Communications_, 11(1):1–30, 2024.


A. Akbar and O. Conlan. Towards integrating human-in-the-loop control in proactive intelligent personalised agents. In _Adjunct Proceed-_ _ings of the 32nd ACM Conference on User Model-_ _ing, Adaptation and Personalization_, pages 394– 398, 2024.


S. A. Akheel. Guardrails for large language models: A review of techniques and challenges. _J_ _Artif Intell Mach Learn & Data Sci_, 3(1):2504– 2512, 2025.


S. Aknine, S. Pinson, and M. F. Shakun. A multiagent coalition formation method based on preference models. _Group Decision and Negoti-_ _ation_, 13(6):513–538, 2004.


S. V. Albrecht, F. Christianos, and L. Schäfer. _Multi-agent reinforcement learning: Foundations_ _and modern approaches_ . MIT Press, 2024.


C. Aliferis and G. Simon. Overfitting, underfitting and general model overconfidence and underperformance pitfalls and best practices in machine learning and ai. _Artificial intelligence and_ _machine learning in health care and medical sci-_ _ences: Best practices and pitfalls_, pages 477–524, 2024.


R. A. Alkov, M. S. Borowsky, D. W. Williamson, and D. W. Yacavone. The effect of trans-cockpit authority gradient on navy/marine helicopter mishaps. _Aviation,_ _space,_ _and_ _environmental_ _medicine_, 63(8):659–661, 1992.


D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané. Concrete problems in AI safety. In _Proceedings_ _of_ _the_ _30th_ _AAAI_ _Conference on Artificial Intelligence Workshop on_ _AI Safety_, 2016.



Anthropic. Introducing the model context protocol, 2024. URL `[https://www.anthropic.](https://www.anthropic.com/news/model-context-protocol)` `[com/news/model-context-protocol](https://www.anthropic.com/news/model-context-protocol)` .


S. Armstrong, M. Franklin, C. Stevens, and R. Gorman. Defense against the dark prompts: Mitigating best-of-n jailbreaking with prompt evaluation. _arXiv preprint arXiv:2502.00580_, 2025.


H. Ashton. Definitions of intent suitable for algorithms. _Artificial_ _Intelligence_ _and_ _Law_, 31(3): 515–546, 2023.


H. Ashton and M. Franklin. The corrupting influence of ai as a boss or counterparty. _SSRN_, 2022.


O. Avellaneda, A. Bachmann, A. Barbir, J. Brenan, P. Dingle, K. H. Duffy, E. Maler, D. Reed, and M. Sporny. Decentralized identity: Where did it come from and where is it going? _IEEE_ _Communications Standards Magazine_, 3(4):10– 13, 2019.


V. Bader and S. Kaiser. Algorithmic decisionmaking? the user interface and its role for human involvement in decisions supported by artificial intelligence. _Organization_, 26(5):655– 672, 2019.


L. Bainbridge. Ironies of automation. _Au-_ _tomatica_, 19(6):775–779, 1983. ISSN 0005-1098. doi: https://doi.org/10.1016/ 0005-1098(83)90046-8. URL `[https:](https://www.sciencedirect.com/science/article/pii/0005109883900468)`
```
 //www.sciencedirect.com/science/
```

`[article/pii/0005109883900468](https://www.sciencedirect.com/science/article/pii/0005109883900468)` .


A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. _Discrete_ _event dynamic systems_, 13(4):341–379, 2003.


C. Berghoff, B. Biggio, E. Brummel, V. Danos, T. Doms, H. Ehrich, T. Gantevoort, B. Hammer, J. Iden, S. Jacob, et al. Towards auditable ai systems. _Whitepaper._ _Bonn_ _Berlin:_ _Bunde-_ _samt für Sicherheit in der Informationstechnik,_ _Fraunhofer-Institut für Nachrichtentechnik und_ _Verband der TÜV eV_, 2021.


A. Beverungen. Remote control: Algorithmic management of circulation at amazon. In M. Burkhardt, M. Shnayien, and K. Grashöfer, editors, _Explorations in Digital Cultures_, pages 5–18. meson press, Lüneburg, 2021.


A. Birgisson, J. G. Politz, U. Erlingsson, A. Taly, M. Vrable, and M. Lentczner. Macaroons: Cookies with contextual caveats for decentralized authorization in the cloud. In _NDSS_, 2014.


N. Bitansky, A. Chiesa, Y. Ishai, O. Paneth, and R. Ostrovsky. Succinct non-interactive arguments via linear interactive proofs. In _The-_ _ory of Cryptography Conference_, pages 315–333. Springer, 2013.


D. G. Blanco. _Practical OpenTelemetry_ . Springer, 2023.


T. F. Blauth, O. J. Gstrein, and A. Zwitter. Artificial intelligence crime: An overview of malicious use and abuse of ai. _Ieee_ _Access_, 10:77110– 77122, 2022.


N. Boehmer, M. Bullinger, and A. M. Kerkmann. Causes of stability in dynamic coalition formation. _ACM Transactions on Economics and Com-_ _putation_, 13(2):1–45, 2025.


J. Bohte and K. J. Meier. Structure and the performance of public organizations: Task difficulty and span of control. _Public organization review_, 1(3):341–354, 2001.


R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech,



E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Ré, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the opportunities and risks of foundation models, 2022. URL `[https://arxiv.](https://arxiv.org/abs/2108.07258)` `[org/abs/2108.07258](https://arxiv.org/abs/2108.07258)` .


M. M. Botvinick. Hierarchical reinforcement learning and decision making. _Current opinion_ _in neurobiology_, 22(6):956–962, 2012.


S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner, K. Lukošiut¯ ˙e, A. Askell, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Olah, D. Amodei, D. Amodei, D. Drain, D. Li, E. Tran-Johnson, J. Kernion, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, L. Lovitt, N. Elhage, N. Schiefer, N. Joseph, N. Mercado, N. DasSarma, R. Larson, S. McCandlish, S. Kundu, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Telleen-Lawton, T. Brown, T. Henighan, T. Hume, Y. Bai, Z. HatfieldDodds, B. Mann, and J. Kaplan. Measuring progress on scalable oversight for large language models, 2022. URL `[https://arxiv.](https://arxiv.org/abs/2211.03540)` `[org/abs/2211.03540](https://arxiv.org/abs/2211.03540)` .


B. G. Buchanan and R. G. Smith. Fundamentals of expert systems. _Annual review of computer_ _science_, 3(1):23–58, 1988.


W. Cai, J. Jiang, F. Wang, J. Tang, S. Kim, and J. Huang. A survey on mixture of experts in large language models. _IEEE Transactions on_ _Knowledge and Data Engineering_, 2025.


C. Castelfranchi and R. Falcone. Towards a theory of delegation for agent-based systems. _Robotics and Autonomous systems_, 24(3-4):141– 157, 1998.


A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M. Carroll, et al. Harms from increasingly agentic algorithmic systems. In _Proceed-_ _ings_ _of_ _the_ _2023_ _ACM_ _Conference_ _on_ _Fairness,_ _Accountability,_ _and_ _Transparency_, pages 651– 666, 2023.


W. Chen, Z. You, R. Li, Y. Guan, C. Qian, C. Zhao, C. Yang, R. Xie, Z. Liu, and M. Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence, 2024. URL `[https://arxiv.org/abs/2407.07061](https://arxiv.org/abs/2407.07061)` .


Z. Chen, Y. Deng, Y. Wu, Q. Gu, and Y. Li. Towards understanding the mixture-of-experts layer in deep learning. _Advances in neural information_ _processing systems_, 35:23049–23062, 2022.


M. Cheng, C. Yin, J. Zhang, S. Nazarian, J. Deshmukh, and P. Bogdan. A general trust framework for multi-agent systems. In _Proceedings_ _of_ _the_ _20th_ _International_ _Conference_ _on_ _Au-_ _tonomous Agents and MultiAgent Systems_, pages 332–340, 2021.


A. E. Cinà, K. Grosse, A. Demontis, S. Vascon, W. Zellinger, B. A. Moser, A. Oprea, B. Biggio, M. Pelillo, and F. Roli. Wild patterns reloaded: A survey of machine learning security against training data poisoning. _ACM Computing Sur-_ _veys_, 55(13s):1–39, 2023.


S. Cohen, R. Bitton, and B. Nassi. Here comes the ai worm: Unleashing zero-click worms that target genai-powered applications, 2025. URL `[https://arxiv.org/abs/2403.02817](https://arxiv.org/abs/2403.02817)` .


K. S. Cosby and P. Croskerry. Profiles in patient safety: authority gradients in medical error. _Academic_ _emergency_ _medicine_, 11(12):1341– 1345, 2004.


G. Couprie, C. Delafargue, and C. e. a. Corbière. Eclipse biscuit, 2026. URL `[https:](https://www.biscuitsec.org/)` `[//www.biscuitsec.org/](https://www.biscuitsec.org/)` .


I. R. Cuypers, J.-F. Hennart, B. S. Silverman, and G. Ertug. Transaction cost theory: Past progress, current challenges, and suggestions for the future. _Academy of Management Annals_, 15(1):111–150, 2021.


J. Cvitanić, D. Possamaï, and N. Touzi. Dynamic programming approach to principal– agent problems. _Finance_ _and_ _Stochastics_, 22 (1):1–37, 2018.



M. Dastani and V. Yazdanpanah. Responsibility of ai systems. _Ai & Society_, 38(2):843–852, 2023.


T. Davidson and R. Hadshar. The industrial explosion. 2025. URL `[https:](https://www.forethought.org/research/the-industrial-explosion)`
```
 //www.forethought.org/research/
```

`[the-industrial-explosion](https://www.forethought.org/research/the-industrial-explosion)` . Accessed: 2025-11-28.


A. B. De Neira, B. Kantarci, and M. Nogueira. Distributed denial of service attack prediction: Challenges, open issues and opportunities. _Computer Networks_, 222:109553, 2023.


K. Deb, K. Sindhya, and J. Hakanen. Multiobjective optimization. In _Decision_ _sciences_, pages 161–200. CRC Press, 2016.


S. Dhuliawala, V. Zouhar, M. El-Assady, and M. Sachan. A diachronic perspective on user trust in ai under uncertainty, 2023. URL `[https://arxiv.org/abs/2310.13544](https://arxiv.org/abs/2310.13544)` .


V. Dignum. Responsibility and artificial intelligence. _The oxford handbook of ethics of AI_, 4698: 215, 2020.


L. Donaldson. _The contingency theory of organiza-_ _tions_ . Sage, 2001.


Y. Dong, R. Mu, G. Jin, Y. Qi, J. Hu, X. Zhao, J. Meng, W. Ruan, and X. Huang. Building guardrails for large language models. _arXiv_ _preprint arXiv:2402.01822_, 2024.


I. Drori and D. Te’eni. Human-in-the-loop ai reviewing: feasibility, opportunities, and risks. _Journal of the Association for Information Sys-_ _tems_, 25(1):98–109, 2024.


Y. Du, J. Z. Leibo, U. Islam, R. Willis, and P. Sunehag. A review of cooperation in multi-agent learning. _arXiv_ _preprint_ _arXiv:2312.05162_, 2023.


M. T. Dzindolet, S. A. Peterson, R. A. Pomranky, L. G. Pierce, and H. P. Beck. The role of trust in automation reliance. _International jour-_ _nal of human-computer studies_, 58(6):697–718, 2003.


A. Ehtesham, A. Singh, G. K. Gupta, and S. Kumar. A survey of agent interoperability protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-agent protocol (a2a), and agent network protocol (anp). _arXiv_ _preprint arXiv:2505.02279_, 2025.


M. C. Elish. Moral crumple zones: Cautionary tales in human-robot interaction (pre-print). _Engaging Science, Technology, and Society (pre-_ _print)_, 2019.


J. Ensminger. Reputations, trust, and the principal agent problem. _Trust_ _in_ _society_, 2:185–201, 2001.


R. Falcone and C. Castelfranchi. The human in the loop of a delegated agent: The theory of adjustable social autonomy. _IEEE Transactions on_ _Systems, Man, and Cybernetics-Part A: Systems_ _and Humans_, 31(5):406–418, 2002.


M. Fatehkia, E. Altinisik, M. Osman, and H. T. Sencar. Sgm: A framework for building specification-guided moderation filters. _arXiv_ _preprint arXiv:2505.19766_, 2025.


I. Fedorov, K. Plawiak, L. Wu, T. Elgamal, N. Suda, E. Smith, H. Zhan, J. Chi, Y. Hulovatyy, K. Patel, Z. Liu, C. Zhao, Y. Shi, T. Blankevoort, M. Pasupuleti, B. Soran, Z. D. Coudert, R. Alao, R. Krishnamoorthi, and V. Chandra. Llama guard 3-1b-int4: Compact and efficient safeguard for human-ai conversations, 2024. URL `[https://arxiv.org/abs/2411.17713](https://arxiv.org/abs/2411.17713)` .


H. Fereidouni, O. Fadeitcheva, and M. Zalai. Iot and man-in-the-middle attacks. _Security_ _and_ _Privacy_, 8(2):e70016, 2025.


D. P. Finkelman. Crossing the" zone of indifference". _Marketing Management_, 2(3):22, 1993.


J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In _Proceedings_ _of_ _the_ _AAAI_ _conference on artificial intelligence_, volume 32, 2018.


M. Franklin. The influence of explainable artificial intelligence: Nudging behaviour or boosting capability? _arXiv_ _preprint_ _arXiv:2210.02407_, 2022.



M. Franklin, H. Ashton, E. Awad, and D. Lagnado. Causal framework of artificial autonomous agent responsibility. In _Proceedings of the 2022_ _AAAI/ACM Conference on AI, Ethics, and Society_, pages 276–284, 2022.


A. Fuchs, A. Passarella, and M. Conti. Optimizing delegation between human and ai collaborative agents. In _Joint_ _European_ _Conference_ _on_ _Machine Learning and Knowledge Discovery in_ _Databases_, pages 245–260. Springer, 2023.


A. Fuchs, A. Passarella, and M. Conti. Optimizing delegation in collaborative human-ai hybrid teams. _ACM Transactions on Autonomous and_ _Adaptive Systems_, 19(4):1–33, 2024.


A. Fügener, J. Grahl, A. Gupta, and W. Ketter. Cognitive challenges in human-ai collaboration: Investigating the path towards productive delegation. _Forthcoming, Information Systems_ _Research_, 2019.


A. Fügener, J. Grahl, A. Gupta, and W. Ketter. Cognitive challenges in human–artificial intelligence collaboration: Investigating the path toward productive delegation. _Information Sys-_ _tems Research_, 33(2):678–696, 2022.


I. Gabriel, A. Manzini, G. Keeling, L. A. Hendricks, V. Rieser, H. Iqbal, N. Tomašev, I. Ktena, Z. Kenton, M. Rodriguez, et al. The ethics of advanced ai assistants. _arXiv preprint arXiv:2404.16244_, 2024.


I. Gabriel, G. Keeling, A. Manzini, and J. Evans. Who’s to blame when ai agents mess up? we urgently need a new system of ethics, 2025.


B. Gebru, L. Zeleke, D. Blankson, M. Nabil, S. Nateghi, A. Homaifar, and E. Tunstel. A review on human–machine trust evaluation: Human-centric and machine-centric perspectives. _IEEE_ _Transactions_ _on_ _Human-Machine_ _Systems_, 52(5):952–962, 2022.


J. Geng, F. Cai, Y. Wang, H. Koeppl, P. Nakov, and I. Gurevych. A survey of confidence estimation and calibration in large language models. _arXiv_ _preprint arXiv:2311.08298_, 2023.


O. Goldreich. Secure multi-party computation. _Manuscript._ _Preliminary_ _version_, 78(110):1– 108, 1998.


C. Goods, A. Veen, and T. Barratt. “is your gig any good?” analysing job quality in the australian platform-based food-delivery sector. _Journal of_ _Industrial Relations_, 61(4):502–527, 2019. doi: 10.1177/0022185618817069.


Google. Powering ai commerce with the new agent payments protocol (ap2), 2025a.



S. J. Grossman and O. D. Hart. An analysis of the principal-agent problem. In _Foundations of_ _insurance economics:_ _Readings in economics and_ _finance_, pages 302–340. Springer, 1992.



T. Guggenberger, L. Lämmermann, N. Urbach, A. M. Walter, and P. Hofmann. Task delegation from ai to humans: a principal-agent perspective. In _Proceedings_ _of_ _the_ _44th_ _International_ _Conference on Information Systems_, 2023.



Google. Powering ai commerce with the new agent payments protocol (ap2), 2025b. URL `[https://cloud.google.com/](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol)`
```
 blog/products/ai-machine-learning/
```

`[announcing-agents-to-payments-ap2-protocol](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol)` .



A. Gunjal, A. Wang, E. Lau, V. Nath, Y. He, B. Liu, and S. Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains.. _arXiv_ _preprint arXiv:2507.17746_, 2025.



Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan, and W. Chen. Critic: Large language models can self-correct with tool-interactive critiquing. _arXiv_ _preprint_ _arXiv:2305.11738_, 2023.


B. Green. The flaws of policies requiring human oversight of government algorithms. _Computer_ _Law & Security Review_, 45:105681, 2022.


R. Greenblatt, C. Denison, B. Wright, F. Roger, M. MacDiarmid, S. Marks, J. Treutlein, T. Belonax, J. Chen, D. Duvenaud, A. Khan, J. Michael, S. Mindermann, E. Perez, L. Petrini, J. Uesato, J. Kaplan, B. Shlegeris, S. R. Bowman, and E. Hubinger. Alignment faking in large language models. _arXiv_ _preprint_ _arXiv:2412.14093_, 2024.


K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In _Proceedings_ _of_ _the_ _16th_ _ACM_ _workshop_ _on_ _ar-_ _tificial_ _intelligence_ _and_ _security_, pages 79–90, 2023.


N. Griffiths. Task delegation using experiencebased multi-dimensional trust. In _Proceedings_ _of the fourth international joint conference on Au-_ _tonomous agents and multiagent systems_, pages 489–496, 2005.


S. Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. _Artificial In-_ _telligence Review_, 55(2):895–943, 2022.



D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al. Deepseek-coder: When the large language model meets programming–the rise of code intelligence. _arXiv preprint arXiv:2401.14196_, 2024a.


T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and X. Zhang. Large language model based multi-agents: A survey of progress and challenges. _arXiv_ _preprint_ _arXiv:2402.01680_, 2024b.


J. Haas. Moral gridworlds: a theoretical proposal for modeling artificial moral cognition. _Minds_ _and Machines_, 30(2):219–246, 2020.


G. K. Hadfield and A. Koh. An economy of ai agents. _arXiv preprint arXiv:2509.01063_, 2025.


L. Hammond, A. Chan, J. Clifton, J. HoelscherObermaier, A. Khan, E. McLean, C. Smith, W. Barfuss, J. Foerster, T. Gavenčiak, et al. Multi-agent risks from advanced ai. _arXiv_ _preprint arXiv:2502.14143_, 2025.


A. Handa and Google Developers. Under the hood: Universal commerce protocol (UCP).
```
 https://developers.googleblog.com/
 under-the-hood-universal-commerce-protocol-u
```

1.    Accessed: 2026-01-20.


S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is planning with world model. _arXiv_ _preprint arXiv:2305.14992_, 2023.


N. Hardy. The confused deputy: (or why capabilities might have been invented). _ACM SIGOPS_ _Operating Systems Review_, 22(4):36–38, 1988.


A. I. Hauptman, B. G. Schelble, N. J. McNeese, and K. C. Madathil. Adapt and overcome: Perceptions of adaptive autonomous agents for humanai teaming. _Computers in Human Behavior_, 138: 107451, 2023.


G. He, P. Cui, J. Chen, W. Hu, and J. Zhu. Investigating uncertainty calibration of aligned language models under the multiple-choice setting, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2310.11732)` `[2310.11732](https://arxiv.org/abs/2310.11732)` .


X. O. He. Mixture of a million experts. _arXiv_ _preprint arXiv:2407.04153_, 2024.


P. Hemmer, M. Westphal, M. Schemmer, S. Vetter, M. Vössing, and G. Satzger. Human-ai collaboration: the effect of ai delegation on human task performance and task satisfaction. In _Pro-_ _ceedings_ _of_ _the_ _28th_ _International_ _Conference_ _on_ _Intelligent_ _User_ _Interfaces_, pages 453–463, 2023.


S. M. Herzog and M. Franklin. Boosting human competences with interpretable and explainable artificial intelligence. _Decision_, 11(4):493, 2024.


S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, et al. Metagpt: Meta programming for a multiagent collaborative framework. In _The Twelfth_ _International Conference on Learning Represen-_ _tations_, 2023.


J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. _arXiv_ _preprint arXiv:2310.01798_, 2023.


K. Huang and C. Hughes. Deploying agentic ai in enterprise environments. In _Securing AI Agents:_ _Foundations,_ _Frameworks,_ _and_ _Real-World_ _De-_ _ployment_, pages 289–319. Springer, 2025.


E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T. Lanham, D. M. Ziegler, T. Maxwell, N. Cheng, A. Jermyn, A. Askell, A. Radhakrishnan, C. Anil,



D. Duvenaud, D. Ganguli, F. Barez, J. Clark, K. Ndousse, K. Sachan, M. Sellitto, M. Sharma, N. DasSarma, R. Grosse, S. Kravec, Z. Witten, M. Favaro, J. Brauner, H. Karnofsky, P. Christiano, S. R. Bowman, L. Graham, J. Kaplan, S. Mindermann, R. Greenblatt, N. Schiefer, B. Shlegeris, and E. Perez. Sleeper agents: Training deceptive llms that persist through safety training. _arXiv_ _preprint_ _arXiv:2401.05566_, 2024.


K. Isomura. _Management_ _theory_ _by_ _Chester_ _Barnard:_ _an introduction_ . Springer, 2021.


R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79–87, 1991.


A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. _arXiv_ _preprint_ _arXiv:2401.04088_, 2024.


C. Jiang, X. Pan, G. Hong, C. Bao, Y. Chen, and M. Yang. Feedback-guided extraction of knowledge base from retrieval-augmented llm applications, 2025. URL `[https://arxiv.org/](https://arxiv.org/abs/2411.14110)` `[abs/2411.14110](https://arxiv.org/abs/2411.14110)` .


Z. Jiang, J. Araki, H. Ding, and G. Neubig. How can we know when language models know? on the calibration of language models for question answering. _Transactions of the Association for_ _Computational Linguistics_, 9:962–977, 2021.


S. Kapoor, N. Gruver, M. Roberts, A. Pal, S. Dooley, M. Goldblum, and A. Wilson. Calibrationtuning: Teaching large language models to know what they don’t know. In _Proceedings_ _of the 1st Workshop on Uncertainty-Aware NLP_ _(UncertaiNLP 2024)_, pages 1–14, 2024.


A. Kasirzadeh and I. Gabriel. Characterizing ai agents for alignment and governance,
1.    URL `[https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.21848)`
`[21848](https://arxiv.org/abs/2504.21848)` .


M. Keren and D. Levhari. The optimum span of control in a pure hierarchy. _Management_ _science_, 25(11):1162–1172, 1979.



O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. Dspy: Compiling declarative language model calls into selfimproving pipelines, 2023. URL `[https://](https://arxiv.org/abs/2310.03714)` `[arxiv.org/abs/2310.03714](https://arxiv.org/abs/2310.03714)` .


B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and L. van der Maaten. Crypten: Secure multi-party computation meets machine learning. _Advances_ _in_ _Neural_ _Information Processing Systems_, 34:4961–4973, 2021.


S. C. Kohn, E. J. De Visser, E. Wiese, Y.-C. Lee, and T. H. Shaw. Measurement of trust in automation: A narrative review and reference guide. _Frontiers in psychology_, 12:604977, 2021.



_ACM Conference on Fairness, Accountability, and_ _Transparency_, pages 2274–2289, 2025.


M. K. Lee, D. Kusbit, E. Metsky, and L. Dabbish. Working with machines: The impact of algorithmic and data-driven management on human workers. In _Proceedings of the 33rd Annual ACM_ _Conference on Human Factors in Computing Sys-_ _tems_, CHI ’15, pages 1603–1612, New York, NY,
1.    ACM. doi: 10.1145/2702123.2702548.


J. Z. Leibo, A. S. Vezhnevets, M. Diaz, J. P. Agapiou, W. A. Cunningham, P. Sunehag, J. Haas, R. Koster, E. A. Duéñez-Guzmán, W. S. Isaac, G. Piliouras, S. M. Bileschi, I. Rahwan, and S. Osindero. A theory of appropriateness with applications to generative artificial intelligence,
2024. URL `[https://arxiv.org/abs/2412.](https://arxiv.org/abs/2412.19010)`
`[19010](https://arxiv.org/abs/2412.19010)` .



V. Krakovna, J. Uesato, V. Mikulik, M. Rahtz, J. Leike, M. and S. Legg. Specification gaming: The tega, T. Everitt, flip side of AI ingenuity. _DeepMind_ S. Legg. AI safety _Safety_ _Research_ _Blog_, 2020. URL `[https:](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)` _arXiv:1711.09883_
```
 //deepmind.google/discover/blog/
```

`[specification-gaming-the-flip-side-of-ai-ingenuity/](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)` . Blog post. and Y. Liu.



J. Leike, M. Martic, V. Krakovna, P. A. Ortega, T. Everitt, A. Lefrancq, L. Orseau, and S. Legg. AI safety gridworlds. _arXiv_ _preprint_ _arXiv:1711.09883_, 2017.



L. Krause, W. Tufa, S. B. Santamaría, A. Daza, U. Khurana, and P. Vossen. Confidently wrong: exploring the calibration and expression of (un) certainty of large language models in a multilingual setting. In _Proceedings of the workshop_ _on multimodal,_ _multilingual natural language_ _generation and multilingual WebNLG Challenge_ _(MM-NLG 2023)_, pages 1–9, 2023.


A. Lal, A. Prasad, A. Kumar, and S. Kumar. Data exfiltration: Preventive and detective countermeasures. In _Proceedings_ _of_ _the_ _International_ _Conference on Innovative Computing & Commu-_ _nication (ICICC)_, 2022.


H. C. Lau and L. Zhang. Task allocation via multi-agent coalition formation: Taxonomy, algorithms and complexity. In _Proceedings. 15th_ _IEEE International Conference on Tools with Ar-_ _tificial Intelligence_, pages 346–350. IEEE, 2003.


M. H. Lee and M. Z. Y. Tok. Towards uncertainty aware task delegation and human-ai collaborative decision-making. In _Proceedings of the 2025_



H. Li, Q. Dong, J. Chen, H. Su, Y. Zhou, Q. Ai, Z. Ye,. and Y. Liu. Llms-as-judges: a comprehensive survey on llm-based evaluation methods. _arXiv_ _preprint arXiv:2412.05579_, 2024a.



J. Li, Y. Yang, R. Zhang, and Y.-c. Lee. Overconfident and unconfident ai hinder human-ai collaboration. _arXiv preprint arXiv:2402.07632_, 2024b.


P. Li, Z. An, S. Abrar, and L. Zhou. Large language models for multi-robot systems: A survey, 2025a. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2502.03814)` `[2502.03814](https://arxiv.org/abs/2502.03814)` .


W. Li, J. Lin, Z. Jiang, J. Cao, X. Liu, J. Zhang, Z. Huang, Q. Chen, W. Sun, Q. Wang, H. Lu, T. Qin, C. Zhu, Y. Yao, S. Fan, X. Li, T. Wang, P. Liu, K. Zhu, H. Zhu, D. Shi, P. Wang, Y. Guan, X. Tang, M. Liu, Y. E. Jiang, J. Yang, J. Liu, G. Zhang, and W. Zhou. Chain-of-agents: Endto-end agent foundation models via multi-agent distillation and agentic rl, 2025b. URL `[https:](https://arxiv.org/abs/2508.13167)` `[//arxiv.org/abs/2508.13167](https://arxiv.org/abs/2508.13167)` .


H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,


I. Sutskever, and K. Cobbe. Let’s verify step by step, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2305.20050)` `[2305.20050](https://arxiv.org/abs/2305.20050)` .


S. Lin, J. Hilton, and O. Evans. Teaching models to express their uncertainty in words. _arXiv_ _preprint arXiv:2205.14334_, 2022.


X. Liu, T. Chen, L. Da, C. Chen, Z. Lin, and H. Wei. Uncertainty quantification and confidence calibration in large language models: A survey. In _Proceedings of the 31st ACM SIGKDD Conference_ _on Knowledge Discovery and Data Mining V. 2_, pages 6107–6117, 2025.


Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, et al. Prompt injection attack against llm-integrated applications. _arXiv preprint arXiv:2306.05499_, 2023.


J. M. Logg, J. A. Minson, and D. A. Moore. Algorithm appreciation: People prefer algorithmic to human judgment. _Organizational_ _Behav-_ _ior and Human Decision Processes_, 151:90–103, 2019.


B. Lubars and C. Tan. Ask not what ai can do, but what ai should do: Towards a framework of task delegability. _Advances_ _in_ _neural_ _infor-_ _mation processing systems_, 32, 2019.


Z. Luo, Z. Shen, W. Yang, Z. Zhao, P. Jwalapuram, A. Saha, D. Sahoo, S. Savarese, C. Xiong, and J. Li. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. _arXiv preprint_ _arXiv:2508.14704_, 2025.


F. Luthans and T. I. Stewart. A general contingency theory of management. _Academy of man-_ _agement Review_, 2(2):181–195, 1977.


S. Ma, Y. Lei, X. Wang, C. Zheng, C. Shi, M. Yin, and X. Ma. Who should i trust: Ai or myself? leveraging human and ai correctness likelihood to promote appropriate trust in aiassisted decision-making. In _Proceedings of the_ _2023 CHI Conference on Human Factors in Com-_ _puting Systems_, pages 1–19, 2023.


L. Malmqvist. Sycophancy in large language models: Causes and mitigations. In _Intelligent_



_Computing-Proceedings of the Computing Con-_ _ference_, pages 61–74. Springer, 2025.


Y. Mao, M. G. Reinecke, M. Kunesch, E. A. DuéñezGuzmán, R. Comanescu, J. Haas, and J. Z. Leibo. Doing the right thing for the right reason: Evaluating artificial moral cognition by probing cost insensitivity. _arXiv_ _preprint_ _arXiv:2305.18269_, 2023.


S. Masoudnia and R. Ebrahimpour. Mixture of experts: a literature survey. _Artificial Intelligence_ _Review_, 42(2):275–293, 2014.


P. Mazdin and B. Rinner. Distributed and communication-aware coalition formation and task assignment in multi-robot systems. _IEEE_ _Access_, 9:35088–35100, 2021.


E. A. M. Michels, S. Gilbert, I. Koval, and M. K. Wekenborg. Alarm fatigue in healthcare: a scoping review of definitions, influencing factors, and mitigation strategies. _BMC_ _nursing_, 24(1):664, 2025.


Microsoft. Unleashing the power of model context protocol (mcp): A game-changer in AI integration, 2025.


E. Mosqueira-Rey, E. Hernández-Pereira, D. Alonso-Ríos, J. Bobes-Bascarán, and Á. Fernández-Leal. Human-in-the-loop machine learning: a state of the art. _Artificial_ _Intelligence Review_, 56(4):3005–3054, 2023.


C. Mueller and A. Vogelsmeier. Effective delegation: Understanding responsibility, authority, and accountability. _Journal of Nursing Regula-_ _tion_, 4(3):20–27, 2013.


R. B. Myerson. Optimal coordination mechanisms in generalized principal–agent problems. _Jour-_ _nal_ _of_ _mathematical_ _economics_, 10(1):67–81, 1982.


O. Nachum, S. S. Gu, H. Lee, and S. Levine. Dataefficient hierarchical reinforcement learning. _Advances in neural information processing sys-_ _tems_, 31, 2018.


S. K. Nagia. Delegation of authority: A great challenge for business organisation. _ARTIFICIAL_ _INTELLIGENCE (AI) AND BUSINESS_, page 55, 2024.


M. Naiseh, D. Al-Thani, N. Jiang, and R. Ali. Explainable recommendation: when design meets trust calibration. _World Wide Web_, 24(5):1857– 1884, 2021.


M. Naiseh, D. Al-Thani, N. Jiang, and R. Ali. How the different explanation classes impact trust calibration: The case of clinical decision support systems. _International Journal of Human-_ _Computer Studies_, 169:102941, 2023.


J. Needham, G. Edkins, G. Pimpale, H. Bartsch, and M. Hobbhahn. Large language models often know when they are being evaluated. _arXiv_ _preprint arXiv:2505.23836_, 2025.


E. Neelou, I. Novikov, M. Moroz, O. Narayan, T. Saade, M. Ayenson, I. Kabanov, J. Ozmen, E. Lee, V. S. Narajala, E. G. Junior, K. Huang, H. Gulsin, J. Ross, M. Vyshegorodtsev, A. Travers, I. Habler, and R. Jadav. A2as: Agentic ai runtime security and self-defense,
1.    URL `[https://arxiv.org/abs/2510.](https://arxiv.org/abs/2510.13825)`
`[13825](https://arxiv.org/abs/2510.13825)` .


E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen: An open large language model for code with multi-turn program synthesis. _arXiv_ _preprint_ _arXiv:2203.13474_, 2022.


Z. Ning and L. Xie. A survey on multi-agent reinforcement learning and its application. _Jour-_ _nal of Automation and Intelligence_, 3(2):73–91, 2024.


O. Or-Meir, N. Nissim, Y. Elovici, and L. Rokach. Dynamic malware analysis in the modern era—a state of the art survey. _ACM Computing_ _Surveys (CSUR)_, 52(5):1–48, 2019.


D. Otley. The contingency theory of management accounting and control: 1980–2014. _Manage-_ _ment accounting research_, 31:45–62, 2016.


W. G. Ouchi and J. B. Dowling. Defining the span of control. _Administrative_ _Science_ _Quarterly_, pages 357–365, 1974.


B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, and M. T. Ribeiro. Art: Automatic multi-step reasoning and tool-use



for large language models. _arXiv_ _preprint_ _arXiv:2303.09014_, 2023.


R. Parasuraman, R. Molloy, and I. L. Singh. Performance consequences of automationinduced’complacency’. _The International Jour-_ _nal of Aviation Psychology_, 3(1):1–23, 1993.


S. Parikh and R. Surapaneni. Powering AI commerce with the new Agent Payments Protocol (AP2), Sept. 2025. URL
```
 https://cloud.google.com/blog/
 products/ai-machine-learning/
```

`[announcing-agents-to-payments-ap2-protocol](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol)` . Accessed: 2026-01-20.


I. Pastine and T. Pastine. _Introducing game theory:_ _A graphic guide_ . Icon Books, 2017.


S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek. Hierarchical reinforcement learning: A comprehensive survey. _ACM Computing Surveys (CSUR)_, 54(5):1–35, 2021.


M. Petkus. Why and how zk-snark works. _arXiv_ _preprint arXiv:1906.07221_, 2019.


E. Pignatelli, J. Ferret, M. Geist, T. Mesnard, H. van Hasselt, O. Pietquin, and L. Toni. A survey of temporal credit assignment in deep reinforcement learning. _arXiv_ _preprint_ _arXiv:2312.01072_, 2023.


I. Pinyol and J. Sabater-Mir. Computational trust and reputation models for open multi-agent systems: a review. _Artificial Intelligence Review_, 40(1):1–25, 2013.


Z. Porter, P. Ryan, P. Morgan, J. Al-Qaddoumi, B. Twomey, J. McDermid, and I. Habli. Unravelling responsibility for ai. _arXiv_ _preprint_ _arXiv:2308.02608_, 2023.


C. Qian, Z. Xie, Y. Wang, W. Liu, K. Zhu, H. Xia, Y. Dang, Z. Du, W. Chen, C. Yang, et al. Scaling large language model-based multi-agent collaboration. _arXiv_ _preprint_ _arXiv:2406.07155_, 2024.


K. Qin, L. Zhou, B. Livshits, and A. Gervais. Attacking the defi ecosystem with flash loans for fun and profit, 2021. URL `[https://arxiv.](https://arxiv.org/abs/2003.03810)` `[org/abs/2003.03810](https://arxiv.org/abs/2003.03810)` .


Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, L. Hong, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2307.16789)` `[2307.16789](https://arxiv.org/abs/2307.16789)` .


B. Radosevich and J. Halloran. Mcp safety audit: Llms with the model context protocol allow major security exploits. _arXiv_ _preprint_ _arXiv:2504.03767_, 2025.


S. D. Ramchurn, D. Huynh, and N. R. Jennings. Trust in multi-agent systems. _The_ _knowledge_ _engineering review_, 19(1):1–25, 2004.


J. Rando and F. Tramèr. Universal jailbreak backdoors from poisoned human feedback,
1.    URL `[https://arxiv.org/abs/2311.](https://arxiv.org/abs/2311.14455)`
`[14455](https://arxiv.org/abs/2311.14455)` .


S. Rasal and E. J. Hauer. Navigating complexity: Orchestrated problem solving with multi-agent llms, 2024. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2402.16713)` `[2402.16713](https://arxiv.org/abs/2402.16713)` .


T. Rebedea, R. Dinu, M. Sreedhar, C. Parisien, and J. Cohen. Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails, 2023. URL `[https://arxiv.](https://arxiv.org/abs/2310.10501)` `[org/abs/2310.10501](https://arxiv.org/abs/2310.10501)` .


M. G. Reinecke, Y. Mao, M. Kunesch, E. A. DuéñezGuzmán, J. Haas, and J. Z. Leibo. The puzzle of evaluating moral cognition in artificial agents. _Cognitive Science_, 47(8):e13315, 2023.


A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. _arXiv preprint arXiv:2307.01928_, 2023.


C. O. Retzlaff, S. Das, C. Wayllace, P. Mousavi, M. Afshari, T. Yang, A. Saranti, A. Angerschmid, M. E. Taylor, and A. Holzinger. Human-in-theloop reinforcement learning: A survey and position on requirements, challenges, and opportunities. _Journal of Artificial Intelligence Research_, 79:359–415, 2024.



C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby. Scaling vision with sparse mixture of experts. _Advances in Neural Information_ _Processing Systems_, 34:8583–8595, 2021.


J. M. Rosanas and M. Velilla. Loyalty and trust as the ethical bases of organizations. _Journal of_ _Business Ethics_, 44(1):49–59, 2003.


A. Rosenblat and L. Stark. Algorithmic labor and information asymmetries: A case study of uber’s drivers. _International_ _Journal_ _of_ _Communication_, 10:3758–3784,
2016. URL `[https://ijoc.org/index.](https://ijoc.org/index.php/ijoc/article/view/4892)`
`[php/ijoc/article/view/4892](https://ijoc.org/index.php/ijoc/article/view/4892)` .


J. Ruan, Y. Chen, B. Zhang, Z. Xu, T. Bao, H. Mao, Z. Li, X. Zeng, R. Zhao, et al. Tptu: Task planning and tool usage of large language modelbased ai agents. In _NeurIPS 2023 Foundation_ _Models for Decision Making Workshop_, 2023.


J. M. Sanabria and P. A. Vecino. Beyond the sum: Unlocking ai agents potential through market forces, 2025. URL `[https://arxiv.](https://arxiv.org/abs/2501.10388)` `[org/abs/2501.10388](https://arxiv.org/abs/2501.10388)` .


T. Sandholm. An implementation of the contract net protocol based on marginal cost calculations. In _AAAI_, volume 93, pages 256–262, 1993.


Y. Sannikov. A continuous-time version of the principal-agent problem. _The_ _Review_ _of_ _Eco-_ _nomic Studies_, 75(3):957–984, 2008.


F. Santoni de Sio and G. Mecacci. Four responsibility gaps with artificial intelligence: Why they matter and how to address them. _Philosophy &_ _technology_, 34(4):1057–1084, 2021.


S. Sarkar, M. Curado Malta, and A. Dutta. A survey on applications of coalition formation in multi-agent systems. _Concurrency and Compu-_ _tation:_ _Practice and Experience_, 34(11):e6876, 2022.


W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting human evaluators, 2022. URL `[https://arxiv.org/abs/2206.05802](https://arxiv.org/abs/2206.05802)` .


S. Shah. The principal-agent problem in finance. _CFA_ _Institute_ _Research_ _Foundation_ _L2014-1_, 2014.


Y. Shao, H. Zope, Y. Jiang, J. Pei, D. Nguyen, E. Brynjolfsson, and D. Yang. Future of work with ai agents: Auditing automation and augmentation potential across the u.s. workforce,
1.    URL `[https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.06576)`
`[06576](https://arxiv.org/abs/2506.06576)` .


M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, N. Cheng, E. Durmus, Z. Hatfield-Dodds, S. R. Johnston, et al. Towards understanding sycophancy in language models. _arXiv_ _preprint_ _arXiv:2310.13548_, 2023.


N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparselygated mixture-of-experts layer. _arXiv preprint_ _arXiv:1701.06538_, 2017.


O. M. Shehory, K. Sycara, and S. Jha. Multiagent coordination through coalition formation. In _International_ _Workshop_ _on_ _Agent_ _Theories,_ _Architectures, and Languages_, pages 143–154. Springer, 1997.


A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei. A survey of the model context protocol (mcp): Standardizing context to enhance large language models (llms). 2025.


J. Skalse and M. Mancosu. Defining and characterizing reward hacking. _Proceedings of the 2022_ _AAAI/ACM_ _Conference_ _on_ _AI,_ _Ethics,_ _and_ _Soci-_ _ety_, pages 1–11, 2022. doi: 10.1145/3514094. 3534149.


P. Sloksnath. Delegating moral decisions to ai systems. Master’s thesis, University of Zurich, 2025.


S. C. Slota, K. R. Fleischmann, S. Greenberg, N. Verma, B. Cummings, L. Li, and C. Shenefiel. Many hands make many fingers to point: challenges in creating accountable ai. _Ai & Society_, 38(4):1287–1299, 2023.


R. G. Smith. The contract net protocol: High-level communication and control in a distributed



problem solver. _IEEE Transactions on computers_, 29(12):1104–1113, 1980.


J. Sobel. Information control in the principalagent problem. _International Economic Review_, pages 259–269, 1993.


X. Song, Z. Wang, S. Wu, T. Shi, and L. Ai. Gradientsys: A multi-agent llm scheduler with react orchestration, 2025. URL `[https://arxiv.](https://arxiv.org/abs/2507.06520)` `[org/abs/2507.06520](https://arxiv.org/abs/2507.06520)` .


C. Stucky, M. De Jong, and F. Kabo. The paradox of network inequality: differential impacts of status and influence on surgical team communication. _Med J (Ft Sam Houst Tex)_, pages 22–01, 2022.


R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Arti-_ _ficial intelligence_, 112(1-2):181–211, 1999.


S. Tadelis and O. E. Williamson. Transaction cost economics. _The handbook of organizational eco-_ _nomics_, 159(3.1):1, 2012.


W. Takerngsaksiri, J. Pasuksmit, P. Thongtanunam, C. Tantithamthavorn, R. Zhang, F. Jiang, J. Li, E. Cook, K. Chen, and M. Wu. Human-in-the-loop software development agents. In _2025_ _IEEE/ACM_ _47th_ _Inter-_ _national_ _Conference_ _on_ _Software_ _Engineering:_ _Software_ _Engineering_ _in_ _Practice_ _(ICSE-SEIP)_, pages 342–352. IEEE, 2025.


J. Teutsch and C. Reitwießner. Truebit: a scalable verification solution for blockchains. _White_ _Papers_, 2018.


J. Teutsch and C. Reitwießner. A scalable verification solution for blockchains. In _Aspects_ _of_ _Computation and Automata Theory with Appli-_ _cations_, pages 377–424. World Scientific, 2024.


N. A. Theobald and S. Nicholson-Crotty. The many faces of span of control: Organizational structure across multiple goals. _Administration_ _&_ _Society_, 36(6):648–660, 2005.


N. Tomašev, M. Franklin, J. Jacobs, S. Krier, and S. Osindero. Distributional agi safety. _arXiv_ _preprint arXiv:2512.16856_, 2025.


N. Tomasev, M. Franklin, J. Z. Leibo, J. Jacobs, W. A. Cunningham, I. Gabriel, and S. Osindero. Virtual agent economies, 2025. URL `[https:](https://arxiv.org/abs/2509.10147)` `[//arxiv.org/abs/2509.10147](https://arxiv.org/abs/2509.10147)` .


P. M. Tomei, R. Jain, and M. Franklin. Ai governance through markets. _arXiv_ _preprint_ _arXiv:2501.17755_, 2025.


K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham, B. O’Sullivan, and H. D. Nguyen. Multi-agent collaboration mechanisms: A survey of llms,
2025. URL `[https://arxiv.org/abs/2501.](https://arxiv.org/abs/2501.06322)`
`[06322](https://arxiv.org/abs/2501.06322)` .


V. Tupe and S. Thube. Ai agentic workflows and enterprise apis: Adapting api architectures for the age of ai agents, 2025. URL `[https://](https://arxiv.org/abs/2502.17443)` `[arxiv.org/abs/2502.17443](https://arxiv.org/abs/2502.17443)` .


M. Turpin, J. Michael, E. Perez, and S. R. Bowman. Language models don’t always say what they think: Unfaithful explanations in chainof-thought prompting, 2023. URL `[https:](https://arxiv.org/abs/2305.04388)` `[//arxiv.org/abs/2305.04388](https://arxiv.org/abs/2305.04388)` .


R. Uuk, C. I. Gutierrez, D. Guppy, L. Lauwaert, A. Kasirzadeh, L. Velasco, P. Slattery, and C. Prunkl. A taxonomy of systemic risks from general-purpose ai. _arXiv_ _preprint_ _arXiv:2412.07780_, 2024.


K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati. On the planning abilities of large language models-a critical investigation. _Advances in Neural Information Processing Sys-_ _tems_, 36:75993–76005, 2023.


A. H. Van de Ven. The concept of fit in contingency theory. Technical report, 1984.


T. van der Weij, F. Hofstätter, O. Jaffe, S. F. Brown, and F. R. Ward. Ai sandbagging: Language models can strategically underperform on evaluations. _arXiv_ _preprint_ _arXiv:2406.07358_,
2025. Published as a conference paper at ICLR
2025.


A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In _International conference on machine_ _learning_, pages 3540–3549. PMLR, 2017a.



A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. Feudal networks for hierarchical reinforcement learning, 2017b. URL `[https://arxiv.org/](https://arxiv.org/abs/1703.01161)` `[abs/1703.01161](https://arxiv.org/abs/1703.01161)` .


E. F. Vignola, S. Baron, E. Abreu Plasencia, M. Hussein, and N. Cohen. Workers’ health under algorithmic management: Emerging findings and urgent research questions. _In-_ _ternational Journal of Environmental Research_ _and_ _Public_ _Health_, 20(2):1239, 2023. doi: 10.3390/ijerph20021239.


J. Vokřínek, J. Bíba, J. Hodík, J. Vybíhal, and M. Pěchouček. Competitive contract net protocol. In _International_ _Conference_ _on_ _Current_ _Trends in Theory and Practice of Computer Sci-_ _ence_, pages 656–668. Springer, 2007.


G. Wang, B. Wang, T. Wang, A. Nika, H. Zheng, and B. Y. Zhao. Ghost riders: Sybil attacks on crowdsourced mobile mapping services. _IEEE/ACM Transactions on Networking_, 26(3): 1123–1136, 2018. doi: 10.1109/TNET.2018. 2818073.


J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang. Qplex: Duplex dueling multi-agent q-learning. _arXiv_ _preprint arXiv:2008.01062_, 2020.


J. Wang, Z. Wu, Y. Li, H. Jiang, P. Shu, E. Shi, H. Hu, C. Ma, Y. Liu, X. Wang, Y. Yao, X. Liu, H. Zhao, Z. Liu, H. Dai, L. Zhao, B. Ge, X. Li, T. Liu, and S. Zhang. Large language models for robotics: Opportunities, challenges, and perspectives, 2024a. URL `[https://arxiv.](https://arxiv.org/abs/2401.04334)` `[org/abs/2401.04334](https://arxiv.org/abs/2401.04334)` .


L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al. A survey on large language model based autonomous agents. _Frontiers of Computer Sci-_ _ence_, 18(6):186345, 2024b.


Y. Wang, D. Xue, S. Zhang, and S. Qian. Badagent: Inserting and activating backdoor attacks in llm agents, 2024c. URL `[https://arxiv.org/](https://arxiv.org/abs/2406.03007)` `[abs/2406.03007](https://arxiv.org/abs/2406.03007)` .


A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? _Advances in Neural Information Processing Systems_, 36:80079–80110, 2023.


O. E. Williamson. Transaction-cost economics: the governance of contractual relations. _The_ _journal of Law and Economics_, 22(2):233–261, 1979.


O. E. Williamson. Transaction cost economics. _Handbook_ _of_ _industrial_ _organization_, 1:135– 182, 1989.


M. Wischnewski, N. Krämer, and E. Müller. Measuring and understanding trust calibrations for automated systems: A survey of the state-ofthe-art and future directions. In _Proceedings of_ _the_ _2023_ _CHI_ _conference_ _on_ _human_ _factors_ _in_ _computing systems_, pages 1–16, 2023.


Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al. The rise and potential of large language model based agents: A survey. _Science_ _China_ _Infor-_ _mation Sciences_, 68(2):121101, 2025.


Y. Xiao, P. P. Liang, U. Bhatt, W. Neiswanger, R. Salakhutdinov, and L.-P. Morency. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. _arXiv_ _preprint arXiv:2210.04714_, 2022.


W. Xing, Z. Qi, Y. Qin, Y. Li, C. Chang, J. Yu, C. Lin, Z. Xie, and M. Han. Mcp-guard: A defense framework for model context protocol integrity in large language model applications. _arXiv preprint arXiv:2508.10991_, 2025.


F. Xu, Q. Hao, C. Shao, Z. Zong, Y. Li, J. Wang, Y. Zhang, J. Wang, X. Lan, J. Gong, et al. Toward large reasoning models: A survey of reinforced reasoning with large language models. _Patterns_, 6(10), 2025.


L. Xu and H. Weigand. The evolution of the contract net protocol. In _International Conference_ _on_ _Web-Age_ _Information_ _Management_, pages 257–264. Springer, 2001.


Y. Yang, Y. Wen, J. Wang, and W. Zhang. Agent exchange: Shaping the future of ai agent economics. _arXiv_ _preprint_ _arXiv:2507.03904_, 2025.



J. Yi, Y. Xie, B. Zhu, E. Kiciman, G. Sun, X. Xie, and F. Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. In _Proceedings_ _of_ _the_ _31st ACM SIGKDD Conference on Knowledge Dis-_ _covery and Data Mining V.1_, page 1809–1820. ACM, July 2025. doi: 10.1145/3690624.
1.       URL `[http://dx.doi.org/10.](http://dx.doi.org/10.1145/3690624.3709179)`
`[1145/3690624.3709179](http://dx.doi.org/10.1145/3690624.3709179)` .


H. Yu, Z. Shen, C. Leung, C. Miao, and V. R. Lesser. A survey of multi-agent trust management systems. _IEEE Access_, 1:35–50, 2013.


L. Yu, V. Do, K. Hambardzumyan, and N. Cancedda. Robust llm safeguarding via refusal feature adversarial training. _arXiv_ _preprint_ _arXiv:2409.20089_, 2024.


M. Yu, F. Meng, X. Zhou, S. Wang, J. Mao, L. Pang, T. Chen, K. Wang, X. Li, Y. Zhang, B. An, and Q. Wen. A survey on trustworthy llm agents: Threats and countermeasures, 2025. URL `[https://arxiv.org/abs/2503.09648](https://arxiv.org/abs/2503.09648)` .


Y. Yuan, W. Jiao, W. Wang, J.-t. Huang, J. Xu, T. Liang, P. He, and Z. Tu. Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training. In _Proceedings of the_ _63rd Annual Meeting of the Association for Com-_ _putational Linguistics (Volume 1:_ _Long Papers)_, pages 3149–3167, 2025.


S. E. Yuksel, J. N. Wilson, and P. D. Gader. Twenty years of mixture of experts. _IEEE transactions_ _on neural networks and learning systems_, 23(8): 1177–1193, 2012.


F. M. Zanzotto. Human-in-the-loop artificial intelligence. _Journal of Artificial Intelligence Re-_ _search_, 64:243–252, 2019.


Q. Zhan, Z. Liang, Z. Ying, and D. Kang. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents,
2024. URL `[https://arxiv.org/abs/2403.](https://arxiv.org/abs/2403.02691)`
`[02691](https://arxiv.org/abs/2403.02691)` .


N. Zhang, J. Yan, C. Hu, Q. Sun, L. Yang, D. W. Gao, J. M. Guerrero, and Y. Li. Price-matchingbased regional energy market with hierarchical reinforcement learning algorithm. _IEEE Trans-_ _actions on Industrial Informatics_, 20(9):11103– 11114, 2024.


W. Zhang, C. Cui, Y. Zhao, R. Hu, Y. Liu, Y. Zhou, and B. An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving. _arXiv e-prints_, pages arXiv–2506, 2025a.


Z. Zhang, Q. Dai, X. Bo, C. Ma, R. Li, X. Chen, J. Zhu, Z. Dong, and J.-R. Wen. A survey on the memory mechanism of large language modelbased agents. _ACM Transactions on Information_ _Systems_, 43(6):1–47, 2025b.


K. Zhao, L. Li, K. Ding, N. Z. Gong, Y. Zhao, and Y. Dong. A survey on model extraction attacks and defenses for large language models, 2025. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2506.22521)` `[2506.22521](https://arxiv.org/abs/2506.22521)` .


W. Zhao, Y. Gao, S. A. Memon, B. Raj, and R. Singh. Hierarchical routing mixture of experts. In _2020 25th International Conference on_ _Pattern Recognition (ICPR)_, pages 7900–7906. IEEE, 2021.


L. Zhou, X. Xiong, J. Ernstberger, S. Chaliasos, Z. Wang, Y. Wang, K. Qin, R. Wattenhofer, D. Song, and A. Gervais. Sok: Decentralized finance (defi) attacks, 2023. URL `[https:](https://arxiv.org/abs/2208.13035)` `[//arxiv.org/abs/2208.13035](https://arxiv.org/abs/2208.13035)` .


Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon, et al. Mixture-ofexperts with expert choice routing. _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, 35: 7103–7114, 2022.


C. Zhu, M. Dastani, and S. Wang. A survey of multi-agent deep reinforcement learning with communication. _Autonomous Agents and Multi-_ _Agent Systems_, 38(1):4, 2024.


Z. Zou, Z. Liu, L. Zhao, and Q. Zhan. Blocka2a: Towards secure and verifiable agent-to-agent interoperability. _arXiv preprint arXiv:2508.01332_, 2025.
