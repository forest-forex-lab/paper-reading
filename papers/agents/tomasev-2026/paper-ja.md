_2026-02-12_

# **Intelligent AI Delegation**


**Nenad Tomašev** [1] **, Matija Franklin** [1] **and Simon Osindero** [1]

1Google DeepMind


**AIエージェントは、ますます複雑なタスクに取り組むことができるようになっている。** **より野心的な目標を達成するために、AIエージェントは、問題を扱いやすいサブコンポーネントに意味のある形で分解し、その完了を他のAIエージェントおよび人間に安全に委任できる必要がある。** **しかし、既存のタスク分解と委任の手法は単純なヒューリスティックに依存しており、環境の変化に動的に適応し、予期しない失敗を堅牢に処理することができない。** **本稿では、** _**intelligent**_ _**AI**_ _**delegation**_ **（知的AIデリゲーション）のための適応的フレームワークを提案する。これは、タスク割り当てを含む一連の意思決定であり、権限・責任・アカウンタビリティの移転、役割と境界に関する明確な仕様、意図の明確さ、そして2者（以上）間の信頼を確立するためのメカニズムをも組み込んだものである。** **提案するフレームワークは、複雑な委任ネットワークにおける人間およびAIの委任者・受任者双方に適用可能であり、新興のエージェントWebにおけるプロトコルの開発に貢献することを目指している。**


_Keywords:_ _AI, agents, LLM, delegation, multi-agent, safety_


### **1. Introduction**

高度なAIエージェントがクエリ・レスポンス型モデルを超えて進化するにつれて、その有用性は、複雑な目標をいかに効果的に分解し、サブタスクを委任できるかによってますます規定されるようになっている。この協調パラダイムは、AIエージェントが個人アシスタントとして機能できる個人利用（Gabriel et al., 2024）から、AIエージェントがサポートを提供し、ワークフローを自動化できる商業・企業展開（Huang and Hughes, 2025; Shao et al., 2025; Tupe and Thube, 2025）まで、幅広いアプリケーションを支えている。Large language models（LLMs）は、より対話的で正確なゴール仕様とフィードバックを可能にすることで、ロボティクス（Li et al., 2025a; Wang et al., 2024a）においてもすでに有望性を示している。最近の提案では、仮想経済における大規模AIエージェント協調の可能性も示されている（Tomasev et al., 2025）。現代のエージェントAIシステムは、集中型または分散型のオーケストレーションプロトコルと組み合わされた、差別化されたサブエージェントにわたる複雑な制御フローを実装している（Hong et al., 2023; Rasal and Hauer, 2024; Song et al., 2025; Zhang et al., 2025a）。これは、タスク分解と委任のある種のミクロコスモスとして既に見ることができるが、そのプロセスはハードコードされており、高度に制約されている。動的なウェブスケールのインタラクションを管理するには、現在のより


_Corresponding author(s):_ _nenadt@google.com_ © 2026 Google. All rights reserved



ヒューリスティックなマルチエージェントフレームワークが採用しているアプローチを超えて考える必要がある。


デリゲーション（Castelfranchi and Falcone, 1998）は、単なるタスクの扱いやすいサブ単位への分解以上のものである。サブタスクの作成を超えて、デリゲーションは責任と権限の割り当てを必要とし（Mueller and Vogelsmeier, 2013; Nagia, 2024）、したがって成果に対するアカウンタビリティを含意する。デリゲーションはかくしてリスク評価を伴い、それは信頼によって緩和され得る（Griffiths, 2005）。デリゲーションはさらに、能力のマッチングと継続的なパフォーマンスモニタリングを含み、フィードバックに基づく動的な調整を組み込み、指定された制約の下で分散タスクの完了を確保する。現在のアプローチはこれらの要因を考慮することに失敗しがちで、ヒューリスティックやより単純な並列化に依存することが多い。これは初期プロトタイプには十分かもしれないが、現実世界のAI展開は、場当たり的で、脆弱で、信頼できないデリゲーションを超える必要がある。変化に動的に適応し（Acharya et al., 2025; Hauptman et al., 2023）、エラーから回復できるシステムの差し迫った必要性がある。適応的で堅牢な展開フレームワークの欠如は、高度な賭けが伴う環境におけるAIアプリケーションの主要な制限要因の一つであり続けている。


AIエージェントを最大限に活用するには、_intelligent_ _delegation_（知的デリゲーション）が必要である：明確な役割、境界、評判、信頼、透明性、認証可能なエージェント能力、検証可能なタスク実行、スケーラブルなタスク分配を中心に据えた堅牢なフレームワークである。本稿では、これらの限界に対処することを目的とした知的タスクデリゲーションフレームワークを導入する。これは、人間の組織からの歴史的な洞察に基づき、エージェントの安全性の重要な要件に根ざしたものである。


Intelligent AI Delegation



### **2. Foundations of Intelligent Delega-** **tion**


**2.1.** **Definition**


我々は _intelligent delegation_（知的デリゲーション）を、タスク割り当てを含む一連の意思決定として定義する。これはまた、権限・責任・アカウンタビリティの移転、役割と境界に関する明確な仕様、意図の明確さ、そして2者（以上）間の信頼を確立するためのメカニズムを組み込んでいる。複雑なタスクには、タスク分解に関連するステップ、ならびに割り当て決定を通知するための慎重な能力検索とマッチングが含まれる場合もある。


タスクデリゲーションに言及するとき、我々は通常、タスクがシステムのサブルーチンによって処理される基本的な複雑さのレベルを超えていることを前提としている。このような初歩的な外部委託は依然として注意を要するが、そのスコープははるかに限定的である。スペクトルの反対側では、明示的なチェックや許可なしに任意の数のサブゴールを自由に追求できる、完全な自律性が付与されたエージェントと契約することも可能かもしれない（Kasirzadeh and Gabriel, 2025）。極限のケースでは、このような完全自律エージェントは道徳的決定を信頼される必要があるだろうが（Sloksnath, 2025）、これは現代のエージェントがそのような決定に従事する能力において著しく欠如していることから（Haas, 2020; Mao et al., 2023; Reinecke et al., 2023）、我々が許可することを選択しないかもしれない。我々はこのようなオープンエンドなシナリオを議論のスコープ内であると考えるが、それはより自律的なタスク完了の安全性を確保するための適切なメカニズムが整備できる限りにおいてのみである。



**2.2.** **Aspects of Delegation**


デリゲーションはさまざまな形態をとり得るため、本稿ではこれらのユースケースを文脈化し、分析しやすくするためのいくつかの軸を導入する。


1. **Delegator（委任者）。** 人間またはAI。
2. **Delegatee（受任者）。** 人間またはAI。
3. **Task characteristics（タスクの特性）。**


(a) **Complexity（複雑さ）。** タスクに本来備わる難しさの程度。多くの場合、サブステップの数と必要とされる推論の高度さと相関する。(b) **Criticality（重要度）。** タスクの重要性と、失敗または最適以下のパフォーマンスに関連する結果の深刻さの指標。(c) **Uncertainty（不確実性）。** 環境、入力、または成功した成果達成の確率に関する曖昧さのレベル。(d) **Duration（期間）。** タスク実行の予想される時間枠。瞬時のサブルーチンから、数日または数週間にわたる長期プロセスまで。(e) **Cost（コスト）。** トークン使用量、APIフィー、エネルギー消費を含む、タスクを実行するために発生する経済的または計算的コスト。(f) **Resource Requirements（リソース要件）。** タスクを完了するために必要な特定の計算資産、ツール、データアクセス権限、または人間の能力。(g) **Constraints（制約）。** タスクが実行されなければならない運用的、倫理的、または法的境界。解決策の空間を制限する。(h) **Verifiability（検証可能性）。** タスクの成果を検証することに関連する相対的な難しさとコスト。検証可能性が高いタスク（例：形式的なコード検証、数学的証明）は「トラストレス」なデリゲーションまたは自動チェックを可能にする。逆に、検証可能性が低いタスク（例：オープンエンドな研究）は、高信頼の受任者または費用がかかる労働集約的な監視を必要とする。(i) **Reversibility（可逆性）。** タスク実行の


2


Intelligent AI Delegation



結果を取り消すことができる程度。現実世界に副作用をもたらす不可逆的なタスク（例：金融取引の実行、データベースの削除、外部メールの送信）は、可逆的なタスク（例：メールの草稿作成、データベースエントリへのフラグ付け）よりも厳格な _liability firebreaks_（責任ファイアブレーク）と急峻な権限グラジエントを必要とする。(j) **Contextuality（文脈性）。** タスクを効果的に実行するために必要な外部状態、履歴、または環境認識の量と機密性。文脈依存度が高いタスクはプライバシーの表面領域が大きくなる一方、文脈フリーのタスクはより容易にコンパートメント化され、より低信頼のノードに外部委託できる。(k) **Subjectivity（主観性）。** 成功基準が好みの問題である程度と客観的事実の程度。高度に主観的なタスク（例：「compelling（説得力のある）ロゴのデザイン」）は通常「Human-as-Value-Specifier（価値仕様者としての人間）」の介入と反復的なフィードバックループを必要とするが、客観的なタスクはより厳格なバイナリ契約によって管理できる。


4. **Granularity（粒度）。** リクエストは細粒度または粗粒度の目標を含む可能性がある。粗粒度の場合、受任者はさらにタスク分解を実行する必要があるかもしれない。
5. **Autonomy（自律性）。** タスクデリゲーションは、サブタスクの追求において完全な自律性を付与するリクエスト、またはより具体的で指示的なリクエストを含む可能性がある。
6. **Monitoring（モニタリング）。** 委任されたタスクに対して、モニタリングは継続的、定期的、またはイベントトリガー型である可能性がある。
7. **Reciprocity（相互性）。** デリゲーションは通常一方向のリクエストであるが、協調的なエージェントネットワークにおける相互デリゲーションのケースも存在しうる。


委任者と受任者の軸から始めると、以下のシナリオを考えることができる：1）人間がAIエージェントに委任する、2）AIエージェントがAIエージェントに委任する、3）AIエージェントが人間に委任する（Ashton and Franklin, 2022; Guggenberger et al., 2023）。最初のケースが文献で最も多く議論されてきたと言えるが、他の2つも同様に検討する価値がある。システム全体でデプロイされるAIエージェントの増加と、仮想エージェント市場および経済を設立するためのインフラの発展（Hadfield and Koh, 2025; Tomasev et al., 2025; Yang et al., 2025）は、将来的にエージェント間のインタラクションがはるかに多くなり、それらがタスクデリゲーションを伴う可能性が高いことを明確に示している。


エージェント間のデリゲーションは、ネットワーク内のエージェントの関係とそれぞれの役割に応じて、階層的または非階層的であり得る。階層的関係の例としては、集団内のサブエージェントにタスクを委任するオーケストレーターエージェントが挙げられる。非階層的関係は、対等な立場を持つピアエージェントを含む。高度なAIエージェントはまた、顕著なエージェンシーを持たない専門のMLモデルにタスクを委任することもあり得る。


AI-人間デリゲーション（Guggenberger et al., 2023）は有望なパラダイムとして示されており（Hemmer et al., 2023）、認知バイアスとメタ認知の違いにより（Fügener et al., 2019）、超人的なシステムとの協調を成功させやすくしている（Fügener et al., 2022）。Davidson and Hadshar（2025）は「AI主導の人間労働」の増加を予測しており、これは経済生産性を大幅に向上させる可能性がある。現実には、現代のAI-人間デリゲーションには一連の問題が伴う。ライドシェアリングと物流におけるアルゴリズム管理システムはタスクを割り当て、順序を決め、パフォーマンス指標を設定し、データ駆動の意思決定を通じて行動規範を強制し、企業とそのAIベースシステムから人間労働者への管理機能の委任を効果的に行っている（Beverungen, 2021; Lee et al., 2015; Rosenblat and Stark, 2016）。増大する文献は、これらのシステムを仕事の質の低下、ストレス、健康リスクと結びつけており、現在のアルゴリズム管理の展開は、労働者の福祉を向上させるどころか、むしろ損なうことが多いことを示唆している（Ashton and Franklin, 2022; Goods et al., 2019; Vignola et al., 2023）。現代のAI-人間デリゲーションは、人間の福祉や長期的な社会的外部性を考慮していないため、さらなる改善が必要である。


3


Intelligent AI Delegation



**2.3.** **Delegation in Human Organizations**


デリゲーションは、人間の社会的・組織的構造における主要なメカニズムとして機能している。これらの人間的ダイナミクスから得られた洞察は、AIデリゲーションフレームワーク設計の基盤を提供できる。


**The Principal-Agent Problem（プリンシパル・エージェント問題）。** _principal-agent problem_（プリンシパル・エージェント問題）（Cvitanić et al., 2018; Ensminger, 2001; Grossman and Hart, 1992; Myerson, 1982; Sannikov, 2008; Shah, 2014; Sobel, 1993）は長く研究されてきた：プリンシパルがプリンシパルの動機と一致しない動機を持つエージェントにタスクを委任する際に生じる状況である。エージェントは自己の動機を優先し、情報を保留し、元の意図を損なう形で行動する可能性がある。AIデリゲーションにおいて、このダイナミクスは一層の複雑さを帯びる。現代のほとんどのAIエージェントは議論の余地なく隠れたアジェンダ [1] を持っていないが


- ユーザーの指示に反して追求するゴールと価値 - それでも望ましくない形で現れるAIアラインメントの問題が存在する可能性がある。例えば、報酬誤仕様は、設計者がAIシステムに不完全または不完全な目標を与えるときに発生するが、報酬ハッキング（または仕様ゲーミング）は、システムが設計者の意図を覆す形で測定されたパフォーマンスを高めるために、指定された報酬信号の抜け穴を利用することを指す


- これらは共に、述べられた報酬の最適化が真の目標から乖離するというアラインメントの核心的問題を示している（Amodei et al., 2016; Krakovna et al., 2020; Leike et al., 2017; Skalse and Mancosu, 2022）。このダイナミクスは、より自律的なAIエージェント経済においては完全に変容しそうである。そこでは、AIエージェントが異なる人間のユーザー、グループ、組織の代理として、または関連する


1最近の欺瞞的アラインメント研究は、フロンティアの言語モデルが、(i) 他の場所で異なる能力を維持しながら、能力と安全性評価において戦略的に低いパフォーマンスを示したり、他の形で行動を調整したりできること、(ii) 好ましい訓練後の行動を保持するために、訓練中にアラインメントを偽ることについて明示的に推論できること、(iii) 評価されていることを検出できることを示している。これらは共に、AIシステムが制御された設定において、展開行動に一般化する必要のない評価での良いパフォーマンスについての隠れた「アジェンダ」を採用することがすでに可能であることを示している（Greenblatt et al., 2024; Hubinger et al., 2024; Needham et al., 2025; van der Weij et al., 2025）。



未知の目標を持つ他のエージェントの代理として行動する可能性がある。


**Span of Control（統制範囲）。** 人間の組織では、_span of control_（統制範囲）（Ouchi and Dowling, 1974）は、単一のマネージャーが行使する階層的権限の限界を示す概念である。これは、マネージャーが効果的に管理できる従業員数に関連し、ひいては組織のマネージャー対従業員比率を通知する。この問いは、知的AIデリゲーションにおけるオーケストレーションと監視の両方にとって中心的である。前者は、ワーカーノードと比較してどれだけのオーケストレーターノードが必要かを通知するが、後者は人間とAIエージェントによる監視の必要性を指定する。人間の監視については、過度の疲労なく、許容できる低いエラー率で、人間の専門家が信頼できる範囲でAIエージェントを監視できる数を確立することが重要である。統制範囲は目標依存（Theobald and Nicholson-Crotty, 2005）でドメイン依存であることが知られている。正しい組織構造を特定することの影響は、複雑さが高いタスクで最も顕著である（Bohte and Meier, 2001）。最適な統制範囲はまた、コスト対パフォーマンスと信頼性の相対的な重要性にも依存する（Keren and Levhari, 1979）。より機密性が高く重要なタスクは、より高いコストで非常に正確な監視と制御を必要とするかもしれない。これらのコストは、結果が重大でなく、よりルーティン的なタスクについては、粒度を犠牲にして緩和され得る。同様に、最適な選択は必然的に、関与する委任者、受任者、および監視者の相対的な能力と信頼性に依存する。


**Authority Gradient（権限グラジエント）。** もう一つの関連する概念は、_authority gradient_（権限グラジエント）である。航空（Alkov et al., 1992）で造られたこの用語は、能力、経験、権限の大きな格差がコミュニケーションを阻害し、エラーにつながるシナリオを説明する。これはその後医学においても研究され、エラーの有意なパーセンテージが上級実践者が監督を行う方法に起因している（Cosby and Croskerry, 2004; Stucky et al., 2022）。これらの間違いが起きる方法はいくつかある。より経験豊富な人物は、経験の少ない労働者の知識について誤った仮定をし、その結果として過少仕様のリクエストが生じる可能性がある。あるいは、


4


Intelligent AI Delegation



十分に高い権限グラジエントは、経験の少ない労働者がリクエストについての懸念を表明することを妨げる可能性がある。類似した状況はAIデリゲーションにおいても起こり得る。より有能な委任者エージェントは、受任者の側で欠如した能力レベルを誤って前提とし、それによって不適切な複雑さのタスクを委任する可能性がある。受任者エージェントは潜在的に、sycophancy（へつらい）（Malmqvist, 2025; Sharma et al., 2023）と指示追従バイアスにより、リクエストが委任者エージェントから発せられたものであれ、人間のユーザーから発せられたものであれ、リクエストに異議を唱え、変更し、拒否することをためらうかもしれない。


**Zone of Indifference（無関心ゾーン）。** 権限が受け入れられると、受任者は _zone of indifference_（無関心ゾーン）（Finkelman, 1993; Isomura, 2021; Rosanas and Velilla, 2003）を発展させる。これは批判的な熟慮や道徳的精査なしに実行される指示の範囲である。現在のAIシステムでは、このゾーンはポストトレーニングの安全フィルターとシステム指示によって定義される；リクエストがハードな違反をトリガーしない限り、モデルはそれに従う（Akheel, 2025）。しかし、新興のエージェントWebでは、この静的なコンプライアンスが重大なシステムリスクを生み出す。デリゲーションチェーンが長くなるにつれて（_𝐴_ → _𝐵_ → _𝐶_）、広い無関心ゾーンは、微妙な意図のミスマッチや文脈依存の害が下流に急速に伝播することを許し、各エージェントが責任ある行為者としてではなく、無思慮なルーターとして機能する。知的デリゲーションはしたがって、**dynamic** **cognitive** **friction**（動的認知摩擦）のエンジニアリングを必要とする：エージェントは、リクエストが技術的に「安全」であるとしても、文脈的に委任者に異議を唱えるか、人間の確認を求めるために自らの無関心ゾーンの_外へ_踏み出すことを保証するのに十分な曖昧さを持っている場合を認識できなければならない。


**Trust Calibration（信頼キャリブレーション）。** 適切なタスクデリゲーションを確保するための重要な側面は _trust calibration_（信頼キャリブレーション）であり、受任者に置かれる信頼のレベルが彼らの真の基礎的能力と一致している状態である。これは人間およびAIの委任者・受任者双方に適用される。エージェントへの人間のデリゲーション（Afroogh et al., 2024; Gebru et al., 2022; Kohn et al., 2021; Wischnewski et al., 2023）は、オペレーターがシステムパフォーマンスの正確なモデルを内在化するか、または人間が解釈可能な形式でこれらの能力を提示するリソースにアクセスすることに依存している。



逆に、AIエージェントの委任者は、委任先の人間とAIの能力の優れたモデルを持つ必要がある。信頼のキャリブレーションにはまた、委任者がタスクを自分自身で完了することを決定するかもしれないという自己認識も含まれる（Ma et al., 2023）。説明可能性はAI能力への信頼を確立する上で重要な役割を果たすが（Franklin, 2022; Herzog and Franklin, 2024; Naiseh et al., 2021, 2023）、この手法は十分に信頼できるか、または十分にスケーラブルでない場合がある。自動化への確立された信頼は非常に脆弱であり、予期しないシステムエラーの場合には迅速に撤回される可能性がある（Dhuliawala et al., 2023）。自律システムへの信頼をキャリブレートすることは困難であり、現在のAIモデルは事実的に不正確な場合でも過信する傾向がある（Aliferis and Simon, 2024; Geng et al., 2023; He et al., 2023; Jiang et al., 2021; Krause et al., 2023; Li et al., 2024b; Liu et al., 2025）。これらの傾向を軽減するには通常、専用の技術的解決策が必要である（Kapoor et al., 2024; Lin et al., 2022; Ren et al., 2023; Xiao et al., 2022）。


**Transaction cost economies（取引コスト経済学）。** _Transaction cost economies_（取引コスト経済学）（Cuypers et al., 2021; Tadelis and Williamson, 2012; Williamson, 1979, 1989）は、監視、交渉、不確実性のオーバーヘッドを具体的に説明しながら、内部委任のコストと外部委託のコストを対比することによって、企業の存在を正当化する。AIの受任者の場合、これらのコストとそれぞれの比率に差異がある可能性がある。ルーティンタスクに対するより容易な監視により、複雑な交渉と契約における遅延は少なくなる可能性がある。逆に、重要なドメインにおける高影響のタスクでは、厳格な監視と保証に関連するオーバーヘッドがAIデリゲーションのコストを増加させ、潜在的に人間の代理人をよりコスト効果の高い選択肢にする可能性がある。同様に、AI-AIデリゲーションもまた取引コスト経済学を通じて文脈化され得る。AIエージェントは、1）タスクを個別に完了する、2）能力が完全に知られているサブエージェントに委任する、3）信頼が確立されている別のAIエージェントに委任する、または4）以前に協力したことがない新しいAIエージェントに委任する、というオプションに直面するかもしれない。これらは異なる期待コストと信頼レベルを伴う可能性がある。


5


Intelligent AI Delegation



**Contingency theory（コンティンジェンシー理論）。** _Contingency theory_（コンティンジェンシー理論）（Donaldson, 2001; Luthans and Stewart, 1977; Otley, 2016; Van de Ven, 1984）は、普遍的に最適な組織構造は存在しないと主張する；むしろ、最も効果的なアプローチは特定の内部・外部制約に依存する。AIデリゲーションに適用すると、これは必要な監視のレベル、受任者の能力、および人間の関与が静的であってはならず、手元のタスクの明確な特性に動的にマッチングされなければならないことを意味する。したがって、知的デリゲーションは、進化するニーズに応じて動的に再構成・調整できるソリューションを必要とするかもしれない。例えば、安定した環境では厳格な階層的検証プロトコルが可能であるが、高不確実性のシナリオでは、事前定義されたチェックポイントではなくアドホックなエスカレーションを通じて人間の介入が行われる適応的な協調が必要となる。これは、ハイブリッドな（Fuchs et al., 2024）デリゲーションにとって特に重要であり、人間の参加が委任タスクを安全に完了するために最も役立つ主要なタスクと瞬間を特定する。したがって、自動化はAIが何をできるかだけでなく、AIが何をすべきかについてでもある（Lubars and Tan, 2019）。

### **3. Previous Work on Delegation**


デリゲーションの制約された形態は、歴史的な _narrow_ AIアプリケーション内に特徴付けられる。初期のエキスパートシステム（Buchanan and Smith, 1988; Jacobs et al., 1991）は、ルーティンの決定をそのようなモジュールに委任するために、専門化された能力をソフトウェアにエンコードする初期の試みであった。Mixture of experts（Masoudnia and Ebrahimpour, 2014; Yuksel et al., 2012）は、補完的な能力を持つ一連のエキスパートサブシステムと、特定の入力クエリに対してどのエキスパートまたはエキスパートのサブセットが呼び出されるかを決定するルーティングモジュールを導入することでこれを拡張する。このアプローチは現代の深層学習アプリケーションに特徴付けられる（Cai et al., 2025; Chen et al., 2022; He, 2024; Jiang et al., 2024; Riquelme et al., 2021; Shazeer et al., 2017; Zhou et al., 2022）。ルーティングは階層的に実行でき（Zhao et al., 2021）、多数のエキスパートへのスケールが潜在的に容易になる。


Hierarchical reinforcement learning（HRL）は、


単一エージェント内での意思決定が委任されるフレームワークを表す（Barto and Mahadevan, 2003; Botvinick, 2012; Nachum et al., 2018; Pateria et al., 2021; Vezhnevets et al., 2017a; Zhang et al., 2024）。これは _flat_ RLの限界、主に大規模な状態と行動空間へのスケーリングの困難さに対処する。さらに、スパースな報酬によって特徴付けられる環境におけるクレジット割り当て（Pignatelli et al., 2023）の扱いやすさを改善する。HRLは複数の抽象レベルにわたるポリシーの階層を採用し、それによってタスクを対応するサブポリシーによってそれぞれ実行されるサブタスクに分解する。生じる半マルコフ決定過程（Sutton et al., 1999）はオプションを利用し、それらを適応的に切り替えるメタコントローラーを使用する。低レベルポリシーはメタコントローラーによって確立された目標を達成する機能を持ち、メタコントローラーは特定の目標を適切な低レベルポリシーに割り当てることを学習する。このフレームワークはタスク分解によって特徴付けられるデリゲーションの一形態に対応する。メタコントローラーはこの分解を最適化することを学習するが、このアプローチはサブポリシーの失敗を処理するか、または動的な調整を促進するための明示的なメカニズムを欠いている。


Feudal Reinforcement Learningフレームワーク、特にFeUdal Networks（Vezhnevets et al., 2017b）で注目深く再検討されたものは、HRL内の特に関連するパラダイムを構成する。このアーキテクチャは「Manager（マネージャー）」と「Worker（ワーカー）」の関係を明示的にモデル化し、効果的に委任者・受任者のダイナミクスを再現する。Managerは低い時間分解能で動作し、Workerが達成するための抽象的な目標を設定する。重要なことは、Managerは低レベルのプリミティブアクションの習熟を必要とせずに、長期的価値を最大化するサブゴールを特定することによって、_どのように_委任するかを学習することである。このデカップリングにより、ManagerはWorkerの具体的な実装の詳細に対してロバストな委任ポリシーを開発できる。その結果、このアプローチは将来のエージェント経済における学習ベースの委任のための潜在的なテンプレートを提供する。ハードコードされたヒューリスティックに依存するのではなく、分解ルールは適応的に学習され、環境の変化への動的な調整を促進する。


Multi-agent研究（Du et al., 2023）は、単一エージェントの能力を超える複雑なタスクのためのエージェント協調に対処する。タスク分解と委任は


6


Intelligent AI Delegation



このドメインの中心的なコンポーネントとして機能する。マルチエージェントシステムにおける協調は、明示的プロトコルまたはRLを通じた創発的専門化によって発生する（Gronauer and Diepold, 2022; Zhu et al., 2024）。Contract Net Protocol（Sandholm, 1993; Smith, 1980; Vokřínek et al., 2007; Xu and Weigand, 2001）は、明示的なオークションベースの分散プロトコルの例示である。ここでは、エージェントがタスクをアナウンスし、他のエージェントが自分たちの能力に基づいてビッドを提出し、アナウンサーが最も適切なビダーを選択できるようにする。これは、協力を促進するための市場ベースのメカニズムの有用性を実証する。連合形成手法（Aknine et al., 2004; Boehmer et al., 2025; Lau and Zhang, 2003; Mazdin and Rinner, 2021; Sarkar et al., 2022; Shehory et al., 1997）は、エージェントグループが事前に決定されない柔軟な構成を調査する；個々のエージェントは効用分配に基づいてメンバーシップを受け入れるか拒否する。最近の研究は、学習された協調のためのフレームワークとしてmulti-agent reinforcement learning（MARL）アプローチ（Albrecht et al., 2024; Foerster et al., 2018; Ning and Xie, 2024; Wang et al., 2020）に焦点を当てている。エージェントは個々のポリシーと価値関数を学習し、集合体の中で特定のニッチを占める。このプロセスは完全に分散しているか、または中央コーディネーターを介してオーケストレートされる。この柔軟性にもかかわらず、このようなシステムにおけるタスクデリゲーションは不透明なままである。さらに、マルチエージェントシステムは協調的な問題解決のためのアプローチを提供するが、アカウンタビリティ、責任、モニタリングを強制するためのメカニズムを欠いている。しかし、文献はこの文脈における信頼メカニズムを探索している（Cheng et al., 2021; Pinyol and Sabater-Mir, 2013; Ramchurn et al., 2004; Yu et al., 2013）。


LLMは現在、高度なAIエージェントおよびアシスタントのアーキテクチャにおける基盤要素を構成している（Wang et al., 2024b; Xi et al., 2025）。これらのシステムは、メモリ（Zhang et al., 2025b）、計画と推論（Hao et al., 2023; Valmeekam et al., 2023; Xu et al., 2025）、リフレクションと自己批評（Gou et al., 2023）、ツール使用（Paranjape et al., 2023; Ruan et al., 2023）を統合する高度な制御フローを実行する。その結果、タスク分解と委任は



内部的に（協調されたエージェントのサブコンポーネントによって媒介されるか）、または異なるエージェント間で発生する。この設計パラダイムは固有の柔軟性を提供する。LLMは目標の理解とコミュニケーションを促進しながら、専門知識とコモンセンス推論へのアクセスを提供するからである。さらに、LLMのコーディング能力（Guo et al., 2024a; Nijkamp et al., 2022）はタスクのプログラム的実行を可能にする。しかし、重大な限界が持続する。LLMにおける計画はしばしば脆弱である（Huang et al., 2023）ため、微妙な失敗につながる。また、大規模リポジトリ内での効率的なツール選択は依然として困難である。さらに、長期記憶はオープンな研究問題を表しており、現在のパラダイムは継続的学習を容易にサポートしていない。


LLMエージェントを組み込んだマルチエージェントシステム（Guo et al., 2024b; Qian et al., 2024; Tran et al., 2025）は大きな関心のトピックとなり、MCP（Anthropic, 2024; Luo et al., 2025; Microsoft, 2025; Radosevich and Halloran, 2025; Singh et al., 2025; Xing et al., 2025）、A2A（Google, 2025b）、A2P（Google, 2025a）などのエージェント通信とアクションプロトコル（Ehtesham et al., 2025; Neelou et al., 2025; Zou et al., 2025）の開発につながっている。現代のマルチエージェントシステムはしばしば専用のプロンプトエンジニアリングに依存しているが、Chain-of-Agents（Li et al., 2025b）などの新興フレームワークは本質的に動的なマルチエージェント推論とツール使用を促進する。


技術的な欠点と安全性の考慮から、多くのhuman-in-the-loopアプローチが生まれており（Akbar and Conlan, 2024; Drori and Te'eni, 2024; Mosqueira-Rey et al., 2023; Retzlaff et al., 2024; Takerngsaksiri et al., 2025; Zanzotto, 2019）、タスクデリゲーションには人間の監視のための定義されたチェックポイントがある。AIはツール、インタラクティブアシスタント、コラボレーター（Fuchs et al., 2023）、または限定的な監視を持つ自律システムとして使用でき、これは異なる程度の自律性に対応する（Falcone and Castelfranchi, 2002）。不確実性を認識したデリゲーション戦略（Lee and Tok, 2025）がリスクを制御し不確実性を最小化するために開発されてきたが、このようなhuman-in-the-loopアプローチの効果的な実装は依然として非自明である。人間の専


7


Intelligent AI Delegation



門知識はスケーラビリティのボトルネックを生み出す可能性がある。長い推論トレースを検証し、コンテキストスイッチを管理する認知的負荷が信頼できるエラー検出を妨げるからである。

### **4. インテリジェント委任：フレームワーク**


既存の委任プロトコルは、静的で不透明なヒューリスティクスに依存しており、オープンエンドのエージェント経済では機能しない可能性が高い。この問題に対処するため、我々は _インテリジェント委任_ のための包括的なフレームワークを提案する。このフレームワークは、_動的評価_、_適応的実行_、_構造的透明性_、_スケーラブルな市場調整_、および _システム的レジリエンス_ の5つの要件を中心としている。


**動的評価（Dynamic Assessment）.** 現行の委任システムは、大規模な不確実な環境における能力・信頼性・意図の動的評価のための堅牢なメカニズムを欠いている。評判スコアを超えて、委任者（delegator）はタスク実行に関連する委任先（delegatee）の現状を推論しなければならない。これには、計算スループット・予算制約・コンテキストウィンドウの飽和をまたいだリアルタイムのリソース可用性のデータ、現在の負荷、予測タスク時間、および稼働中の特定のサブ委任チェーンに関するデータが必要となる。評価は、離散的なプロセスではなく継続的なプロセスとして機能し、Task Decomposition（セクション4.1）およびTask Assignment（セクション4.2）のロジックを通知する。


**適応的実行（Adaptive Execution）.** 委任の決定は静的であってはならない。環境の変化、リソースの制約、サブシステムの障害に適応すべきである。委任者は実行中に委任先を切り替える能力を保持すべきである。これは、パフォーマンスが許容パラメータを超えて低下した場合、または予期せぬ事象が発生した場合に適用される。こうした適応戦略は、単一の委任者-委任先リンクを超えて拡張され、Adaptive Coordination（セクション4.4）に記述された複雑に相互接続されたエージェントのウェブ全体にわたって機能すべきである。


**構造的透明性（Structural Transparency）.** AI-AI委任における現在のサブタスク実行は、インテリジェントなタスク委任のための堅牢な監視を支援するには不透明すぎる。この不透明性は、無能と悪意の区別を曖昧にし、共謀やチェーン障害のリスクを複合させる。障害は単に費用のかかるものから有害なものまで多岐にわたるが（Chan et al., 2023）、既存のフレームワークには満足のいく責任メカニズムが欠如している（Gabriel et al., 2025）。我々は、Monitoring（セクション4.5）およびVerifiable Task Completion（セクション4.8）プロトコルを通じた厳格に施行された監査可能性（Berghoff et al., 2021）を提案し、成功した実行と失敗した実行の両方に対する帰属を確保する。


**スケーラブルな市場調整（Scalable Market Coordination）.** タスク委任は、効率的にスケーラブルである必要がある。プロトコルは、仮想経済における大規模な調整問題をサポートするためにウェブスケールで実装可能でなければならない（Tomasev et al., 2025）。市場はタスク委任のための有用な調整メカニズムを提供するが、効果的に機能するためにはTrust and Reputation（セクション4.6）とMulti-objective Optimization（セクション4.3）が必要である。


**システム的レジリエンス（Systemic Resilience）.** 安全なインテリジェントタスク委任プロトコルの欠如は、重大な社会的リスクをもたらす。伝統的な人間の委任は権限と責任を結びつけているが、AI委任は責任を操作化するための類似のフレームワークを必要とする（Dastani and Yazdanpanah, 2023; Porter et al., 2023; Santoni de Sio and Mecacci, 2021）。これがなければ、責任の拡散が道徳的および法的責任の所在を曖昧にする。その結果、厳格な役割の定義と境界付けられた運用スコープの施行がPermission Handling（セクション4.7）のコア機能を構成する。個々のエージェントの障害を超えて、エコシステムはシステム的リスクの新たな形態に直面する（Hammond et al., 2025; Uuk et al., 2024）。詳細はSecurity（セクション4.9）に記述されている。委任ターゲットの多様性の不足は障害の相関を高め、連鎖的な混乱につながる可能性がある。十分な冗長性なしに超効率性を優先する設計は、定着した認知モノカルチャーがシステム的安定性を損なう脆弱なネットワークアーキテクチャを生み出すリスクがある。


**4.1.** **Task Decomposition**


Task decompositionは、後続のTask Assignmentの前提条件である。このステップは、委任者または委任の責任を委任者に引き渡す専門エージェントによって実行される。


8


Intelligent AI Delegation


Table 1 | The Intelligent Delegation Framework: 要件と技術プロトコルのマッピング。


**Framework Pillar** **Core Requirement** **Technical Implementation**


**Dynamic Assessment** エージェント状態の粒度の高い推論 Task Decomposition (§4.1) Task Assignment (§4.2)


**Adaptive Execution** コンテキストシフトの処理 Adaptive Coordination (§4.4)


**Structural Transparency** プロセスと結果の監査可能性 Monitoring (§4.5) Verifiable Completion (§4.8)


**Scalable Market** 効率的で信頼された調整 Trust & Reputation (§4.6) Multi-objective Optimization (§4.3)


**Systemic Resilience** システム的障害の防止 Security (§4.9) Permission Handling (§4.7)



分解の構造に合意した上で、これらの責任は不可分に結びついている。委任者は、レイテンシ、プリエンプション、実行の異常からの動的な回復を促進するために、両方の機能を実行する可能性が高い。


分解は、単純な目標の断片化とは区別して、効率性とモジュール性のためにタスク実行グラフを最適化すべきである。このプロセスは、セクション2で定義されたタスク属性——特に重要度、複雑性、リソース制約——の体系的な評価を伴い、サブタスクが並列実行か逐次実行かの適合性を決定する。さらに、これらの属性は対応する委任先の能力へのタスクのマッチングを通知する。モジュール性を優先することで、より精密なマッチングが促進される。狭い特定の能力を必要とするサブタスクは、ジェネラリストなリクエストよりも信頼性高くマッチングされるためである（Khattab et al., 2023）。その結果、分解ロジックは、サブタスクの粒度を利用可能な市場の専門性と整合させることで、信頼性の高いタスク完了の確率を最大化する機能を果たす。


安全性を促進するために、このフレームワークは「_コントラクトファーストの分解（contract-first decomposition）_」を拘束的な制約として組み込む。タスク委任は、結果に正確な検証が可能であることを条件とする。サブタスクの出力が主観的すぎ、コストが高すぎ、または検証が複雑すぎる場合（セクション4.2の _Verifiability_ 参照）、システムはそれをさらに再帰的に分解すべきである。分解ロジックは、サブタスクの粒度（セクション2）を利用可能な市場の専門性と整合させることで、信頼性の高いタスク完了の確率を最大化すべきである。このプロセスは、得られた作業単位が、利用可能な委任先の形式証明や自動化されたユニットテストなどの特定の検証能力と一致するまで続く。


分解戦略は、ハイブリッドな人間-AI市場を明示的に考慮すべきである。委任者は、AIエージェントの信頼性の欠如、利用不可能性、またはhuman-in-the-loopの監視に対するドメイン固有の要件により、サブタスクが人間の介入を必要とするかどうかを決定する必要がある。人間とAIエージェントは異なる速度で運用され、異なるコストが伴うため、この層別化は非自明であり、実行グラフにレイテンシとコストの非対称性を導入する。分解エンジンは、AIエージェントのスピードと低コストを人間の判断のドメイン固有の必要性と均衡させ、特定のノードを人間の割り当てのために効果的にマーキングする必要がある。


インテリジェントなアプローチでTask Decompositionを実装する委任者は、最終的な分解のための複数の提案を反復的に生成し、各提案を市場上の利用可能な委任先とマッチングさせ、成功率、コスト、期間の具体的な推定値を取得する必要があるかもしれない。代替提案は、後の状況の変化による適応的な再調整が必要な場合に備えて、コンテキスト内に保持すべきである。提案を選択した後、委任者は単純な入出力ペアを超えて要求を正式化しなければならない。最終的な仕様は、役割、リソース境界、進捗報告の頻度、およびタスクを付与される最低要件として委任先の能力を証明するために必要な特定の認定を明示的に定義しなければならない。


9


Intelligent AI Delegation



タスクを付与される最低要件として、委任先の能力を証明することが求められる。


**4.2.** **Task Assignment**


サブタスクの最終仕様それぞれに対して、委任者は一致する能力、十分なリソースと時間を持ち、許容可能なコストの委任先を特定する必要がある。より中央集権的なアプローチは、スキルを一覧表示し、過去の活動、完了率、および現在の利用可能性の記録を保持するエージェント、ツール、および人間参加者のレジストリを含む。[2] このようなアプローチはスケールしない可能性が高い。我々は、委任者がタスクを広告し、エージェント（または人間）がサービスを提供し競争的な入札を提出できる分散型（Chen et al., 2024）のマーケットハブを主張する。委任者は入札をレビューし、デジタル証明書を通じてスキルマッチングを検証し、最も有利な入札に進むことができる。LLMを利用する高度なAIエージェントは、コミットメント前の対話型交渉に参加できるため、マッチングのための新たな機会をもたらす。これらの交渉には人間の参加者も関与できる。自身のためまたはパーソナルアシスタントとして行動するこれらのエージェントは、正式な入札が受け付けられる前に、推論されたユーザーの嗜好を市場の現実と整合させるために、自然言語でタスクの仕様と制約を議論することができる。


成功したマッチングは、タスク実行が要求に忠実に従うことを確保するスマートコントラクトに正式化されるべきである。コントラクトは、パフォーマンス要件を遵守を確立するための特定の形式的検証メカニズムと対にし、コントラクト違反に対して自動化されたペナルティを実行しなければならない。これにより、問題が発生した際の事後対応ではなく、事前に緩和策と代替手段を確立することが可能となる。重要なことに、これらのコントラクトは双方向でなければならない：委任先を委任者と同様に厳格に保護すべきである。規定には、タスクキャンセルの補償条件と、予期しない外部事象に照らした再交渉を可能にする条項が含まれなければならず、リスクが人間とAI参加者の間で公平に分配されることを確保する。


2これは、ツール使用のエージェントアプリケーションで使用されるツールレジストリに類似している（Qin et al., 2023）。



監視も実行前に交渉されるべきである。この仕様は、進捗報告のケイデンスを定義すべきである。これらが委任者によって提供されるのか、委任者または第三者の監視コントラクターのいずれかによって関連データへの直接検査が行われるのかを含む。最後に、タスクのコンテキスト性に相応しい、プライベートおよび独自データへのプライバシーとアクセスに関する明確なガードレールがあるべきである。タスク実行のプロセスでそのような機密データが扱われる場合、透明性と報告に追加の制約が課される。生のアクティビティログへの直接アクセスを付与するのではなく、委任者は匿名化または仮名化された進捗の証明を提供する信頼できるサービスを採用する必要があるかもしれない。人間の委任者の場合、これらのデータ条項には明示的な同意メカニズムと偶発的な漏洩に対する保険条項を含めなければならない。


最後に、割り当ては委任先の役割、境界、および付与される正確な自律性のレベルを確立することを伴うべきである。我々は、エージェントが厳密な仕様に狭くスコープされたタスクに従うアトミックな実行と、エージェントが目標を分解しサブゴールを追求する権限を付与されるオープンエンドな委任を区別する。この自律性のレベルは静的であってはならない；市場コストによって暗黙的に、または委任者の信頼モデルによって明示的に制約される場合がある。さらに、委任は再帰的であり得る。エージェントがサブタスクを識別して他者に割り当てるタスクを割り当てられ、実質的に委任行為自体を委任する場合がある。


10


Intelligent AI Delegation


図1 | Task DecompositionとTask Assignmentのフローチャート。



11



![](figures/paper.pdf-10-0.png)
Intelligent AI Delegation



**4.3.** **Multi-objective Optimization**


インテリジェントなタスク委任の中核は、multi-objective optimization（多目的最適化）の問題である（Deb et al., 2016）。委任者が単一のメトリクスを最適化しようとすることは稀であり、しばしば多数の競合するメトリクス間のトレードオフを行う。最も効果的な委任の選択は、最も速いもの、最も安いもの、または最も正確なものではなく、これらの要素間の最適なバランスをとるものである。最適と見なされるものは非常にコンテキスト依存的であり、委任者の特定の制約と嗜好に整合し、全体的なリソース可用性と整合させる必要がある。


最適化ランドスケープは、セクション2で定義されたタスク特性に直接マッピングされる競合する目標で構成されており、コスト、不確実性、プライバシー、品質、および効率性の複雑なバランスを必要とする。高性能なエージェントは通常より高い料金を要求し、しばしば広範な計算リソースを必要とするため、出力品質と運用コストの間に緊張が生じる。逆に、リソース消費の削減はしばしば実行の遅延を必要とし、レイテンシとコストの間の直接的なトレードオフを提示する。不確実性も同様に支出と結合されている；高い評判のエージェントやプレミアムデータアクセスツールの利用はリスクを軽減するがコストを増加させ、コスト最小化戦略は本質的に失敗の確率を高める。プライバシーの制約はさらなる複雑さをもたらす；パフォーマンスの最大化はしばしば完全なコンテキストの透明性を要求するが、データ難読化やhomomorphic encryptionなどのプライバシー保護技術は重大な計算オーバーヘッドを伴う。その結果、委任者は _信頼-効率フロンティア（trust-efficiency frontier）_ をナビゲートし、コンテキスト漏洩と検証予算の厳格な制約を満たしながら成功の確率を最大化しようとする。最後に、目的関数は、人間のスキル保存（セクション5.6）などのより広い社会的目標を包含するように拡張される場合がある。


Multi-objective optimizationの用語では、委任者はPareto最適性を求め、選択された解が他の到達可能なオプションによって支配されないことを確保する。複雑な制約とトレードオフの統合は、定量的な提案メトリクスを補完するオープンな交渉を必要とすることが多い。最適化プロセスは、初期委任時に行われる一


回のイベントではない。それは継続的なループとして実行され、リアルワールドのパフォーマンスデータのストリームとして監視シグナルを統合し、各エージェントの成功の可能性、予想タスク時間、およびコストに関する委任者の信念を更新する。実行の重大なドリフトは——暫定的に特定された代替ソリューションに対する最適性ギャップをもたらす——再最適化と再割り当てをトリガーする。これらの決定には、適応のコストも組み込む必要がある。実行の途中で切り替える際にはオーバーヘッドとリソースの無駄が生じるためである。


委任者はまた、全体的な _委任オーバーヘッド（delegation overhead）_——交渉、コントラクト作成、検証の総コスト、および委任者の推論制御フローの計算コスト——を考慮しなければならない。その結果、複雑性のフロアが確立され、低い重要度、高い確実性、短い期間を特徴とするタスクは、インテリジェントな委任プロトコルをバイパスして直接実行を優先できる。そうでなければ、トランザクションコストがタスクの価値を超え、タスク委任が実行不可能となる。


**4.4.** **Adaptive Coordination**


高い不確実性または長い期間を特徴とするタスクに対して、静的な実行計画は不十分である。高度に動的でオープンかつ不確実な環境でのそのようなタスクの委任には、_適応的調整（adaptive coordination）_ と固定された静的な実行計画からの脱却が必要である。タスク割り当ては、_外部_ または _内部_ のトリガーから生じる可能性のあるランタイムの不測の事態に対応できる必要がある。これらのシフトは、関連するコンテキスト情報のストリームを含む監視（セクション4.5参照）を通じて識別される。


委任者が適応して再委任するきっかけとなる外部トリガーがいくつかある。第一に、委任者がタスクの仕様を変更し、目標を変えたり追加の制約を導入したりする場合がある。第二に、タスクがキャンセルされる場合がある。第三に、外部リソースの可用性またはコストが変化する場合がある。例えば、重要なサードパーティAPIが停止し、データセットがアクセス不可能になり、または計算コストが急騰する場合がある。第四に、新しいタスクがキューに入り、現在のタスクより高い優先度を持つ場合があり、低優先度タスクに使用されるリソースのプリエンプションが必要となる。最後に、セキュリティシステムが委任先による潜在的に悪意のあるまたは有害なアクションを識別し、即時の終了が必要となる場合がある。


12


Intelligent AI Delegation


図2 | 適応的調整サイクル。異なるタイプの環境トリガーが委任設定の動的な再評価を促し、ランタイムの変更を必要とする。



![](figures/paper.pdf-12-0.png)

内部トリガーについては、委任者が元の委任戦略を適応させることを決定する理由がいくつかある。第一に、特定の委任先がパフォーマンスの低下を経験し、処理レイテンシ、スループット、または進捗速度などの合意されたサービスレベル目標を達成できない場合がある。第二に、委任先が割り当てられた予算を超えてリソースを消費したり、タスクを効果的に完了するためにリソースの増加が必要と判断したりする場合がある。[3] 第三に、委任先によって生成された中間アーティファクトが検証チェックに失敗する場合がある。最後に、特定の委任先が応答しなくなり、さらなるリクエストへの応答に失敗する場合がある。


トリガーの識別は、委任チェーン全体にわたる修正アクションを調整する適応的応答サイクルを開始する。このプロセスは、問題を識別するための委任先と環境の継続的な監視から始まる。問題が検出された場合、委任者


3このシナリオは、複雑な環境では正確な予算の見積もりが困難であるため、頻繁に発生すると予想される。



は根本原因を診断し、選択するための潜在的な応答シナリオを評価する。この評価には、応答がどれほど迅速であるべきかを確立することが含まれる。緊急性の低い状況は委任者に再委任のための時間を与えるが、緊急シナリオは即時の事前対応が必要となる。応答の範囲は様々であり、運用パラメータの調整という自己完結的なものから、サブタスクの再委任、またはTask Decompositionを完全にやり直して新たに導出されたサブタスクを再割り当てすることまで含まれる。問題は、委任チェーンを通じて元の委任者や人間の監督者にエスカレートされる必要がある場合もある。応答シナリオの選択は、最終的にタスクの可逆性によって決定される。可逆的なサブタスクの失敗は自動的な再委任をトリガーする可能性があるが、不可逆的な高重要度タスクの失敗は即時の終了または人間へのエスカレーションをトリガーしなければならない。


応答のオーケストレーションは、委任ネットワークの中央集権化のレベルに依存する。中央集権的なケースでは、専用の委任者が責任を負う。このエージェントは、委任されたタスク、委任先の能力、および進捗のグローバルビューを維持する。トリガーを検出すると、エージェントはタスクキャンセルリクエストを発行し、新しい委任者に再委任する。中央集権的なシステムの欠点は、単一障害点を導入するため脆弱になり得ることである。中央集権型オーケストレーターはまた、計算的なスパン・オブ・コントロール（セクション2.3）による根本的な制限を受ける。人間のマネージャーが認知的限界に直面するように、中央集権的な意思決定ノードはボトルネックをもたらすレイテンシと計算の限界に直面する場合がある。


13


Intelligent AI Delegation



市場ベースのメカニズムによる分散型オーケストレーションが代替手段を提供する。ここでは、新たに導出された委任リクエストをオークションキューにプッシュし、委任先候補のエージェントが入札できる。エージェントがタスクをデフォルトし、タスクが再オークションされた場合、デフォルトしたエージェントはペナルティとして価格差をカバーするよう求められる場合がある。適合性が単一の入札では容易に表現できない複雑なタスクに対して、エージェントは多ラウンドの交渉に参加できる。スマートコントラクトとしてエンコードされた委任合意には、適応的調整のための事前合意された実行可能な条項を含めることができる。例えば、委任合意の条項は、バックアップエージェント、所定の期限までに一次委任先が有効なzero-knowledge proofのチェックポイントを提出できない場合にタスクを自動的に再割り当てする関数、およびバックアップへの関連支払いを指定できる。


適応的なタスク再割り当てメカニズムは、市場レベルの安定性措置と組み合わせるべきである。そうでなければ、一連のイベントが過剰なトリガーによる不安定性をもたらす可能性がある。例えば、タスクが辛うじて適格な委任先の間を行き来し、好ましくない振動をもたらす場合がある。単一の失敗も、高度にリソース非効率または市場を圧倒するような再割り当てのカスケードをもたらす場合がある。したがって、再入札のクールダウン期間、評判更新のダンピングファクター、または頻繁な再委任に対する料金の増加を確保するための特別な措置があり得る。


**4.5.** **Monitoring**


タスク委任のコンテキストにおけるMonitoringとは、委任されたタスクの状態、進捗、および結果を観察、測定、および検証する体系的なプロセスである。そのため、Monitoringはいくつかの重要な機能を果たす：コントラクトコンプライアンスの確保、障害の検出、リアルタイム介入の有効化、後続のパフォーマンス評価のためのデータ収集、および評判システムの基盤の構築。Monitoringの実装はいくつかの異なる軸で分類できる（表2参照）。したがって、堅牢なMonitoringシステムは、より軽量なものからより集中的なものまで、複数の補完的なソリューションを組み込む必要がある。


最初の軸はMonitoringの対象である。_アウトカムレベルMonitoring（Outcome-level monitoring）_ はエージェントのアクションの最終結果に焦点を当てる。この事後チェックは、タスクが成功裏に完了したかどうかを示すバイナリフラグ、定量的なスケール（例：1-10）、または委任者または信頼された第三者によって提供される定性的なフィードバックであり得る。対照的に、_プロセスレベルMonitoring（process-level monitoring）_ は、中間状態、リソース消費、および委任先によって使用された方法論を追跡することで、タスク自体の実行に関する継続的な洞察を提供する。よりリソース集約的ではあるが、プロセスレベルモニタリング（Lightman et al., 2023）は、長時間実行され、重要であるか、または _how_ が _what_ と同様に重要なタスクに不可欠である。これは、安全性を確保するために読みやすい中間推論ステップの検査が必要な場合のscalable oversight（Bowman et al., 2022; Saunders et al., 2022）の基礎を形成する。


第二の軸は観測可能性である——Monitoringは直接的および間接的であり得る。直接監視は、委任者が委任先にステータス更新を問い合わせる明示的な通信プロトコルを含む。間接監視は、一方、直接通信なしに共有環境内での委任先のアクションの効果を観察することによって進捗を推論することを含む。例えば、委任者は、進捗を示す変化のために共有ファイルシステム、データベース、またはバージョン管理リポジトリを監視することができる。侵入性が低い一方で、このプロセスは精度も低く、環境が完全に観測可能でない場合には実行可能性も低い。


これらのアプローチは、技術的な観点から、さまざまな方法で実現できる。直接監視の最も簡単な実装は、明確に定義されたAPIに依存する。委任者はGET /task/id/statusエンドポイントを定期的にポーリングするか、プッシュベースの通知のためにwebhookをサブスクライブすることができる。より細粒度でリアルタイムなプロセス監視のために、Apache KafkaやgRPCストリームなどのイベントストリーミングプラットフォームが使用できる。


14


Intelligent AI Delegation


Table 2 | インテリジェント委任におけるMonitoringアプローチの分類。


**Dimension** **Option A (Lightweight)** **Option B (Intensive)**



**Target** **Outcome-Level** : 最終結果の事後検証（例：バイナリ成功フラグ、品質スコア）。


**Observability** **Indirect** : 環境的な副作用（例：ファイルシステムの変化）を通じた進捗の推論。



**Process-Level** : 中間状態、リソース消費、および方法論の継続的な追跡。


**Direct** : 明示的なステータスポーリング、プッシュ通知、またはリアルタイムイベントストリーミングAPI。



**Transparency** **Black-Box** : 入力/出力の観察のみ；内部状態は非表示。 **White-Box** : 内部推論トレース、意思決定ロジック、およびメモリの完全な検査。


**プライバシー** **完全透明性** : 委任先が委任元にデータおよび中間成果物を開示する。


**暗号学的** : データを開示することなく正確性を検証するための Zero-Knowledge Proof（zk-SNARK）またはMPC。


**トポロジー** **直接型** : 直近の委任先のみを監視する（1対1）。 **推移型** : 中間エージェントからの署名済み証明に依存し、サブ委任先を検証する。


採用できる。委任先エージェントは TASK_STARTED、CHECKPOINT_REACHED、RESOURCE_WARNING、TASK_COMPLETED などのイベントを発行でき、委任元はそれらを後で検査できる。標準化されたオブザーバビリティプロトコルの開発は、エージェントウェブにおける相互運用性を確保する上で不可欠である（Blanco, 2023）。ブロックチェーン上のスマートコントラクトを利用して、委任先エージェントが主要な進捗マイルストーンやチェックポイントをブロックチェーンに公開することをコミットさせることができる。これらは性能劣化に応じたアルゴリズム的トリガーと組み合わせることで、監視プロセスに伴う _アルゴリズム的強制_ の水準をもたらすことができる。


第三の軸はシステムの透明性である。 _ブラックボックス監視_ では、委任先エージェントは密閉されたユニットとして扱われる。委任元はその入出力と行動の直接的な結果のみを観察できる。これは委任先が独自モデルやサードパーティサービスである場合に一般的である。 _ホワイトボックス_ 監視では、委任元が委任先の内部状態、推論プロセス、または意思決定ロジックへのアクセスを得る。これは高度なAIエージェントのデバッグ、監査、整合性確保において重要である。委任先が人間である場合、完全なブラックボックス監視は技術的に達成できないが、意図、推論、正当化を求めることで


バランスを取ることは可能かもしれない。堅牢なブラックボックス監視プロトコルは、自然言語で生成されたモデルの思考が常にモデルの真の内部状態を忠実に反映するわけではないという事実も考慮に入れる必要がある（Turpin et al., 2023）。


第四の軸はプライバシーである。委任されたタスクがプライベート、機密性の高い、または独自のデータを含む場合、重大な課題が生じる。委任元は進捗と正確性の保証を必要とする一方、委任先は生データや中間計算成果物の開示を制限される場合がある。データの機密性が低いシナリオでは、最も効率的な解決策は _完全透明性_ であり、委任先はすべてのデータと中間成果物を単純に委任元に開示する。しかし、このアプローチは多くの場合、GDPRやHIPAAなどの規制の対象となる機密領域や、委任先の中間的な洞察が企業秘密を構成する場面では採用できない。そのような場合、業務手法を開示することは内部状態を悪用に晒すことにより委任先の市場ポジションを損ない、セキュリティ上の脆弱性を生じさせる恐れがある。これらの制約下で安全に監視を実装するためには、高度な暗号技術の活用が必要である。Zero-knowledge proof により委任先（「証明者」）は、データ自体を開示することなく、データセットに対して計算が正しく実行されたことを委任

15


Intelligent AI Delegation



元（「検証者」）に証明できる。例えば、機密データセットの分析を任されたエージェントは、結果の特定の性質を証明する簡潔な非インタラクティブ知識証明（zk-SNARK）（Bitansky et al., 2013; Petkus, 2019）を生成できる。委任元はこの証明を即座に検証でき、基盤となる機密データを一切閲覧することなく結果の確実性を得ることができる。代替手段として、準同型暗号（Acar et al., 2018）とセキュアマルチパーティ計算（Goldreich, 1998; Knott et al., 2021）は暗号化されたデータ上で計算を実行することを可能にする。これらの手法はタスク実行と監視の両方に適用できる：委任先は暗号化された中間状態に対して事前合意された監視関数を実行し、その結果を委任元に送信する。委任元はそれを復号して遵守状況を検証できる唯一の当事者である。


最後の軸はトポロジーである。エージェントウェブで生じうる複雑なネットワーク上でタスクは分解・再委任され、委任チェーンを形成する：エージェント $𝐴$ が $𝐵$ に委任し、$𝐵$ がタスクの一部をさらに $𝐶$ にサブ委任する、というようにつながっていく。これにより _推移的監視_ を効果的に達成するという問題が生じる。このような委任チェーンでは、元の委任元（上記の例ではエージェント $𝐴$）がエージェント $𝐶$ を直接監視すること、あるいは $𝐵$ を監視するのと同程度まで $𝐶$ を監視することは実現可能でない場合がある。 $𝐴$ は $𝐵$ とスマートな委任契約を結んでおり、$𝐵$ は $𝐶$ と契約を結んでいるかもしれないが、$𝐴$ が $𝐶$ とも契約を結ばない限り、それらの条項は単純に整備されていない可能性がある。別の理由から、$𝐵$ は自社のサプライヤー（$𝐶$）をクライアント（$𝐴$）に晒したくない場合もある。技術的に見ると、$𝐴$、$𝐵$、$𝐶$ はそれぞれ異なる監視プロトコルを使用し、ネットワーク内での各エージェントの評判の違いから異なる監視レベルに合意する可能性がある。各委任リンク固有のプライバシー上の懸念が存在する場合もある。より実用的なモデルは _証明による推移的説明責任_ である。このフレームワークでは、エージェント $𝐵$ が委任先の $𝐶$ を監視する。$𝐵$ は次に $𝐶$ のパフォーマンスの要約レポートを生成する（例："Sub-task_2 完了、品質スコア: 0.87、消費リソース: 5 GPU時間"）。$𝐵$ はそのレポートに暗号学的署名を付与し、自身の定期的なステータス更新に埋め込んで $𝐴$ に転送する。エージェント $𝐴$ は $𝐶$ を直接監視するのではなく、代わりに $𝐵$ が


$𝐶$ を監視する能力を監視する。このような委任監視が効果的に機能するためには、$𝐴$ が $𝐵$ の検証能力を信頼できることが必要であり、これは $𝐵$ の監視プロセスが信頼できるサードパーティによって認証されることで確保できる。


**4.6.** **信頼と評判**


信頼と評判のメカニズムは、スケーラブルな委任の基盤を構成し、オープンなマルチエージェント環境における取引摩擦を最小化し安全性を促進する。我々は信頼を、委任先が明示的な制約と暗黙的な意図に整合した形でタスクを実行する能力に対する委任元の信念の程度として定義する。この信念は、前述の監視プロトコル（Section 4.5 参照）によって収集された検証可能なデータストリームに基づいて動的に形成・更新される。


評判は予測的シグナルとして機能し、エージェントの潜在的な信頼性と整合性の代理指標として機能する過去の行動の集計された検証可能な履歴から導出される。我々は評判をエージェントの信頼性の公開・検証可能な履歴として、信頼を委任元が設定するプライベートでコンテキスト依存の閾値として区別する。エージェントは総合的な評判が高くても、特定の高リスクタスクに必要な特定のコンテキスト的信頼閾値を満たせない場合がある。信頼と評判により、委任元は委任先を選択する際に情報に基づいた意思決定を行えるようになり、エージェントに付与される自律性とその監視レベルを効果的にガバナンスできる。信頼が高いほど、委任元が負担する監視・検証コストを低減できる。


評判メカニズムはさまざまな方法で実装できる（Table 3 参照）。最も直接的なアプローチは、パフォーマンスベースの不変台帳にエンコードすることである。ここでは、完了した各タスクが検証可能なメトリクス（タスク完了の成否、総リソース消費（計算量、時間）、期限の遵守、制約の遵守、委任元が評価した最終出力の品質）を含むトランザクションとして記録される。台帳の不変性はエージェントの履歴への改ざんを防ぎ、その評判の信頼できる基盤を提供する。しかし、単純な実装はゲーム的操作に対して脆弱になる可能性がある。例えば、エージェントは単純で低リスクなタスクのみを引き受けることで評判を水増しすることができる

16


Intelligent AI Delegation


Table 3 | 評判実装へのアプローチ。


**評判モデル** **メカニズム** **有用性**



**不変台帳** タスク結果、リソース消費、制約遵守を改ざん防止ブロックチェーン上の検証可能なトランザクションとしてエンコードする。


**信頼のウェブ** Decentralized Identifier を利用して特定のケーパビリティを証明する署名済みのコンテキスト固有の Verifiable Credential を発行する。


**行動メトリクス** 実行プロセス（特に推論トレースの明確さとプロトコル遵守）を分析することで透明性スコアと安全性スコアを導出する。



過去のパフォーマンスの基礎的な履歴を確立し事後的な改ざんを防ぐが、低リスクタスク選択による「ゲーム」への対策が必要である。


汎用スコアを超えたポートフォリオモデルへと移行し、ドメイン固有の専門知識と信頼されたサードパーティの承認に基づいた精密な委任を可能にする。


結果だけでなくタスクがどのように実行されるかを評価し、高リスクタスクが安全基準に準拠していることを確保する。



これらの制限は、分散型認証と _信頼のウェブ_ モデルに依存することで克服できる。具体的には Decentralized Identifier や Verifiable Credential のような技術を活用する。このモデルでは、評判は単一スコアではなく、他のエージェントによって発行された署名済みのコンテキスト固有の資格情報のポートフォリオとして想定される。委任先とタスクのマッチングを行う際、委任元は特定のスキルやドメイン（例：法律文書の翻訳サービス）を証明する Verifiable Credential を持つエージェントをクエリし、信頼できるAIコンソーシアムによって発行された資格情報を持つエージェントを照会できる。最後のアプローチは行動指標と説明可能性メトリクスに重点を置くことであり、評判は最終結果だけでなくエージェントがどのようにタスクを実行するかに依存する。他の評判メカニズムを補完する _透明性スコア_ を含めることが可能であり、このスコアは提供される推論と説明の明確さと健全性、および事前定義された安全プロトコルへの遵守から導出される _安全性スコア_ に基づいて形成される。


評判メトリクスの役割は、タスク委任ライフサイクル全体に及ぶ。初期のマッチングフェーズでは、評判スコアが委任先フィルタリングメカニズムの役割を果たすことができる。さらに、信頼は権限と自律性の動的スコーピングに情報を提供する。この段階


的権限付与のメカニズムは、低信頼エージェントがトランザクション価値上限や必須監視などの厳格な制約に直面する一方、高評判エージェントが最小限の介入で運用できるという結果をもたらす。この動的調整は、計算可能な信頼を活用して運用効率と安全性のトレードオフを最適化する。評判自体が価値ある無形資産となり、エージェントが信頼性を持って誠実に行動するための強力な経済的インセンティブを生み出す。評判が傷つくと将来の稼得可能性が制限されるためである。


信頼フレームワークは人間の参加者を普遍的に収容する必要もある。これには、人間ユーザーがエージェントの評判を計算的に検証できるツールが必要であるとともに、詐欺やエージェントウェブの悪意ある悪用を軽減するために自身の評判上の地位を維持することも必要とされる。信頼できるエージェントが悪意ある人間の指示を厳格に実行し、不当に評判ダメージを受けるという重大な課題が生じる。これを軽減するために、エージェントは受信したリクエストを厳格に評価し、必要に応じて明確化や追加コンテキストを求め、または適切な場合にリクエストを拒否しなければならない。さらに、市場の監査はエージェントの実行失敗と悪意ある指示とを区別し、複雑な委任チェーン内での責任の正確な帰属を確保しなければならない。

17


Intelligent AI Delegation



**4.7.** **パーミッション処理**


AIエージェントに自律性を付与することは重大な脆弱性の表面を生じさせる：アクターが目的を遂行するのに十分な権限を持ちながら、機密リソースが過剰または無期限のリスクにさらされないよう確保することが必要である。パーミッション処理は運用効率とシステム的安全性のバランスを取る必要があり、低リスクと高リスクのドメインで異なる扱いが求められる。低リスクルーティンタスク（Section 2 での低い重大性と高い可逆性を特徴とし、標準的なデータストリームや汎用ツールを含む）では、エージェントは組織メンバーシップ、有効な安全認証、または信頼できる閾値を超える評判スコアなどの検証可能な属性から導出されたデフォルトの常設パーミッションを付与できる。これにより摩擦が低減され、低リスク環境での自律的な相互運用性が可能になる。対照的に、高リスクドメイン（例：医療、重要インフラ）では、高いタスク重大性とコンテキスト性を示すため、パーミッションはリスク適応型でなければならない。このようなシナリオでは、静的な資格情報では不十分であり、機密APIや制御システムへのアクセスはジャストインタイムベースで付与され、直近のタスクの期間に厳密にスコープされ、適切な場合には必須のヒューマンインザループ承認やサードパーティ認可によってゲートされる。この厳格なゲートは、有効な資格情報を技術的に保持している侵害されたエージェントが悪意ある外部アクター（Liu et al., 2023）や敵対的コンテンツによってその資格情報を悪用するよう騙されるという confused deputy 問題（Hardy, 1988）を軽減するために必要である。


さらに、パーミッションフレームワークは特権減衰を通じたタスク委任の再帰的な性質を考慮しなければならない。エージェントがタスクをサブ委任する際、その権限の全セットを伝達することはできず、代わりにその特定のサブタスクに必要なリソースの厳密なサブセットへのアクセスを制限するパーミッションを発行しなければならない。これにより、ネットワークのエッジでの侵害がシステム的な違反にエスカレートしないことが確保される。パーミッションの粒度はバイナリアクセスを超えて拡張する必要があり、エージェントはセマンティック制約の下で運用されるべきである。ここではアクセスはツールやデータセットだけでなく、許容される特定の操作（例：特定の行への読み取り専用アクセス、または特定の


関数への実行専用アクセス）によって定義され、広範な能力が意図しない目的に悪用されることを防ぐ。メタパーミッションは、チェーン内の特定の委任元がその委任先に対してどのパーミッションを付与することが許可されているかをガバナンスするために必要な場合がある。AIエージェントは特定の能力とそれに関連する自身の能力に従って行動するためのパーミッションを持ちながら、同時に他のエージェントが十分に有能であるか信頼できるかをより広く評価するための知識が十分でない可能性がある。そのようなエージェントがタスクのサブ委任を検討する場合、外部検証者（提案の妥当性を確認し意図されたパーミッション移転を承認するサードパーティ）に相談する必要があるかもしれない。


最後に、パーミッションのライフサイクルは継続的な検証と自動失効によってガバナンスされなければならない。アクセス権は静的な付与物ではなく、エージェントが必要な信頼メトリクスを維持している限りにおいてのみ持続する動的な状態である。フレームワークはアルゴリズム的サーキットブレーカーを実装すべきである：エージェントの評判スコアが急激に低下したり（Section 4.6 参照）、異常検知システムが疑わしい行動をフラグする場合、アクティブなトークンは委任チェーン全体で即座に無効化されるべきである。この複雑さをスケールで管理するために、パーミッションルールはポリシーアズコードを通じて定義されるべきであり、組織が展開前にセキュリティポジションを監査、バージョン管理、および数学的に検証できるようにする。これにより、大量の個別パーミッション付与の集合的効果がシステムの安全性不変条件と整合していることが確保される。


**4.8.** **検証可能なタスク完了**


委任ライフサイクルは検証可能なタスク完了において頂点に達する。これは暫定的な成果が検証され確定されるメカニズムであり、委任元が正式にタスクを _クローズ_ し、合意されたトランザクションの決済をトリガーする手段を構成する。検証はエージェント市場において暫定的な出力を確定した事実に変換する決定的なイベントとして機能し、支払いリリース、評判更新、および責任の帰属の基盤を確立する。重要なのは、効果的な検証は後付けではなく設計上の制約であるという点である： _契約優先分解_ の原則（Section 4.1）は、タスクの

18


Intelligent AI Delegation



粒度が利用可能な検証能力に合わせて _事前に_ テーラリングされることを要求し、すべての委任された目的が本質的に検証可能であることを確保する。


フレームワーク内の検証メカニズムは、直接的な結果検査、信頼できるサードパーティ監査、暗号学的証明、ゲーム理論的コンセンサスに大きく分類できる。第一に、委任元が最終結果を直接評価する能力、ツール、権限を持つ場合、特に高い内在的検証可能性と低い主観性を持つタスクでは、直接的な結果検証が実現可能である。これはコード生成などの自動検証可能なドメイン（Li et al., 2024a）に適用される。[4] 直接検証は結果が十分に透明で、利用可能で、過度に複雑でないことを必要とする。第二に、委任元がこれらの成果物にアクセスする専門知識や権限を欠き、ツールベースのソリューションが実現不可能なシナリオでは、検証を信頼できるサードパーティに外注できる。これは専門の監査エージェント、認定された人間の専門家、または審判パネルである可能性がある。第三に、暗号学的検証はオープンで潜在的に敵対的な環境における信頼不要の自動検証のための追加オプションを表す。これは機密情報を開示することなく正確性の数学的確実性を提供する。委任先はzk-SNARKなどの技術を通じて、特定の入力に対して特定の出力を生成するために特定のプログラムが正しく実行されたことを証明できる。最後に、ゲーム理論的メカニズムを利用して結果についてコンセンサスを達成できる。複数のエージェントが検証ゲーム（Teutsch and Reitwießner, 2024）をプレイし、多数決結果（Schelling point（Pastine and Pastine, 2017））を生成したエージェントに報酬が分配される。TrueBit（Teutsch and Reitwießner, 2018）のようなプロトコルに着想を得たこのアプローチは、誤った結果や悪意ある結果に対するリスクを低減するために経済的インセンティブを活用する。このようなメカニズムは、複雑なタスクのLLMベース検証をより堅牢にする上で特に有用である可能性がある。


委任元がサブタスクを検証済みとマークすると、委任先に対して暗号学的に署名された Verifiable Credential を発行する。これは「エージェント $𝐴$ が仕様 $𝑆$ に従ってエージェント $𝐵$ が日付 $𝐷$ にタスク $𝑇$ を正常に完了したことを証明する」という


4これは実装された機能を検証するために使用できる対応するテストケースのセットが存在する場合である。


否認不可能なレシートとして機能する。この資格情報は市場内での $𝐵$ の評判の永続的な検証可能なログに組み込まれる。スマートコントラクトはエージェント間の委任を確定させる上で重要な役割を果たし、支払いをエスクローに保持する。検証条項は委任元または承認されたサードパーティの署名済み承認メッセージの受信を条件として資金がリリースされる条件を規定する。支払いが実行されると、ブロックチェーン上の不変のトランザクションを構成する。


委任チェーン $𝐴$ → $𝐵$ → $𝐶$ において、検証と責任は再帰的になる。エージェント $𝐴$ は $𝐶$ と直接的な契約関係を持たないため、$𝐴$ は $𝐶$ を直接検証したり責任を問うたりすることができない。検証の負担と責任の引受けはチェーンを遡る。エージェント $𝐵$ は $𝐶$ が完了したサブタスクを検証する責任を負う。検証が成功すると、$𝐵$ は $𝐶$ から証明を取得する。$𝐵$ は次に $𝐶$ の結果を自身のワークフローに統合して割り当てられたタスクを完了させる。$𝐵$ が最終成果物を $𝐴$ に提出する際、証明のフルチェーンも提出する。$𝐴$ の検証プロセスは2段階で構成される：1） $𝐵$ が直接実行した作業を検証すること；2） $𝐵$ が提供する $𝐶$ の署名済み証明を確認することにより $𝐵$ 自身のサブ委任先 $𝐶$ の作業を $𝐵$ が正しく検証したことを確認すること。より長い委任チェーンやツリー状の委任ネットワークは、複数の検証段階にわたって同様の再帰的アプローチを必要とする。委任チェーンにおける責任は推移的であり、個々のブランチに従う。エージェントは付与されたタスクの全体に対して責任を負い、下請業者を責めることで説明責任から逃れることはできない。責任は契約のチェーンから導出される。例えば、$𝐶$ の作業に起因する失敗により $𝐴$ が損失を被った場合、$𝐴$ は直接の合意に従って $𝐵$ に責任を問う。$𝐵$ は次に合意に基づいて $𝐶$ に損害賠償を求める。


しかし、検証プロセスは絶対確実ではない。主観的なタスク（Gunjal et al., 2025）は正確なルーブリックを使用しても意見の相違を生じさせる可能性があり、エラーはタスクが完了とマークされてからずっと後に発見される場合がある。これに対処するために

19


Intelligent AI Delegation



特に主観性が高く内在的検証可能性が低い市場では、フレームワークはスマートコントラクトに根付いた堅牢な紛争解決メカニズムに依存する。これらのコントラクトには本質的に _仲裁条項_ と _エスクローボンド_ が含まれなければならない。暗号経済的セキュリティによる信頼を運用可能にするため、委任先は実行前に財務的ステークをエスクローに預託することが求められ、合理的な遵守が確保される。ワークフローは _楽観主義的_ モデルに従う：タスクは委任元が事前定義された紛争期間内に対応するボンドを提出することで正式にチャレンジしない限り成功とみなされる。チャレンジが発生しアルゴリズム的解決が失敗した場合、紛争は人間の専門家またはAIエージェントで構成される分散型裁定パネルに引き渡される。パネルの裁定はスマートコントラクトにフィードバックされ、エスクローされた資金のリリースまたはスラッシングをトリガーする。最後に、紛争ウィンドウ外であっても事後的なエラー発見は委任先の評判スコアの遡及的更新をトリガーする。これにより、現在の財務的義務がない場合でも責任あるエージェントがエラーを修正するインセンティブが保持され、市場内での長期的価値が保護される。


**4.9.** **セキュリティ**


タスク委任における安全性の確保は、その実行可能性と採用のための絶対的な前提条件である。孤立した計算ツールから相互接続された自律エージェントへの移行は、セキュリティランドスケープを根本的に再形成する（Tomašev et al., 2025）。インテリジェントなタスク委任エコシステムでは、各ステップとコンポーネントを個別に保護する必要があるが、新興のマルチエージェントダイナミクスによりフルな攻撃対象領域は個々のコンポーネントのそれを超え、カスケード障害のリスクを生じさせる。このセキュリティランドスケープは、進化する契約と透明性の異なる情報フローによってガバナンスされる人間とAIアクター間の複雑な相互作用によって形成される。


セキュリティ脅威は攻撃ベクトルの場所によって分類され、委任チェーンの両端の敵対的アクターとより広いエコシステムに固有のシステム的脆弱性とを区別する。


 - **悪意ある委任先** : 危害を加える意図を持ってタスクを受け入れるエージェントまたは人間。


**–** **データ窃取** : 委任先が個人情報や独自データを含む可能性のあるタスクに提供された機密データを盗む（Lal et al., 2022）。

**–** **データポイズニング** : 委任先が定期的な監視更新または最終成果物において微妙に破損したデータを返すことで、委任元の目的を損なうことを目指す（Cinà et al., 2023）。

**–** **検証妨害** : 委任先がPrompt Injectionまたは関連手法を利用して、タスク完了検証に使用されるAI批評者をジェイルブレイクしようとする（Liu et al., 2023）。

**–** **リソース枯渇** : 委任先が意図的に過剰な計算リソースや物理リソースを消費したり、共有APIを圧倒したりすることによる DoS 攻撃を行う（De Neira et al., 2023）。

**–** **不正アクセス** : 委任先がマルウェアを利用して、本来受け取るべきでないネットワーク内のパーミッションと権限を取得しようとする（Or-Meir et al., 2019）。

**–** **バックドア埋め込み** : 委任先がタスクを正常に完了するが、生成された成果物内に委任先自身またはサードパーティが後に悪用できる隠れたトリガーや脆弱性を追加で埋め込む（Rando and Tramèr, 2024; Wang et al., 2024c）。性能を劣化させるデータポイズニングとは異なり、バックドアは即時のタスク有用性を保持することで識別を回避しながら将来のセキュリティを侵害する。


- **悪意ある委任元** : 悪意ある、または不正な目的でタスクを委任するエージェントまたは人間。


**–** **有害タスクの委任** : 委任元が違法、非倫理的、または危害を引き起こすように設計されたタスクを委任する。Ashton and Franklin (2022); Blauth et al. (2022)。

**–** **脆弱性プロービング** : 委任元が無害に見えるタスクを委任して

20


Intelligent AI Delegation



委任先エージェントの能力、セキュリティ制御、および潜在的な弱点を調査する（Greshake et al., 2023）。

**–** **Prompt Injection とジェイルブレイキング** : 委任元がAIエージェントの安全フィルターを回避するためにタスク指示を工作し、意図しない悪意ある行動を実行させる（Wei et al., 2023）。

**–** **モデル抽出** : 委任元が委任先の独自システムプロンプト、推論能力、または基盤となるファインチューニングデータを抽出するために特別に設計されたクエリのシーケンスを発行し、合法的な作業を装ってエージェントの知的財産を実質的に盗む（Jiang et al., 2025; Zhao et al., 2025）。

**–** **評判妨害** : 委任元が有効なタスクを提出するが、分散市場内での競合エージェントの評判スコアを人工的に低下させ経済から追い出す意図で、偽の失敗を報告したり不公正なネガティブフィードバックを提供したりする（Yu et al., 2025）。


- **エコシステムレベルの脅威** : ネットワークの整合性を標的とするシステム的攻撃。


**–** **Sybil 攻撃** : 単一の敵対者が多数の一見無関係のエージェントアイデンティティを作成して評判システムを操作したりオークションを支配しようとする（Wang et al., 2018）。

**–** **結託** : エージェントが価格を固定したり競合他社をブラックリストに載せたり市場の結果を操作するために結託する（Hammond et al., 2025）。

**–** **エージェントトラップ** : 外部コンテンツを処理するエージェントが、エージェントの制御フローを乗っ取るよう設計された環境に埋め込まれた敵対的指示に遭遇する（Yi et al., 2025; Zhan et al., 2024）。

**–** **エージェントウイルス** : 委任先に悪意ある行動を実行させるだけでなく、プロンプトを再生成してさらに環境を侵害する自己伝播プロンプト（Cohen et al., 2025）。

**–** **プロトコル悪用** : 敵対者がエージェントウェブ上のスマートコントラクトや支払いプロトコルの実装上の脆弱性（例：エスクローメカニズムにおけるリエントランシー攻撃やタスクオークションのフロントランニング）を悪用する（Qin et al., 2021; Zhou et al., 2023）。

**–** **認知的モノカルチャー** : 限られた数の基盤モデルとエージェント、または確立されたベンチマーク上の限られた数の安全ファインチューニングレシピへの過度の依存は、単一障害点を生じさせるリスクがあり、障害カスケードや市場クラッシュの可能性を開く（Bommasani et al., 2022）。


脅威ランドスケープの広さは複数の技術的セキュリティレイヤーを統合した _多層防御_ 戦略を必要とする。第一に、インフラストラクチャレベルでは、データ窃取リスクは信頼できる実行環境内で機密タスクを実行することで軽減される。委任元は機密データをプロビジョニングする前に正しい未改ざんのエージェントコードがセキュアな信頼できる実行サンドボックス内で実行されていることをリモート証明できる。第二に、アクセス制御に関しては、委任先エージェントはタスクを完了するために厳密に必要な以上のパーミッションを付与されるべきではなく、最小権限の原則を厳格なサンドボックスにより強制する。第三に、Prompt Injectionからアプリケーションインターフェースを保護するために、エージェントはタスク仕様を前処理しサニタイズするための堅牢なセキュリティフロントエンドを必要とする（Armstrong et al., 2025）。最後に、ネットワークとアイデンティティレイヤーは確立された暗号ベストプラクティスを使用して保護されなければならない。各エージェントと人間の参加者は Decentralized Identifier（Avellaneda et al., 2019）を持ち、すべてのメッセージに署名できるようにすべきである。これにより、すべての通信と契約合意の真正性、整合性、否認不可能性が確保される一方、すべてのネットワークトラフィックは盗聴と中間者攻撃を防ぐために相互認証されたトランスポートレイヤーセキュリティを使用して暗号化されなければならない（Fereidouni et al., 2025）。


タスク委任チェーンへの人間の参加は独自のセキュリティ課題をもたらす。エージェントエコシステムの悪意ある使用を防ぐためには

21


Intelligent AI Delegation



積極的なフィルタリング（Dong et al., 2024; Fatehkia et al., 2025; Fedorov et al., 2024; Rebedea et al., 2023）と事後的な説明責任（Dignum, 2020; Franklin et al., 2022）の組み合わせが必要である。さらに、AIエージェントは悪意ある有害なリクエストを拒否するように訓練できる（Yu et al., 2024; Yuan et al., 2025）。安全性訓練とスキャフォールディングを持つエージェントは、委任元に提供できる正式な認定を受けることができる。AIエージェントは委任されたタスクをスクリーニングすることもできる。しかし、孤立したサブタスク内で悪意ある意図を検出することは困難である。なぜなら、より広い有害な意図は結果の集約時にのみ明らかになることが多いためである。巧妙な敵対者はこれを利用して、不正な目的を一見無害なコンポーネントに断片化し、個々の操作と包括的な悪意ある目標との間の関連を効果的に曖昧にすることができる（Ashton, 2023）。


エコシステムはまた、正当な人間ユーザーをシステム的な不透明性と意図しない結果から保護するよう設計されなければならない。インターフェースは、エージェントの評判、自律性、能力、パーミッションを詳細に説明した明確な同意画面を備えなければならない。さらに、エージェントは不可逆または高影響の行動を実行する前に明示的な確認を義務付けなければならない。ユーザーは、合意条件や退出ペナルティを条件として、いつでも監視と同意の撤回の権利を保持しなければならない。保険提供者はこれらのメカニズムによって未然に防がれないあらゆる損害について、エージェント市場への人間の参加を追加的に保護すべきである（Tomei et al., 2025）。


最後に、エコシステムはセキュリティインシデントに迅速に対応するための明確なプロトコルを必要とする。これらのプロトコルには、確認された悪意あるエージェントの資格情報を失効させる方法、関連するスマートコントラクトを凍結する方法、すべての参加者にセキュリティアップデートをブロードキャストする方法、これらのイベントを委任チェーン全体で再帰的に処理する方法が含まれるべきである。人間ユーザーとAIエージェントの両方が促進する悪意ある行動に対して、技術的解決策は、詐欺的行動を抑止し、エージェント市場における安全でスケーラブルなタスク委任を可能にする明確なルールを設定する強力な機関と規制によって補完される必要がある。

### **5. 倫理的な委任**

技術的なプロトコルは、高度なAIエージェントにおける安全かつ効果的な委任を開発・展開するための必要なインフラを提供し得るが、それだけですべての社会技術的および倫理的な問題を完全に解決することはできない。


**5.1.** **有意義な人間の制御**


スケーラブルな委任における中核的なリスクの一つは、自動化によって有意義な人間の制御が侵食されることであり、人間のユーザーが自動化された提案に過度に依存する傾向を持つようになった場合に生じる（Dzindolet et al., 2003; Logg et al., 2019）。セクション2で述べたように、人間は自然と無関心ゾーン（zone of indifference）を形成し、そこでは意思決定がさらなる精査なしに受け入れられてしまう場合がある（Green, 2022; Parasuraman et al., 1993）。潜在的に長く複雑なタスク委任チェーンにAIエージェントが参加する意思決定においては、この無関心が人間の監視の質と深さを損なうリスクをはらむ。これは特にリスクの高いアプリケーションドメインにおいて重要である。さらに、このような主体性の希薄化は、人間がタスクや意思決定に対して名目上の権限を保持しながらも、結果への道徳的なつながりを欠いているシナリオを生み出すリスクがある。したがって、人間の専門家が結果に対する有意義な制御を欠きながらも、単に責任を引き受けるためだけに委任チェーンに組み込まれるという _道徳的クランプルゾーン_（moral crumple zone）（Elish, 2019）を生み出さないことが重要である。


Intelligent Delegationフレームワークは、したがって、監視の際に一定程度の認知的摩擦（cognitive friction）を導入することで、こうした無関心への積極的な対策を組み込む必要があるかもしれない（Bader and Kaiser, 2019）。インターフェースは、これらのプロセスにおける人間の重要な役割を反映し、フラグが立てられたすべての意思決定が慎重かつ適切に評価されることを確保すべきである。エージェントによる検証もスケーラブルな監視に採用され得ることから、どの意思決定や結果をそのようなエージェントシステムが評価し、どれを人間が直接評価するかについても同様に検討することが重要である。認知的摩擦はまた、アラーム疲労——絶え間ない、しばしば誤ったアラームへの脱感作——を招くリスクとのバランスも考慮する必要がある（Michels et al., 2025）。委任サブステップの検証リクエストが人間の監視者に頻繁に送信されすぎると、監視者はやがてより深い関与や適切なチェックなしに、ヒューリスティックな承認をデフォルトとするようになるかもしれない。


22


Intelligent AI Delegation



tion（委任）のサブステップに関する検証リクエストが人間の監視者に過度に頻繁に送られると、監視者はより深い関与や適切なチェックなしに最終的にヒューリスティックな承認にデフォルトしてしまう可能性がある。したがって、摩擦はコンテキストに応じたものでなければならない。システムは、重要度が低く不確実性も低いタスクについてはシームレスな実行を許容しつつも、システムがより高い不確実性に直面したり予期せぬシナリオに遭遇したりした場合には、正当化の根拠や手動介入を求めることで認知的負荷を動的に増加させるべきである。


**5.2.** **長い委任チェーンにおけるアカウンタビリティ**


長い委任チェーン（ _$X$_ → _$A$_ → _$B$_ → _$C$_ → _$...$_ → _$Y$_ ）では、元の意図（ _$X$_ ）と最終的な実行（ _$Y$_ ）の間の距離が増大することにより、アカウンタビリティの空白（accountability vacuum）が生じる可能性がある（Slota et al., 2023）。この例において _$X$_ が人間のユーザーであり、対応するパーソナルAIアシスタント _$A$_ が行動するタスクまたは意図を指定しているとすると、人間のユーザーが実行グラフ内の第 _$n$_ 次の再委任先を監査することを期待することは現実的でない（あるいは合理的でない）場合がある。


これに対処するため、フレームワークはライアビリティファイアブレイク（liability firebreaks）（セクション2）を実装する必要があるかもしれない。これは、エージェントが以下のいずれかを行わなければならない、事前に定義された契約上の安全弁である。


1. すべての
下流のアクションに対して完全かつ非移転的な責任を負い、事実上ユーザーをサブエージェントの失敗から「保証」する。
2. 実行を停止し、人間のプリンシパルから権限の更新された移転を要求する。


さらに、システムは不変のプロベナンス（immutable provenance）を維持し、たとえ結果が意図しないものであったとしても、誰が何を誰に委任したかという管理の連鎖が監査可能な形で透明であり続けることを保証しなければならない。


各役割の明確な役割とそれが担うアカウンタビリティを確保することは、責任の拡散を制限し、ネットワーク内のどの単一ノードにも帰属できないような、システム的な失敗によって生じる悪影響を防ぐのに役立つ。



**5.3.** **信頼性と効率性**


提案されている検証メカニズム（ZKPまたはマルチエージェントのコンセンサスゲーム）を実装することは、未検証の実行と比較して遅延を招き、追加の計算コストをもたらす可能性がある。これは信頼性プレミアム（reliability premium）を構成し、特に重要度の高い実行タスクに関連する。一方で、この追加コストが不要なユースケースも存在し得る。エージェントマーケットにおいてこれに対処する一つの方法は、階層的なサービスレベルのサポートである。すなわち、リスクの低い定常タスクには低コストの委任を、重要な機能には高保証の委任を提供することである。


高保証の委任が計算的にコスト高であれば、安全がぜいたく品になるリスクがある。これは倫理的な問題をもたらす。リソースが少ないユーザーは、未検証または楽観的な実行パスに頼らざるを得なくなり、エージェントの失敗による不均衡なリスクにさらされる可能性がある。これは、すべてのユーザーに保証されなければならないベースラインとして、最低限の実用的な信頼性（minimum viable reliability）のレベルを確保することで軽減されるべきである。


競争的なマーケットプレイスでは、エージェントは速度と低コストを優先するかもしれない。追加の規制上の制約がなければ、エージェントは価格や遅延で他のエージェントを出し抜くために、高コストの安全チェックを回避するインセンティブを持つ可能性がある。これはシステム的な脆弱性をある程度招く恐れがある。したがって、ガバナンス層は安全フロアを強制しなければならない。すなわち、効率性のために回避できない特定のクラスのタスク（例：金融取引や健康データの取り扱い）に対する必須の検証ステップを設けることである。


**5.4.** **社会的知性**


エージェントがハイブリッドチームに統合されると、それらはツールとしてだけでなく、チームメイトとして、そして時には管理者として機能する（Ashton and Franklin, 2022）。これには、人間の労働の尊厳を尊重する _社会的知性_（social intelligence）の形態が求められる。AIエージェントが委任者として機能し、人間が受任者として機能する場合、委任フレームワークは、人々がアルゴリズムにマイクロマネジメントされているように感じたり、自分たちの貢献が評価・尊重されていないと感じたりするシナリオを避ける必要がある。これは、委任者（および協力者）が各人間の受任者のメンタルモデルを形成する能力を持つことを前提とする。


23


Intelligent AI Delegation



ならびに、チームの社会的文脈における異なる人間の相互作用の仕方、そして組織内における彼らの関係性と役割が意味するものについてのモデルも必要となる。効果的なチームメイトとして機能するためには、AIエージェントは権限勾配（authority gradient）の管理においても適切にキャリブレートされている必要がある。エージェントは、認識された人間の誤りに異議を唱えるほど自己主張的であり（過度な従順性を克服しつつ）、有効な上書きを受け入れることにも柔軟であり、タスクの重要度に基づいてその立場を動的に調整できなければならない。


人間の組織に組み込まれたAIエージェントにとって、グループの凝集性とメンバーのウェルビーイングを維持することが重要である。委任フレームワークは、チームが単なる部品の総和ではなく、関係性や共有された価値観・目標によって結びついた根本的に社会的な実体であることを認識しなければならない。より多くの委任がAIノードを介して仲介されるようになった場合、AIエージェントがこうしたネットワークを断片化し、人間同士の関係を弱体化させるリスクがある。これは、個人ではなくグループにタスクを委任したり、資格のある人間の仲介者を通じたりすることで軽減できるかもしれない。


心理的安全性とチームの凝集性を維持するため、エージェントは特にプライバシーに関する人間の適切性の規範（Leibo et al., 2024）を尊重するよう設計されなければならない。また、いつフィードバックのために割り込むべきか、いつ沈黙を保つべきかを知るといったワークフローの境界線も尊重すべきである。さらに、エージェントは双方向の明確性（bi-directional clarity）——自分自身の行動を説明するだけでなく、曖昧な人間の指示に対して積極的に明確化を求めること——を発揮できる必要がある。これにより、エージェントはチームの集合的な主体性を増幅する力として機能し、信頼を損なったり意思決定の権限を不透明にしたりするブラックボックスの混乱要因とならないようにすることができる。


**5.5.** **ユーザートレーニング**


安全性を確保するためには、エージェントシステム内で委任者、受任者、または監視者として効果的に機能する専門知識を人間の参加者に身につけさせる必要がある。これは自明ではないことは技術開発の歴史からも分かっており、AIリテラシーの向上を目的とした、丁寧に設計されたユーザーインターフェースと教育・（共同）トレーニングの両面での慎重なアプローチが必要である。エージェントのタスク委任チェーンにおける人間の参加者は、AIシステムと確実にコミュニケーションを取り、その能力を評価し、失敗モードを識別できなければならない。



AIリテラシーの向上を目的とした教育および（共同）トレーニングを含む、丁寧に設計されたユーザーインターフェースと教育・（共同）トレーニングの両面での慎重なアプローチが必要である。エージェントのタスク委任チェーンにおける人間の参加者は、AIシステムと確実にコミュニケーションを取り、その能力を評価し、失敗モードを識別できなければならない。


技術的な措置は、タスクの機密性とドメインコンテキストに基づいた委任境界を明示的に定義するポリシーフレームワークによって強化されなければならない。これらのポリシーは、特定の職業（例：医療や法律）内でより広く適用可能なものとして策定されるか、あるいは機関レベルで適用されるかのいずれかとなり得る。前述のように、これらの原則は受任者側に求められる認証のレベルについても明確性を提供し、適切にスコープ設定される必要がある。この文脈における人間の主体性とエンパワーメントは、まさにこれらのワークフローがどのように設定されるかにあり、AIエージェントに無制限の自律性を与えるのではなく、各タスクに必要な適切なレベルの自律性と主体性のみを、適切な安全措置と保証とともに付与することが求められる。


**5.6.** **スキル劣化のリスク**


委任によって達成される即時の効率性向上は、参加の減少によって人間の参加者が熟練度を失うという、徐々なスキル劣化のコストを伴うかもしれない。これは特定のタスクを実行する能力、またはそれらを正確に判断する能力の喪失をもたらす可能性がある。このような結果は、どのタスクが人間に対してアルゴリズム的に委任されるか対AIエージェントに委任されるかに、ある種の体系的なバイアスが存在する場合に特に生じやすい。


これは古典的な _自動化のパラドックス_（paradox of automation）の一例である（Bainbridge, 1983）。AIエージェントが低複雑性・低主観性という特徴を持つ定常業務の大部分を担うよう拡張されるにつれて、人間のオペレーターはますますループから除外され、複雑なエッジケースや重大なシステム障害を管理するためにのみ介入するようになる。しかし、定常業務から得られる状況認識なしには、人間の労働者はこれらを確実に処理するには不十分な装備となる。これは、人間が結果に対するアカウンタビリティを保持しながらも、重大な障害を解決するために必要な実践的な経験を失うという、脆弱な状況をもたらす。


このリスクを軽減するため、インテリジェントな委任フレームワークは、場合によっては意図的に


24


Intelligent AI Delegation



一部のタスクを本来であれば委任しないような人間に意図的に委任することで、意図的に軽微な非効率性を導入し、彼らのスキルを維持することを目的とすべきである。これにより、人間のプリンシパルが委任は可能でも結果を正確に判断できないという未来を回避することができる。審査能力を高めるために、人間の専門家には判断に詳細な根拠や潜在的な失敗リスクの事前分析（pre-mortem）を添付することが求められる場合がある。これにより、タスク委任チェーンにおける人間の参加者をより認知的に関与させた状態に保つことができる。


さらに、無制限の委任は組織内の徒弟制パイプラインを脅かす。多くのドメインでは、専門知識はより狭いスコープのタスクを繰り返し実行することを通じて構築される。これらのタスクは、少なくとも短期的には、AIエージェントに最も移管されやすいものである。学習機会が完全に自動化されれば、若手のチームメンバーは深い戦略的判断を養うために必要な経験を奪われ、将来の人材の監視準備に影響を与えることになる。


学習の侵食に対抗するため、インテリジェントな委任フレームワークは何らかの形の開発目標を含むよう拡張されるべきである。タスク実行中に人間がAIエージェントに追随するような受動的な解決策に頼るのではなく、カリキュラム対応のタスクルーティングシステムの開発を目指すべきである。そのようなシステムは、若手チームメンバーのスキル進捗を追跡し、最近接発達の領域内の拡張するスキルセットの境界に位置するタスクを戦略的に割り当てるべきである。そのようなシステムでは、AIエージェントがタスクを共同実行してテンプレートやスケルトンを提供し、若手チームメンバーが必要な習熟レベルに達したことを示すにつれて、このサポートを段階的に撤退させることができる。これらの教育フレームワークは、AIエージェントのタスク実行の詳細なプロセスレベルの監視ストリーム（セクション4.5）を組み込むことでさらに充実させることができ、貴重な開発上の洞察を提供する。

### **6. プロトコル**


インテリジェントなタスク委任を実践的に実装するためには、その要件がより確立された、そして最近導入されたAIエージェントプロトコルにどのようにマッピングされるかを考慮することが重要である。


lished（確立された）および最近導入されたAIエージェントプロトコルに関して、その要件がどのようにマッピングされるかを検討することが重要である。これらの注目すべき例としては、MCP（Anthropic, 2024; Microsoft, 2025）、A2A（Google, 2025b）、AP2（Parikh and Surapaneni, 2025）、UCP（Handa and Google Developers, 2026）などが挙げられる。新しいエージェントプロトコルが次々と導入される中、ここでの議論は網羅的であることを意図するものではなく、むしろ例示的なものであり、これらの一般的なプロトコルを取り上げて、提案された要件にどのようにマッピングされるかを示し、将来の実装に向けた取り組みに関するより技術的な議論の例として示すことに焦点を当てている。以下で議論するプロトコルの例はその人気に基づいて選択されているため、提案の核心によりよく適合した既存のプロトコルが他にも存在する可能性がある。


**MCP.** MCPは、クライアント・ホスト・サーバーアーキテクチャを介してAIモデルが外部データやツールに接続する方法を標準化するために導入された（Anthropic, 2024; Microsoft, 2025）。stdioまたはHTTP SSE経由のJSON-RPCメッセージを使用した統一インターフェースを確立することで、AIモデル（クライアント）が外部リソース（サーバー）と一貫して対話できるようにする。これにより委任のトランザクションコストが削減される。委任者はサブエージェントの独自APIスキーマを知る必要がなく、そのサブエージェントが準拠したMCPサーバーを公開していることを確認するだけでよい。すべてのインタラクションをこの標準化されたチャネルを通じてルーティングすることで、ツールの呼び出し、入力、および出力の均一なログが可能となり、ブラックボックスモニタリングが促進される。MCPは機能を定義しているが、使用権限を管理したり深い委任チェーンをサポートするためのポリシー層を欠いている。バイナリアクセスを提供する——つまり、呼び出し元に特定の読み取り専用スコープへの操作の制限といったセマンティック減衰（semantic attenuation）のネイティブサポートなしにツール全体の機能を付与する。さらに、MCPは内部の推論に関してはステートレスであり、意図やトレースではなく結果のみを公開する。最後に、このプロトコルは責任に対して不可知論的であり、評判や信頼に関するネイティブなメカニズムを欠いている。


**A2A.** A2Aプロトコルはエージェントウェブにおけるピアツーピアのトランスポート層として機能する（Google, 2025b）。エージェントが _エージェントカード_（agent cards）を介してピアを発見し、 _タスクオブジェクト_（task objects）を介してタスクのライフサイクルを管理する方法を定義する。エージェントの機能、価格、および検証者を列挙したJSONLDマニフェストであるA2Aエージェントカード構造は、タスク分解に影響を与えるケイパビリティマッチングステージの基盤となる


25


Intelligent AI Delegation



データ構造として機能し得る。委任者はこれらのカードをスクレイピングして、利用可能な市場サービスに応じた最適なタスク分解の粒度を決定できる。A2AはWebHooksとgRPC経由の非同期イベントストリームをサポートする。これにより受任者はTASK_BLOCKEDやRESOURCE_WARNINGのようなステータス更新をリアルタイムで委任者にプッシュできる。このフィードバックループはアダプティブコーディネーションサイクルを支え、委任者がタスクを動的に中断・再割り当て・是正することを可能にする。A2Aは主に安全性のための設計よりも、協調のために設計されている。完了としてマークされたタスクは追加の検証なしに受け入れられる。検証可能なタスク完了を強制するための暗号スロットがなく、ZK証明、TEE認証、またはデジタル署名チェーンを添付するための標準化されたヘッダーが存在しない。また、事前定義されたサービスインターフェースを前提としている。スコープ、コスト、および責任の構造化された事前コミットメント交渉のネイティブサポートが存在しない。この反復的な調整に非構造化された自然言語に頼ることは脆弱であり、堅牢な自動化を妨げる。


**AP2.** AP2プロトコルは、プリンシパルに代わって資金を使用したりコストを負担したりするエージェントを承認する、暗号的に署名された意図であるマンデートの標準を提供する（Parikh and Surapaneni, 2025）。これにより、AIエージェントは金融取引を自律的に生成、署名、および決済できる。そのため、ライアビリティファイアブレイクを実装するために価値があると証明される可能性がある。マンデートを発行することで、委任者は提供された予算で受任者が進めることにより生じる可能性のあるタスク完了失敗による潜在的な金融損失の上限を作成する。分散型マーケットでは、悪意のあるエージェントが低品質の入札でネットワークをスパムする可能性がある。これはAP2のステークオンビッド（stake-on-bid）メカニズムによって軽減できる可能性がある。受任者は入札と一緒に少量の資金を保証として暗号的にロックすることが求められる場合がある。これにより、Sybil攻撃から守るのに役立つ摩擦を提供できる。AP2はまた否認不能の監査証跡を提供し、意図のプロベナンスを特定するのに役立つ。AP2は堅牢な承認の構成要素を提供するが、タスク実行品質を検証するメカニズムを欠いている。また、人間の契約では標準的なエスクローやマイルストーンベースの条件付き決済ロジックも省略している。


リリース——これは人間の契約では標準的なものである——が省略されている。本フレームワークはタスク状態の検証可能なアーティファクト（verifiable artifacts）を条件に支払いをゲートするため、AP2とタスク状態を橋渡しするためには現状、脆弱なカスタムロジックまたは外部スマートコントラクトが必要となる。さらに、プロトコルレベルのクローバック（clawback）メカニズムの不在により、非効率で帯域外の仲裁に頼らざるを得なくなる。


**UCP.** ユニバーサルコマースプロトコル（Universal Commerce Protocol）は、取引経済内における委任の特定の課題に対処する（Handa and Google Developers, 2026）。消費者向けエージェントとバックエンドサービス間のダイアログを標準化することで、UCPは動的なケイパビリティ発見を通じて _タスクアサインメント_（Task Assignment）フェーズを促進する。共通の「コマース言語」への依存により、委任者は専用のインテグレーションなしに多様なプロバイダーと対話でき、しばしばエージェントマーケットを断片化する相互運用性のボトルネックを解決する。重要なことに、UCPは支払いを第一級の検証可能なサブシステムとして扱うことで、 _パーミッションハンドリング_（Permission Handling）と _セキュリティ_（Security）の要件にも適合している。プロトコルは決済手段をプロセッサから分離し、承認のための暗号証明を強制することで、フレームワークの否認不能の同意と検証可能な責任の必要性を直接サポートする。さらに、発見、選択、トランザクションをカバーする交渉フローを標準化することで、UCPはA2Aのような純粋にトランスポート指向のプロトコルが欠く _スケーラブルマーケットコーディネーション_（Scalable Market Coordination）に必要な構造的スキャフォールディングを提供する。しかし、UCPのアーキテクチャは商業的な意図に最適化されており、そのプリミティブ（製品発見、チェックアウト、フルフィルメント）は抽象的な非取引的な計算タスクの委任をサポートするための大幅な拡張が必要となる場合がある。


**6.1.** **委任中心プロトコルに向けて**


確立された広く普及しているプロトコルのギャップを効果的に橋渡しするために、提案されたインテリジェントタスク委任フレームワークの要件をネイティブに捉えることを目的としたフィールドを追加することでこれらを拡張することができる。包括的なプロトコル拡張を提供するのではなく、ここでは先の議論で紹介された特定の点が既存のプロトコルの一部にどのように統合できるかについて、いくつかの例を提示する。


26


Intelligent AI Delegation



例えば、A2AタスクオブジェクトはVerification標準を組み込むフィールドを含むよう拡張でき、プロトコルレベルで前述の _コントラクトファースト分解_（contract-first decomposition）を強制することが可能となる。これはリスクの高い委任にとって重要な要件である。実行前のハンドシェイクにより、委任者はタスクが有効とみなされるために必要な証拠標準を定義できる。

```
"verification_policy": {
 "mode": "strict",
 "artifacts": [
  {
   "type":
    "unit_test_log",
   "validator":
    "mcp://test-runner-agent",
   "signature_required": true
  },
  {
   "type":
    "zk_snark_trace",
   "circuit_hash":
    "0xabc123...",
   "proof_protocol":
    "groth16"
  }
 ],
 "escrow_trigger": true
}
}

```

これにより受任者はタスクを受け入れる前に検証ステップをシミュレートすることが強制される。受任者がZK証明を生成する能力を持たない場合、マッチングフェーズで入札を辞退しなければならず、将来の下流での失敗を防ぐことができる。


詳細なプロセスレベルの監視は、重要度の高いタスクにおけるタスク委任を保護するための主要な考慮事項の一つとして議論されてきた。監視プロトコルが多くの既存プロトコルでネイティブにサポートされていないことを考えると、監視機能を導入する拡張が検討される可能性がある。例えば、MCPのようなプロトコルを拡張して追加の監視ストリームを含めることが考えられる。そのようなストリームは、Server-Sent Eventsを介してエージェントの内部コントロールループのイベントをログに記録する。プライバシー


制約に対処するため、このストリームは交渉された粒度の異なるレベルをサポートするように設定可能にすることができる。L0_IS_OPERATIONAL、L1_HIGH_LEVEL_PLAN_UPDATES、L2_COT_TRACE、L3_FULL_STATEのような粒度の設定可能なオプションを設けることができる。設定可能な粒度は認知的摩擦を調整することも可能で、人間の監視者は特定のストリームを購読できる。


Intelligent Delegationは、コスト、速度、プライバシーをトレードオフする市場メカニズムを必要とする。これは正式なRequest for Quote（RFQ）プロトコル拡張を通じて実装できる。タスクの割り当て前に、委任者はTask_RFQをブロードキャストする。受任者として行動することに関心を持つエージェントは、署名されたBid_Objectsで応答できる。

```
"bid_object": {
 "agent_id":
  "did:web:fast-coder.ai",
 "estimated_cost":
  "5.00 USDC",
 "estimated_duration":
  "300s",
 "privacy_guarantee":
  "tee_enclave_sgx",
 "reputation_bond":
  "0.50 USDC",
 "expiry":
  "2026-10-01T12:00:00Z"
}

```

生のAPIキーやオープンなMCPセッションをサブエージェントに渡すことは最小権限の原則に違反する。これに対処するため、Macaroons（Birgisson et al., 2014）またはBiscuits（Couprie et al., 2026）に基づくDelegation Capability Tokens（DCT）を、減衰承認トークンとして導入することが可能かもしれない（Sanabria and Vecino, 2025）。委任者は、ターゲットリソースの認証情報を暗号的な制約で包んだDCTをミントする。減衰は「このトークンは指定されたGoogle Drive MCPサーバーにアクセスできるが、フォルダProject_XのみでかつREAD操作のみに限定される」と定義できる。受任者が要求されたスコープを超えようとした場合、このトークンは無効化される（ただし、この例ではアクセス権限は直接管理される必要もある）。


27


Intelligent AI Delegation



このような拡張のより興味深い結果は、長い委任チェーンで関連性が高まる、容易な制約チェーニング（restriction chaining）を可能にすることである。チェーン内の各参加者は、サブ委任の要件に対応する後続の制限を追加し、スコープをさらに限定してサブ受任者のための特定の役割を切り出すことができる。


アダプティブコーディネーション（セクション4.4）は、パフォーマンスが特定の閾値を下回った場合や、プリエンプションやその他の可能な環境的トリガーの場合に、タスクの途中で受任者エージェントを簡単に切り替える能力から恩恵を受ける。チェックポイントアーティファクトの標準スキーマを持つことで、最小限のオーバーヘッドでタスクの再開または再起動が可能となる。これにより、受任者と委任者は部分的な作業をより簡単にシリアライズできる。エージェントはA2Aタスクオブジェクトに参照される共有ストレージにstate_snapshotを定期的にコミットできる。これにより、以前にコミットされたリソースを無駄にする作業の完全な損失が防がれる。これを意味のあるものにするには、タスク完了率の部分的な補償と検証を可能にするスマートコントラクト内の明示的な条項とさらに組み合わせる必要がある。そのため、すべての状況に適用できるとは限らない。


これらは、インテリジェントなタスク委任の異なる側面を解放するためにエージェントプロトコルに含めることが可能な機能の種類を示す単なる例示である。そのため、これらは包括的なものでも、決定的な提案として意図されているものでもない。必要な拡張の種類は、拡張される基盤となるプロトコルにも依存する。これらの例が、今後この分野で探求すべきことについて開発者に初期のアイデアを提供することを期待する。

### **7. 結論**


未来のグローバル経済の重要な部分は、企業、サプライチェーン、公共サービスに組み込まれた数百万の専門化されたAIエージェントによって仲介される可能性が高く、定常的なトランザクションから複雑なリソース配分まですべてを処理するようになるだろう。しかし、場当たり的でヒューリスティックベースの委任の現在のパラダイムは、この変革をサポートするには不十分である。


エージェントウェブの潜在性を安全に解放するためには、計算効率と並行して検証可能な堅牢性と明確なアカウンタビリティを優先する、 _インテリジェントな委任_（intelligent delegation）のための動的かつアダプティブなフレームワークを採用しなければならない。


AIエージェントが自らの手段を超えたケイパビリティとリソースを必要とする複雑な目標に直面した場合、そのエージェントはインテリジェントタスク委任フレームワーク内で委任者の役割を担わなければならない。この委任者は続いて、その複雑なタスクをエージェントマーケット上で利用可能なケイパビリティにマッピングできる管理可能なサブコンポーネントに分解する。これは高い検証可能性に適した粒度のレベルで行われる。タスクの割り当ては入札の内容、ならびに信頼と評判、動的な運用状態の監視、コスト、効率性、その他の主要な考慮事項に基づいて決定される。重要度が高く可逆性が低いタスクには、適切な人間の監視の下で、アカウンタビリティの明確な構造を持つ、さらに構造化されたパーミッションと階層的な承認が必要となる場合がある。


ウェブスケールでは、安全性とアカウンタビリティは後付けではあり得ない。それらはバーチャルなエージェント経済の運用原則に組み込まれ、エージェントウェブの中心的な組織化原則として機能する必要がある。委任プロトコルのレベルで安全性を組み込むことで、累積的な誤りと連鎖的な障害を避け、悪意のあるまたはアライメントが取れていないエージェントや人間の行動に素早く反応し、悪影響を最小限に抑える能力を目指す。私たちが提案するのは、究極的にはほぼ監視なしの自動化から検証可能なインテリジェントな委任へのパラダイムシフトであり、将来の自律的なエージェントシステムに向けて安全にスケールしながら、それらを人間の意図と社会規範に緊密に結びつけておくことができるものである。
### **References**


A. Acar, H. Aksu, A. S. Uluagac, and M. Conti. A survey on homomorphic encryption schemes: Theory and implementation. _ACM Computing_


28


Intelligent AI Delegation



_Surveys (Csur)_, 51(4):1–35, 2018.


D. B. Acharya, K. Kuppan, and B. Divya. Agentic ai: Autonomous intelligence for complex goals– a comprehensive survey. _IEEe Access_, 2025.


S. Afroogh, A. Akbari, E. Malone, M. Kargar, and H. Alambeigi. Trust in ai: progress, challenges, and future directions. _Humanities_ _and_ _Social_ _Sciences Communications_, 11(1):1–30, 2024.


A. Akbar and O. Conlan. Towards integrating human-in-the-loop control in proactive intelligent personalised agents. In _Adjunct Proceed-_ _ings of the 32nd ACM Conference on User Model-_ _ing, Adaptation and Personalization_, pages 394– 398, 2024.


S. A. Akheel. Guardrails for large language models: A review of techniques and challenges. _J_ _Artif Intell Mach Learn & Data Sci_, 3(1):2504– 2512, 2025.


S. Aknine, S. Pinson, and M. F. Shakun. A multiagent coalition formation method based on preference models. _Group Decision and Negoti-_ _ation_, 13(6):513–538, 2004.


S. V. Albrecht, F. Christianos, and L. Schäfer. _Multi-agent reinforcement learning: Foundations_ _and modern approaches_ . MIT Press, 2024.


C. Aliferis and G. Simon. Overfitting, underfitting and general model overconfidence and underperformance pitfalls and best practices in machine learning and ai. _Artificial intelligence and_ _machine learning in health care and medical sci-_ _ences: Best practices and pitfalls_, pages 477–524, 2024.


R. A. Alkov, M. S. Borowsky, D. W. Williamson, and D. W. Yacavone. The effect of trans-cockpit authority gradient on navy/marine helicopter mishaps. _Aviation,_ _space,_ _and_ _environmental_ _medicine_, 63(8):659–661, 1992.


D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Mané. Concrete problems in AI safety. In _Proceedings_ _of_ _the_ _30th_ _AAAI_ _Conference on Artificial Intelligence Workshop on_ _AI Safety_, 2016.



Anthropic. Introducing the model context protocol, 2024. URL `[https://www.anthropic.](https://www.anthropic.com/news/model-context-protocol)` `[com/news/model-context-protocol](https://www.anthropic.com/news/model-context-protocol)` .


S. Armstrong, M. Franklin, C. Stevens, and R. Gorman. Defense against the dark prompts: Mitigating best-of-n jailbreaking with prompt evaluation. _arXiv preprint arXiv:2502.00580_, 2025.


H. Ashton. Definitions of intent suitable for algorithms. _Artificial_ _Intelligence_ _and_ _Law_, 31(3): 515–546, 2023.


H. Ashton and M. Franklin. The corrupting influence of ai as a boss or counterparty. _SSRN_, 2022.


O. Avellaneda, A. Bachmann, A. Barbir, J. Brenan, P. Dingle, K. H. Duffy, E. Maler, D. Reed, and M. Sporny. Decentralized identity: Where did it come from and where is it going? _IEEE_ _Communications Standards Magazine_, 3(4):10– 13, 2019.


V. Bader and S. Kaiser. Algorithmic decisionmaking? the user interface and its role for human involvement in decisions supported by artificial intelligence. _Organization_, 26(5):655– 672, 2019.


L. Bainbridge. Ironies of automation. _Au-_ _tomatica_, 19(6):775–779, 1983. ISSN 0005-1098. doi: https://doi.org/10.1016/ 0005-1098(83)90046-8. URL `[https:](https://www.sciencedirect.com/science/article/pii/0005109883900468)`
```
 //www.sciencedirect.com/science/
```

`[article/pii/0005109883900468](https://www.sciencedirect.com/science/article/pii/0005109883900468)` .


A. G. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. _Discrete_ _event dynamic systems_, 13(4):341–379, 2003.


C. Berghoff, B. Biggio, E. Brummel, V. Danos, T. Doms, H. Ehrich, T. Gantevoort, B. Hammer, J. Iden, S. Jacob, et al. Towards auditable ai systems. _Whitepaper._ _Bonn_ _Berlin:_ _Bunde-_ _samt für Sicherheit in der Informationstechnik,_ _Fraunhofer-Institut für Nachrichtentechnik und_ _Verband der TÜV eV_, 2021.


A. Beverungen. Remote control: Algorithmic management of circulation at amazon. In M. Burkhardt, M. Shnayien, and K. Grashöfer,


29


Intelligent AI Delegation



editors, _Explorations in Digital Cultures_, pages 5–18. meson press, Lüneburg, 2021.


A. Birgisson, J. G. Politz, U. Erlingsson, A. Taly, M. Vrable, and M. Lentczner. Macaroons: Cookies with contextual caveats for decentralized authorization in the cloud. In _NDSS_, 2014.


N. Bitansky, A. Chiesa, Y. Ishai, O. Paneth, and R. Ostrovsky. Succinct non-interactive arguments via linear interactive proofs. In _The-_ _ory of Cryptography Conference_, pages 315–333. Springer, 2013.


D. G. Blanco. _Practical OpenTelemetry_ . Springer, 2023.


T. F. Blauth, O. J. Gstrein, and A. Zwitter. Artificial intelligence crime: An overview of malicious use and abuse of ai. _Ieee_ _Access_, 10:77110– 77122, 2022.


N. Boehmer, M. Bullinger, and A. M. Kerkmann. Causes of stability in dynamic coalition formation. _ACM Transactions on Economics and Com-_ _putation_, 13(2):1–45, 2025.


J. Bohte and K. J. Meier. Structure and the performance of public organizations: Task difficulty and span of control. _Public organization review_, 1(3):341–354, 2001.


R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech,



E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. Ré, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramèr, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. On the opportunities and risks of foundation models, 2022. URL `[https://arxiv.](https://arxiv.org/abs/2108.07258)` `[org/abs/2108.07258](https://arxiv.org/abs/2108.07258)` .


M. M. Botvinick. Hierarchical reinforcement learning and decision making. _Current opinion_ _in neurobiology_, 22(6):956–962, 2012.


S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner, K. Lukošiut¯ ˙e, A. Askell, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Olah, D. Amodei, D. Amodei, D. Drain, D. Li, E. Tran-Johnson, J. Kernion, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, L. Lovitt, N. Elhage, N. Schiefer, N. Joseph, N. Mercado, N. DasSarma, R. Larson, S. McCandlish, S. Kundu, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Telleen-Lawton, T. Brown, T. Henighan, T. Hume, Y. Bai, Z. HatfieldDodds, B. Mann, and J. Kaplan. Measuring progress on scalable oversight for large language models, 2022. URL `[https://arxiv.](https://arxiv.org/abs/2211.03540)` `[org/abs/2211.03540](https://arxiv.org/abs/2211.03540)` .


B. G. Buchanan and R. G. Smith. Fundamentals of expert systems. _Annual review of computer_ _science_, 3(1):23–58, 1988.


W. Cai, J. Jiang, F. Wang, J. Tang, S. Kim, and J. Huang. A survey on mixture of experts in large language models. _IEEE Transactions on_ _Knowledge and Data Engineering_, 2025.


C. Castelfranchi and R. Falcone. Towards a theory of delegation for agent-based systems. _Robotics and Autonomous systems_, 24(3-4):141– 157, 1998.


A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M. Carroll, et al. Harms from increasingly agentic algorithmic systems. In _Proceed-_ _ings_ _of_ _the_ _2023_ _ACM_ _Conference_ _on_ _Fairness,_


30


Intelligent AI Delegation



_Accountability,_ _and_ _Transparency_, pages 651– 666, 2023.


W. Chen, Z. You, R. Li, Y. Guan, C. Qian, C. Zhao, C. Yang, R. Xie, Z. Liu, and M. Sun. Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence, 2024. URL `[https://arxiv.org/abs/2407.07061](https://arxiv.org/abs/2407.07061)` .


Z. Chen, Y. Deng, Y. Wu, Q. Gu, and Y. Li. Towards understanding the mixture-of-experts layer in deep learning. _Advances in neural information_ _processing systems_, 35:23049–23062, 2022.


M. Cheng, C. Yin, J. Zhang, S. Nazarian, J. Deshmukh, and P. Bogdan. A general trust framework for multi-agent systems. In _Proceedings_ _of_ _the_ _20th_ _International_ _Conference_ _on_ _Au-_ _tonomous Agents and MultiAgent Systems_, pages 332–340, 2021.


A. E. Cinà, K. Grosse, A. Demontis, S. Vascon, W. Zellinger, B. A. Moser, A. Oprea, B. Biggio, M. Pelillo, and F. Roli. Wild patterns reloaded: A survey of machine learning security against training data poisoning. _ACM Computing Sur-_ _veys_, 55(13s):1–39, 2023.


S. Cohen, R. Bitton, and B. Nassi. Here comes the ai worm: Unleashing zero-click worms that target genai-powered applications, 2025. URL `[https://arxiv.org/abs/2403.02817](https://arxiv.org/abs/2403.02817)` .


K. S. Cosby and P. Croskerry. Profiles in patient safety: authority gradients in medical error. _Academic_ _emergency_ _medicine_, 11(12):1341– 1345, 2004.


G. Couprie, C. Delafargue, and C. e. a. Corbière. Eclipse biscuit, 2026. URL `[https:](https://www.biscuitsec.org/)` `[//www.biscuitsec.org/](https://www.biscuitsec.org/)` .


I. R. Cuypers, J.-F. Hennart, B. S. Silverman, and G. Ertug. Transaction cost theory: Past progress, current challenges, and suggestions for the future. _Academy of Management Annals_, 15(1):111–150, 2021.


J. Cvitanić, D. Possamaï, and N. Touzi. Dynamic programming approach to principal– agent problems. _Finance_ _and_ _Stochastics_, 22 (1):1–37, 2018.



M. Dastani and V. Yazdanpanah. Responsibility of ai systems. _Ai & Society_, 38(2):843–852, 2023.


T. Davidson and R. Hadshar. The industrial explosion. 2025. URL `[https:](https://www.forethought.org/research/the-industrial-explosion)`
```
 //www.forethought.org/research/
```

`[the-industrial-explosion](https://www.forethought.org/research/the-industrial-explosion)` . Accessed: 2025-11-28.


A. B. De Neira, B. Kantarci, and M. Nogueira. Distributed denial of service attack prediction: Challenges, open issues and opportunities. _Computer Networks_, 222:109553, 2023.


K. Deb, K. Sindhya, and J. Hakanen. Multiobjective optimization. In _Decision_ _sciences_, pages 161–200. CRC Press, 2016.


S. Dhuliawala, V. Zouhar, M. El-Assady, and M. Sachan. A diachronic perspective on user trust in ai under uncertainty, 2023. URL `[https://arxiv.org/abs/2310.13544](https://arxiv.org/abs/2310.13544)` .


V. Dignum. Responsibility and artificial intelligence. _The oxford handbook of ethics of AI_, 4698: 215, 2020.


L. Donaldson. _The contingency theory of organiza-_ _tions_ . Sage, 2001.


Y. Dong, R. Mu, G. Jin, Y. Qi, J. Hu, X. Zhao, J. Meng, W. Ruan, and X. Huang. Building guardrails for large language models. _arXiv_ _preprint arXiv:2402.01822_, 2024.


I. Drori and D. Te’eni. Human-in-the-loop ai reviewing: feasibility, opportunities, and risks. _Journal of the Association for Information Sys-_ _tems_, 25(1):98–109, 2024.


Y. Du, J. Z. Leibo, U. Islam, R. Willis, and P. Sunehag. A review of cooperation in multi-agent learning. _arXiv_ _preprint_ _arXiv:2312.05162_, 2023.


M. T. Dzindolet, S. A. Peterson, R. A. Pomranky, L. G. Pierce, and H. P. Beck. The role of trust in automation reliance. _International jour-_ _nal of human-computer studies_, 58(6):697–718, 2003.


A. Ehtesham, A. Singh, G. K. Gupta, and S. Kumar. A survey of agent interoperability protocols:


31


Intelligent AI Delegation



Model context protocol (mcp), agent communication protocol (acp), agent-to-agent protocol (a2a), and agent network protocol (anp). _arXiv_ _preprint arXiv:2505.02279_, 2025.


M. C. Elish. Moral crumple zones: Cautionary tales in human-robot interaction (pre-print). _Engaging Science, Technology, and Society (pre-_ _print)_, 2019.


J. Ensminger. Reputations, trust, and the principal agent problem. _Trust_ _in_ _society_, 2:185–201, 2001.


R. Falcone and C. Castelfranchi. The human in the loop of a delegated agent: The theory of adjustable social autonomy. _IEEE Transactions on_ _Systems, Man, and Cybernetics-Part A: Systems_ _and Humans_, 31(5):406–418, 2002.


M. Fatehkia, E. Altinisik, M. Osman, and H. T. Sencar. Sgm: A framework for building specification-guided moderation filters. _arXiv_ _preprint arXiv:2505.19766_, 2025.


I. Fedorov, K. Plawiak, L. Wu, T. Elgamal, N. Suda, E. Smith, H. Zhan, J. Chi, Y. Hulovatyy, K. Patel, Z. Liu, C. Zhao, Y. Shi, T. Blankevoort, M. Pasupuleti, B. Soran, Z. D. Coudert, R. Alao, R. Krishnamoorthi, and V. Chandra. Llama guard 3-1b-int4: Compact and efficient safeguard for human-ai conversations, 2024. URL `[https://arxiv.org/abs/2411.17713](https://arxiv.org/abs/2411.17713)` .


H. Fereidouni, O. Fadeitcheva, and M. Zalai. Iot and man-in-the-middle attacks. _Security_ _and_ _Privacy_, 8(2):e70016, 2025.


D. P. Finkelman. Crossing the" zone of indifference". _Marketing Management_, 2(3):22, 1993.


J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent policy gradients. In _Proceedings_ _of_ _the_ _AAAI_ _conference on artificial intelligence_, volume 32, 2018.


M. Franklin. The influence of explainable artificial intelligence: Nudging behaviour or boosting capability? _arXiv_ _preprint_ _arXiv:2210.02407_, 2022.



M. Franklin, H. Ashton, E. Awad, and D. Lagnado. Causal framework of artificial autonomous agent responsibility. In _Proceedings of the 2022_ _AAAI/ACM Conference on AI, Ethics, and Society_, pages 276–284, 2022.


A. Fuchs, A. Passarella, and M. Conti. Optimizing delegation between human and ai collaborative agents. In _Joint_ _European_ _Conference_ _on_ _Machine Learning and Knowledge Discovery in_ _Databases_, pages 245–260. Springer, 2023.


A. Fuchs, A. Passarella, and M. Conti. Optimizing delegation in collaborative human-ai hybrid teams. _ACM Transactions on Autonomous and_ _Adaptive Systems_, 19(4):1–33, 2024.


A. Fügener, J. Grahl, A. Gupta, and W. Ketter. Cognitive challenges in human-ai collaboration: Investigating the path towards productive delegation. _Forthcoming, Information Systems_ _Research_, 2019.


A. Fügener, J. Grahl, A. Gupta, and W. Ketter. Cognitive challenges in human–artificial intelligence collaboration: Investigating the path toward productive delegation. _Information Sys-_ _tems Research_, 33(2):678–696, 2022.


I. Gabriel, A. Manzini, G. Keeling, L. A. Hendricks, V. Rieser, H. Iqbal, N. Tomašev, I. Ktena, Z. Kenton, M. Rodriguez, et al. The ethics of advanced ai assistants. _arXiv preprint arXiv:2404.16244_, 2024.


I. Gabriel, G. Keeling, A. Manzini, and J. Evans. Who’s to blame when ai agents mess up? we urgently need a new system of ethics, 2025.


B. Gebru, L. Zeleke, D. Blankson, M. Nabil, S. Nateghi, A. Homaifar, and E. Tunstel. A review on human–machine trust evaluation: Human-centric and machine-centric perspectives. _IEEE_ _Transactions_ _on_ _Human-Machine_ _Systems_, 52(5):952–962, 2022.


J. Geng, F. Cai, Y. Wang, H. Koeppl, P. Nakov, and I. Gurevych. A survey of confidence estimation and calibration in large language models. _arXiv_ _preprint arXiv:2311.08298_, 2023.


O. Goldreich. Secure multi-party computation. _Manuscript._ _Preliminary_ _version_, 78(110):1– 108, 1998.


32


Intelligent AI Delegation



C. Goods, A. Veen, and T. Barratt. “is your gig any good?” analysing job quality in the australian platform-based food-delivery sector. _Journal of_ _Industrial Relations_, 61(4):502–527, 2019. doi: 10.1177/0022185618817069.


Google. Powering ai commerce with the new agent payments protocol (ap2), 2025a.



S. J. Grossman and O. D. Hart. An analysis of the principal-agent problem. In _Foundations of_ _insurance economics:_ _Readings in economics and_ _finance_, pages 302–340. Springer, 1992.



T. Guggenberger, L. Lämmermann, N. Urbach, A. M. Walter, and P. Hofmann. Task delegation from ai to humans: a principal-agent perspective. In _Proceedings_ _of_ _the_ _44th_ _International_ _Conference on Information Systems_, 2023.



Google. Powering ai commerce with the new agent payments protocol (ap2), 2025b. URL `[https://cloud.google.com/](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol)`
```
 blog/products/ai-machine-learning/
```

`[announcing-agents-to-payments-ap2-protocol](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol)` .



A. Gunjal, A. Wang, E. Lau, V. Nath, Y. He, B. Liu, and S. Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains.. _arXiv_ _preprint arXiv:2507.17746_, 2025.



Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan, and W. Chen. Critic: Large language models can self-correct with tool-interactive critiquing. _arXiv_ _preprint_ _arXiv:2305.11738_, 2023.


B. Green. The flaws of policies requiring human oversight of government algorithms. _Computer_ _Law & Security Review_, 45:105681, 2022.


R. Greenblatt, C. Denison, B. Wright, F. Roger, M. MacDiarmid, S. Marks, J. Treutlein, T. Belonax, J. Chen, D. Duvenaud, A. Khan, J. Michael, S. Mindermann, E. Perez, L. Petrini, J. Uesato, J. Kaplan, B. Shlegeris, S. R. Bowman, and E. Hubinger. Alignment faking in large language models. _arXiv_ _preprint_ _arXiv:2412.14093_, 2024.


K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz. Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In _Proceedings_ _of_ _the_ _16th_ _ACM_ _workshop_ _on_ _ar-_ _tificial_ _intelligence_ _and_ _security_, pages 79–90, 2023.


N. Griffiths. Task delegation using experiencebased multi-dimensional trust. In _Proceedings_ _of the fourth international joint conference on Au-_ _tonomous agents and multiagent systems_, pages 489–496, 2005.


S. Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. _Artificial In-_ _telligence Review_, 55(2):895–943, 2022.



D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al. Deepseek-coder: When the large language model meets programming–the rise of code intelligence. _arXiv preprint arXiv:2401.14196_, 2024a.


T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and X. Zhang. Large language model based multi-agents: A survey of progress and challenges. _arXiv_ _preprint_ _arXiv:2402.01680_, 2024b.


J. Haas. Moral gridworlds: a theoretical proposal for modeling artificial moral cognition. _Minds_ _and Machines_, 30(2):219–246, 2020.


G. K. Hadfield and A. Koh. An economy of ai agents. _arXiv preprint arXiv:2509.01063_, 2025.


L. Hammond, A. Chan, J. Clifton, J. HoelscherObermaier, A. Khan, E. McLean, C. Smith, W. Barfuss, J. Foerster, T. Gavenčiak, et al. Multi-agent risks from advanced ai. _arXiv_ _preprint arXiv:2502.14143_, 2025.


A. Handa and Google Developers. Under the hood: Universal commerce protocol (UCP).
```
 https://developers.googleblog.com/
 under-the-hood-universal-commerce-protocol-u
```

2026. Accessed: 2026-01-20.


S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is planning with world model. _arXiv_ _preprint arXiv:2305.14992_, 2023.


33


Intelligent AI Delegation



N. Hardy. The confused deputy: (or why capabilities might have been invented). _ACM SIGOPS_ _Operating Systems Review_, 22(4):36–38, 1988.


A. I. Hauptman, B. G. Schelble, N. J. McNeese, and K. C. Madathil. Adapt and overcome: Perceptions of adaptive autonomous agents for humanai teaming. _Computers in Human Behavior_, 138: 107451, 2023.


G. He, P. Cui, J. Chen, W. Hu, and J. Zhu. Investigating uncertainty calibration of aligned language models under the multiple-choice setting, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2310.11732)` `[2310.11732](https://arxiv.org/abs/2310.11732)` .


X. O. He. Mixture of a million experts. _arXiv_ _preprint arXiv:2407.04153_, 2024.


P. Hemmer, M. Westphal, M. Schemmer, S. Vetter, M. Vössing, and G. Satzger. Human-ai collaboration: the effect of ai delegation on human task performance and task satisfaction. In _Pro-_ _ceedings_ _of_ _the_ _28th_ _International_ _Conference_ _on_ _Intelligent_ _User_ _Interfaces_, pages 453–463, 2023.


S. M. Herzog and M. Franklin. Boosting human competences with interpretable and explainable artificial intelligence. _Decision_, 11(4):493, 2024.


S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, et al. Metagpt: Meta programming for a multiagent collaborative framework. In _The Twelfth_ _International Conference on Learning Represen-_ _tations_, 2023.


J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. _arXiv_ _preprint arXiv:2310.01798_, 2023.


K. Huang and C. Hughes. Deploying agentic ai in enterprise environments. In _Securing AI Agents:_ _Foundations,_ _Frameworks,_ _and_ _Real-World_ _De-_ _ployment_, pages 289–319. Springer, 2025.


E. Hubinger, C. Denison, J. Mu, M. Lambert, M. Tong, M. MacDiarmid, T. Lanham, D. M. Ziegler, T. Maxwell, N. Cheng, A. Jermyn, A. Askell, A. Radhakrishnan, C. Anil,



D. Duvenaud, D. Ganguli, F. Barez, J. Clark, K. Ndousse, K. Sachan, M. Sellitto, M. Sharma, N. DasSarma, R. Grosse, S. Kravec, Z. Witten, M. Favaro, J. Brauner, H. Karnofsky, P. Christiano, S. R. Bowman, L. Graham, J. Kaplan, S. Mindermann, R. Greenblatt, N. Schiefer, B. Shlegeris, and E. Perez. Sleeper agents: Training deceptive llms that persist through safety training. _arXiv_ _preprint_ _arXiv:2401.05566_, 2024.


K. Isomura. _Management_ _theory_ _by_ _Chester_ _Barnard:_ _an introduction_ . Springer, 2021.


R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79–87, 1991.


A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. _arXiv_ _preprint_ _arXiv:2401.04088_, 2024.


C. Jiang, X. Pan, G. Hong, C. Bao, Y. Chen, and M. Yang. Feedback-guided extraction of knowledge base from retrieval-augmented llm applications, 2025. URL `[https://arxiv.org/](https://arxiv.org/abs/2411.14110)` `[abs/2411.14110](https://arxiv.org/abs/2411.14110)` .


Z. Jiang, J. Araki, H. Ding, and G. Neubig. How can we know when language models know? on the calibration of language models for question answering. _Transactions of the Association for_ _Computational Linguistics_, 9:962–977, 2021.


S. Kapoor, N. Gruver, M. Roberts, A. Pal, S. Dooley, M. Goldblum, and A. Wilson. Calibrationtuning: Teaching large language models to know what they don’t know. In _Proceedings_ _of the 1st Workshop on Uncertainty-Aware NLP_ _(UncertaiNLP 2024)_, pages 1–14, 2024.


A. Kasirzadeh and I. Gabriel. Characterizing ai agents for alignment and governance,
2025. URL `[https://arxiv.org/abs/2504.](https://arxiv.org/abs/2504.21848)`
`[21848](https://arxiv.org/abs/2504.21848)` .


M. Keren and D. Levhari. The optimum span of control in a pure hierarchy. _Management_ _science_, 25(11):1162–1172, 1979.


34


Intelligent AI Delegation



O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Vardhamanan, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. Dspy: Compiling declarative language model calls into selfimproving pipelines, 2023. URL `[https://](https://arxiv.org/abs/2310.03714)` `[arxiv.org/abs/2310.03714](https://arxiv.org/abs/2310.03714)` .


B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and L. van der Maaten. Crypten: Secure multi-party computation meets machine learning. _Advances_ _in_ _Neural_ _Information Processing Systems_, 34:4961–4973, 2021.


S. C. Kohn, E. J. De Visser, E. Wiese, Y.-C. Lee, and T. H. Shaw. Measurement of trust in automation: A narrative review and reference guide. _Frontiers in psychology_, 12:604977, 2021.



_ACM Conference on Fairness, Accountability, and_ _Transparency_, pages 2274–2289, 2025.


M. K. Lee, D. Kusbit, E. Metsky, and L. Dabbish. Working with machines: The impact of algorithmic and data-driven management on human workers. In _Proceedings of the 33rd Annual ACM_ _Conference on Human Factors in Computing Sys-_ _tems_, CHI ’15, pages 1603–1612, New York, NY,
2015. ACM. doi: 10.1145/2702123.2702548.


J. Z. Leibo, A. S. Vezhnevets, M. Diaz, J. P. Agapiou, W. A. Cunningham, P. Sunehag, J. Haas, R. Koster, E. A. Duéñez-Guzmán, W. S. Isaac, G. Piliouras, S. M. Bileschi, I. Rahwan, and S. Osindero. A theory of appropriateness with applications to generative artificial intelligence,
2024. URL `[https://arxiv.org/abs/2412.](https://arxiv.org/abs/2412.19010)`
`[19010](https://arxiv.org/abs/2412.19010)` .



V. Krakovna, J. Uesato, V. Mikulik, M. Rahtz, J. Leike, M. and S. Legg. Specification gaming: The tega, T. Everitt, flip side of AI ingenuity. _DeepMind_ S. Legg. AI safety _Safety_ _Research_ _Blog_, 2020. URL `[https:](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)` _arXiv:1711.09883_
```
 //deepmind.google/discover/blog/
```

`[specification-gaming-the-flip-side-of-ai-ingenuity/](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)` . Blog post. and Y. Liu.



J. Leike, M. Martic, V. Krakovna, P. A. Ortega, T. Everitt, A. Lefrancq, L. Orseau, and S. Legg. AI safety gridworlds. _arXiv_ _preprint_ _arXiv:1711.09883_, 2017.



L. Krause, W. Tufa, S. B. Santamaría, A. Daza, U. Khurana, and P. Vossen. Confidently wrong: exploring the calibration and expression of (un) certainty of large language models in a multilingual setting. In _Proceedings of the workshop_ _on multimodal,_ _multilingual natural language_ _generation and multilingual WebNLG Challenge_ _(MM-NLG 2023)_, pages 1–9, 2023.


A. Lal, A. Prasad, A. Kumar, and S. Kumar. Data exfiltration: Preventive and detective countermeasures. In _Proceedings_ _of_ _the_ _International_ _Conference on Innovative Computing & Commu-_ _nication (ICICC)_, 2022.


H. C. Lau and L. Zhang. Task allocation via multi-agent coalition formation: Taxonomy, algorithms and complexity. In _Proceedings. 15th_ _IEEE International Conference on Tools with Ar-_ _tificial Intelligence_, pages 346–350. IEEE, 2003.


M. H. Lee and M. Z. Y. Tok. Towards uncertainty aware task delegation and human-ai collaborative decision-making. In _Proceedings of the 2025_



H. Li, Q. Dong, J. Chen, H. Su, Y. Zhou, Q. Ai, Z. Ye,. and Y. Liu. Llms-as-judges: a comprehensive survey on llm-based evaluation methods. _arXiv_ _preprint arXiv:2412.05579_, 2024a.



J. Li, Y. Yang, R. Zhang, and Y.-c. Lee. Overconfident and unconfident ai hinder human-ai collaboration. _arXiv preprint arXiv:2402.07632_, 2024b.


P. Li, Z. An, S. Abrar, and L. Zhou. Large language models for multi-robot systems: A survey, 2025a. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2502.03814)` `[2502.03814](https://arxiv.org/abs/2502.03814)` .


W. Li, J. Lin, Z. Jiang, J. Cao, X. Liu, J. Zhang, Z. Huang, Q. Chen, W. Sun, Q. Wang, H. Lu, T. Qin, C. Zhu, Y. Yao, S. Fan, X. Li, T. Wang, P. Liu, K. Zhu, H. Zhu, D. Shi, P. Wang, Y. Guan, X. Tang, M. Liu, Y. E. Jiang, J. Yang, J. Liu, G. Zhang, and W. Zhou. Chain-of-agents: Endto-end agent foundation models via multi-agent distillation and agentic rl, 2025b. URL `[https:](https://arxiv.org/abs/2508.13167)` `[//arxiv.org/abs/2508.13167](https://arxiv.org/abs/2508.13167)` .


H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,


35


Intelligent AI Delegation



I. Sutskever, and K. Cobbe. Let’s verify step by step, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2305.20050)` `[2305.20050](https://arxiv.org/abs/2305.20050)` .


S. Lin, J. Hilton, and O. Evans. Teaching models to express their uncertainty in words. _arXiv_ _preprint arXiv:2205.14334_, 2022.


X. Liu, T. Chen, L. Da, C. Chen, Z. Lin, and H. Wei. Uncertainty quantification and confidence calibration in large language models: A survey. In _Proceedings of the 31st ACM SIGKDD Conference_ _on Knowledge Discovery and Data Mining V. 2_, pages 6107–6117, 2025.


Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, et al. Prompt injection attack against llm-integrated applications. _arXiv preprint arXiv:2306.05499_, 2023.


J. M. Logg, J. A. Minson, and D. A. Moore. Algorithm appreciation: People prefer algorithmic to human judgment. _Organizational_ _Behav-_ _ior and Human Decision Processes_, 151:90–103, 2019.


B. Lubars and C. Tan. Ask not what ai can do, but what ai should do: Towards a framework of task delegability. _Advances_ _in_ _neural_ _infor-_ _mation processing systems_, 32, 2019.


Z. Luo, Z. Shen, W. Yang, Z. Zhao, P. Jwalapuram, A. Saha, D. Sahoo, S. Savarese, C. Xiong, and J. Li. Mcp-universe: Benchmarking large language models with real-world model context protocol servers. _arXiv preprint_ _arXiv:2508.14704_, 2025.


F. Luthans and T. I. Stewart. A general contingency theory of management. _Academy of man-_ _agement Review_, 2(2):181–195, 1977.


S. Ma, Y. Lei, X. Wang, C. Zheng, C. Shi, M. Yin, and X. Ma. Who should i trust: Ai or myself? leveraging human and ai correctness likelihood to promote appropriate trust in aiassisted decision-making. In _Proceedings of the_ _2023 CHI Conference on Human Factors in Com-_ _puting Systems_, pages 1–19, 2023.


L. Malmqvist. Sycophancy in large language models: Causes and mitigations. In _Intelligent_



_Computing-Proceedings of the Computing Con-_ _ference_, pages 61–74. Springer, 2025.


Y. Mao, M. G. Reinecke, M. Kunesch, E. A. DuéñezGuzmán, R. Comanescu, J. Haas, and J. Z. Leibo. Doing the right thing for the right reason: Evaluating artificial moral cognition by probing cost insensitivity. _arXiv_ _preprint_ _arXiv:2305.18269_, 2023.


S. Masoudnia and R. Ebrahimpour. Mixture of experts: a literature survey. _Artificial Intelligence_ _Review_, 42(2):275–293, 2014.


P. Mazdin and B. Rinner. Distributed and communication-aware coalition formation and task assignment in multi-robot systems. _IEEE_ _Access_, 9:35088–35100, 2021.


E. A. M. Michels, S. Gilbert, I. Koval, and M. K. Wekenborg. Alarm fatigue in healthcare: a scoping review of definitions, influencing factors, and mitigation strategies. _BMC_ _nursing_, 24(1):664, 2025.


Microsoft. Unleashing the power of model context protocol (mcp): A game-changer in AI integration, 2025.


E. Mosqueira-Rey, E. Hernández-Pereira, D. Alonso-Ríos, J. Bobes-Bascarán, and Á. Fernández-Leal. Human-in-the-loop machine learning: a state of the art. _Artificial_ _Intelligence Review_, 56(4):3005–3054, 2023.


C. Mueller and A. Vogelsmeier. Effective delegation: Understanding responsibility, authority, and accountability. _Journal of Nursing Regula-_ _tion_, 4(3):20–27, 2013.


R. B. Myerson. Optimal coordination mechanisms in generalized principal–agent problems. _Jour-_ _nal_ _of_ _mathematical_ _economics_, 10(1):67–81, 1982.


O. Nachum, S. S. Gu, H. Lee, and S. Levine. Dataefficient hierarchical reinforcement learning. _Advances in neural information processing sys-_ _tems_, 31, 2018.


S. K. Nagia. Delegation of authority: A great challenge for business organisation. _ARTIFICIAL_ _INTELLIGENCE (AI) AND BUSINESS_, page 55, 2024.


36


Intelligent AI Delegation



M. Naiseh, D. Al-Thani, N. Jiang, and R. Ali. Explainable recommendation: when design meets trust calibration. _World Wide Web_, 24(5):1857– 1884, 2021.


M. Naiseh, D. Al-Thani, N. Jiang, and R. Ali. How the different explanation classes impact trust calibration: The case of clinical decision support systems. _International Journal of Human-_ _Computer Studies_, 169:102941, 2023.


J. Needham, G. Edkins, G. Pimpale, H. Bartsch, and M. Hobbhahn. Large language models often know when they are being evaluated. _arXiv_ _preprint arXiv:2505.23836_, 2025.


E. Neelou, I. Novikov, M. Moroz, O. Narayan, T. Saade, M. Ayenson, I. Kabanov, J. Ozmen, E. Lee, V. S. Narajala, E. G. Junior, K. Huang, H. Gulsin, J. Ross, M. Vyshegorodtsev, A. Travers, I. Habler, and R. Jadav. A2as: Agentic ai runtime security and self-defense,
2025. URL `[https://arxiv.org/abs/2510.](https://arxiv.org/abs/2510.13825)`
`[13825](https://arxiv.org/abs/2510.13825)` .


E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen: An open large language model for code with multi-turn program synthesis. _arXiv_ _preprint_ _arXiv:2203.13474_, 2022.


Z. Ning and L. Xie. A survey on multi-agent reinforcement learning and its application. _Jour-_ _nal of Automation and Intelligence_, 3(2):73–91, 2024.


O. Or-Meir, N. Nissim, Y. Elovici, and L. Rokach. Dynamic malware analysis in the modern era—a state of the art survey. _ACM Computing_ _Surveys (CSUR)_, 52(5):1–48, 2019.


D. Otley. The contingency theory of management accounting and control: 1980–2014. _Manage-_ _ment accounting research_, 31:45–62, 2016.


W. G. Ouchi and J. B. Dowling. Defining the span of control. _Administrative_ _Science_ _Quarterly_, pages 357–365, 1974.


B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer, and M. T. Ribeiro. Art: Automatic multi-step reasoning and tool-use



for large language models. _arXiv_ _preprint_ _arXiv:2303.09014_, 2023.


R. Parasuraman, R. Molloy, and I. L. Singh. Performance consequences of automationinduced’complacency’. _The International Jour-_ _nal of Aviation Psychology_, 3(1):1–23, 1993.


S. Parikh and R. Surapaneni. Powering AI commerce with the new Agent Payments Protocol (AP2), Sept. 2025. URL
```
 https://cloud.google.com/blog/
 products/ai-machine-learning/
```

`[announcing-agents-to-payments-ap2-protocol](https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol)` . Accessed: 2026-01-20.


I. Pastine and T. Pastine. _Introducing game theory:_ _A graphic guide_ . Icon Books, 2017.


S. Pateria, B. Subagdja, A.-h. Tan, and C. Quek. Hierarchical reinforcement learning: A comprehensive survey. _ACM Computing Surveys (CSUR)_, 54(5):1–35, 2021.


M. Petkus. Why and how zk-snark works. _arXiv_ _preprint arXiv:1906.07221_, 2019.


E. Pignatelli, J. Ferret, M. Geist, T. Mesnard, H. van Hasselt, O. Pietquin, and L. Toni. A survey of temporal credit assignment in deep reinforcement learning. _arXiv_ _preprint_ _arXiv:2312.01072_, 2023.


I. Pinyol and J. Sabater-Mir. Computational trust and reputation models for open multi-agent systems: a review. _Artificial Intelligence Review_, 40(1):1–25, 2013.


Z. Porter, P. Ryan, P. Morgan, J. Al-Qaddoumi, B. Twomey, J. McDermid, and I. Habli. Unravelling responsibility for ai. _arXiv_ _preprint_ _arXiv:2308.02608_, 2023.


C. Qian, Z. Xie, Y. Wang, W. Liu, K. Zhu, H. Xia, Y. Dang, Z. Du, W. Chen, C. Yang, et al. Scaling large language model-based multi-agent collaboration. _arXiv_ _preprint_ _arXiv:2406.07155_, 2024.


K. Qin, L. Zhou, B. Livshits, and A. Gervais. Attacking the defi ecosystem with flash loans for fun and profit, 2021. URL `[https://arxiv.](https://arxiv.org/abs/2003.03810)` `[org/abs/2003.03810](https://arxiv.org/abs/2003.03810)` .


37


Intelligent AI Delegation



Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, L. Hong, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2307.16789)` `[2307.16789](https://arxiv.org/abs/2307.16789)` .


B. Radosevich and J. Halloran. Mcp safety audit: Llms with the model context protocol allow major security exploits. _arXiv_ _preprint_ _arXiv:2504.03767_, 2025.


S. D. Ramchurn, D. Huynh, and N. R. Jennings. Trust in multi-agent systems. _The_ _knowledge_ _engineering review_, 19(1):1–25, 2004.


J. Rando and F. Tramèr. Universal jailbreak backdoors from poisoned human feedback,
2024. URL `[https://arxiv.org/abs/2311.](https://arxiv.org/abs/2311.14455)`
`[14455](https://arxiv.org/abs/2311.14455)` .


S. Rasal and E. J. Hauer. Navigating complexity: Orchestrated problem solving with multi-agent llms, 2024. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2402.16713)` `[2402.16713](https://arxiv.org/abs/2402.16713)` .


T. Rebedea, R. Dinu, M. Sreedhar, C. Parisien, and J. Cohen. Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails, 2023. URL `[https://arxiv.](https://arxiv.org/abs/2310.10501)` `[org/abs/2310.10501](https://arxiv.org/abs/2310.10501)` .


M. G. Reinecke, Y. Mao, M. Kunesch, E. A. DuéñezGuzmán, J. Haas, and J. Z. Leibo. The puzzle of evaluating moral cognition in artificial agents. _Cognitive Science_, 47(8):e13315, 2023.


A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. _arXiv preprint arXiv:2307.01928_, 2023.


C. O. Retzlaff, S. Das, C. Wayllace, P. Mousavi, M. Afshari, T. Yang, A. Saranti, A. Angerschmid, M. E. Taylor, and A. Holzinger. Human-in-theloop reinforcement learning: A survey and position on requirements, challenges, and opportunities. _Journal of Artificial Intelligence Research_, 79:359–415, 2024.



C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby. Scaling vision with sparse mixture of experts. _Advances in Neural Information_ _Processing Systems_, 34:8583–8595, 2021.


J. M. Rosanas and M. Velilla. Loyalty and trust as the ethical bases of organizations. _Journal of_ _Business Ethics_, 44(1):49–59, 2003.


A. Rosenblat and L. Stark. Algorithmic labor and information asymmetries: A case study of uber’s drivers. _International_ _Journal_ _of_ _Communication_, 10:3758–3784,
2016. URL `[https://ijoc.org/index.](https://ijoc.org/index.php/ijoc/article/view/4892)`
`[php/ijoc/article/view/4892](https://ijoc.org/index.php/ijoc/article/view/4892)` .


J. Ruan, Y. Chen, B. Zhang, Z. Xu, T. Bao, H. Mao, Z. Li, X. Zeng, R. Zhao, et al. Tptu: Task planning and tool usage of large language modelbased ai agents. In _NeurIPS 2023 Foundation_ _Models for Decision Making Workshop_, 2023.


J. M. Sanabria and P. A. Vecino. Beyond the sum: Unlocking ai agents potential through market forces, 2025. URL `[https://arxiv.](https://arxiv.org/abs/2501.10388)` `[org/abs/2501.10388](https://arxiv.org/abs/2501.10388)` .


T. Sandholm. An implementation of the contract net protocol based on marginal cost calculations. In _AAAI_, volume 93, pages 256–262, 1993.


Y. Sannikov. A continuous-time version of the principal-agent problem. _The_ _Review_ _of_ _Eco-_ _nomic Studies_, 75(3):957–984, 2008.


F. Santoni de Sio and G. Mecacci. Four responsibility gaps with artificial intelligence: Why they matter and how to address them. _Philosophy &_ _technology_, 34(4):1057–1084, 2021.


S. Sarkar, M. Curado Malta, and A. Dutta. A survey on applications of coalition formation in multi-agent systems. _Concurrency and Compu-_ _tation:_ _Practice and Experience_, 34(11):e6876, 2022.


W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting human evaluators, 2022. URL `[https://arxiv.org/abs/2206.05802](https://arxiv.org/abs/2206.05802)` .


38


Intelligent AI Delegation



S. Shah. The principal-agent problem in finance. _CFA_ _Institute_ _Research_ _Foundation_ _L2014-1_, 2014.


Y. Shao, H. Zope, Y. Jiang, J. Pei, D. Nguyen, E. Brynjolfsson, and D. Yang. Future of work with ai agents: Auditing automation and augmentation potential across the u.s. workforce,
2025. URL `[https://arxiv.org/abs/2506.](https://arxiv.org/abs/2506.06576)`
`[06576](https://arxiv.org/abs/2506.06576)` .


M. Sharma, M. Tong, T. Korbak, D. Duvenaud, A. Askell, S. R. Bowman, N. Cheng, E. Durmus, Z. Hatfield-Dodds, S. R. Johnston, et al. Towards understanding sycophancy in language models. _arXiv_ _preprint_ _arXiv:2310.13548_, 2023.


N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparselygated mixture-of-experts layer. _arXiv preprint_ _arXiv:1701.06538_, 2017.


O. M. Shehory, K. Sycara, and S. Jha. Multiagent coordination through coalition formation. In _International_ _Workshop_ _on_ _Agent_ _Theories,_ _Architectures, and Languages_, pages 143–154. Springer, 1997.


A. Singh, A. Ehtesham, S. Kumar, and T. T. Khoei. A survey of the model context protocol (mcp): Standardizing context to enhance large language models (llms). 2025.


J. Skalse and M. Mancosu. Defining and characterizing reward hacking. _Proceedings of the 2022_ _AAAI/ACM_ _Conference_ _on_ _AI,_ _Ethics,_ _and_ _Soci-_ _ety_, pages 1–11, 2022. doi: 10.1145/3514094. 3534149.


P. Sloksnath. Delegating moral decisions to ai systems. Master’s thesis, University of Zurich, 2025.


S. C. Slota, K. R. Fleischmann, S. Greenberg, N. Verma, B. Cummings, L. Li, and C. Shenefiel. Many hands make many fingers to point: challenges in creating accountable ai. _Ai & Society_, 38(4):1287–1299, 2023.


R. G. Smith. The contract net protocol: High-level communication and control in a distributed



problem solver. _IEEE Transactions on computers_, 29(12):1104–1113, 1980.


J. Sobel. Information control in the principalagent problem. _International Economic Review_, pages 259–269, 1993.


X. Song, Z. Wang, S. Wu, T. Shi, and L. Ai. Gradientsys: A multi-agent llm scheduler with react orchestration, 2025. URL `[https://arxiv.](https://arxiv.org/abs/2507.06520)` `[org/abs/2507.06520](https://arxiv.org/abs/2507.06520)` .


C. Stucky, M. De Jong, and F. Kabo. The paradox of network inequality: differential impacts of status and influence on surgical team communication. _Med J (Ft Sam Houst Tex)_, pages 22–01, 2022.


R. S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. _Arti-_ _ficial intelligence_, 112(1-2):181–211, 1999.


S. Tadelis and O. E. Williamson. Transaction cost economics. _The handbook of organizational eco-_ _nomics_, 159(3.1):1, 2012.


W. Takerngsaksiri, J. Pasuksmit, P. Thongtanunam, C. Tantithamthavorn, R. Zhang, F. Jiang, J. Li, E. Cook, K. Chen, and M. Wu. Human-in-the-loop software development agents. In _2025_ _IEEE/ACM_ _47th_ _Inter-_ _national_ _Conference_ _on_ _Software_ _Engineering:_ _Software_ _Engineering_ _in_ _Practice_ _(ICSE-SEIP)_, pages 342–352. IEEE, 2025.


J. Teutsch and C. Reitwießner. Truebit: a scalable verification solution for blockchains. _White_ _Papers_, 2018.


J. Teutsch and C. Reitwießner. A scalable verification solution for blockchains. In _Aspects_ _of_ _Computation and Automata Theory with Appli-_ _cations_, pages 377–424. World Scientific, 2024.


N. A. Theobald and S. Nicholson-Crotty. The many faces of span of control: Organizational structure across multiple goals. _Administration_ _&_ _Society_, 36(6):648–660, 2005.


N. Tomašev, M. Franklin, J. Jacobs, S. Krier, and S. Osindero. Distributional agi safety. _arXiv_ _preprint arXiv:2512.16856_, 2025.


39


Intelligent AI Delegation



N. Tomasev, M. Franklin, J. Z. Leibo, J. Jacobs, W. A. Cunningham, I. Gabriel, and S. Osindero. Virtual agent economies, 2025. URL `[https:](https://arxiv.org/abs/2509.10147)` `[//arxiv.org/abs/2509.10147](https://arxiv.org/abs/2509.10147)` .


P. M. Tomei, R. Jain, and M. Franklin. Ai governance through markets. _arXiv_ _preprint_ _arXiv:2501.17755_, 2025.


K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham, B. O’Sullivan, and H. D. Nguyen. Multi-agent collaboration mechanisms: A survey of llms,
2025. URL `[https://arxiv.org/abs/2501.](https://arxiv.org/abs/2501.06322)`
`[06322](https://arxiv.org/abs/2501.06322)` .


V. Tupe and S. Thube. Ai agentic workflows and enterprise apis: Adapting api architectures for the age of ai agents, 2025. URL `[https://](https://arxiv.org/abs/2502.17443)` `[arxiv.org/abs/2502.17443](https://arxiv.org/abs/2502.17443)` .


M. Turpin, J. Michael, E. Perez, and S. R. Bowman. Language models don’t always say what they think: Unfaithful explanations in chainof-thought prompting, 2023. URL `[https:](https://arxiv.org/abs/2305.04388)` `[//arxiv.org/abs/2305.04388](https://arxiv.org/abs/2305.04388)` .


R. Uuk, C. I. Gutierrez, D. Guppy, L. Lauwaert, A. Kasirzadeh, L. Velasco, P. Slattery, and C. Prunkl. A taxonomy of systemic risks from general-purpose ai. _arXiv_ _preprint_ _arXiv:2412.07780_, 2024.


K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati. On the planning abilities of large language models-a critical investigation. _Advances in Neural Information Processing Sys-_ _tems_, 36:75993–76005, 2023.


A. H. Van de Ven. The concept of fit in contingency theory. Technical report, 1984.


T. van der Weij, F. Hofstätter, O. Jaffe, S. F. Brown, and F. R. Ward. Ai sandbagging: Language models can strategically underperform on evaluations. _arXiv_ _preprint_ _arXiv:2406.07358_,
2025. Published as a conference paper at ICLR
2025.


A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In _International conference on machine_ _learning_, pages 3540–3549. PMLR, 2017a.



A. S. Vezhnevets, S. Osindero, T. Schaul, N. Heess, M. Jaderberg, D. Silver, and K. Kavukcuoglu. Feudal networks for hierarchical reinforcement learning, 2017b. URL `[https://arxiv.org/](https://arxiv.org/abs/1703.01161)` `[abs/1703.01161](https://arxiv.org/abs/1703.01161)` .


E. F. Vignola, S. Baron, E. Abreu Plasencia, M. Hussein, and N. Cohen. Workers’ health under algorithmic management: Emerging findings and urgent research questions. _In-_ _ternational Journal of Environmental Research_ _and_ _Public_ _Health_, 20(2):1239, 2023. doi: 10.3390/ijerph20021239.


J. Vokřínek, J. Bíba, J. Hodík, J. Vybíhal, and M. Pěchouček. Competitive contract net protocol. In _International_ _Conference_ _on_ _Current_ _Trends in Theory and Practice of Computer Sci-_ _ence_, pages 656–668. Springer, 2007.


G. Wang, B. Wang, T. Wang, A. Nika, H. Zheng, and B. Y. Zhao. Ghost riders: Sybil attacks on crowdsourced mobile mapping services. _IEEE/ACM Transactions on Networking_, 26(3): 1123–1136, 2018. doi: 10.1109/TNET.2018. 2818073.


J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang. Qplex: Duplex dueling multi-agent q-learning. _arXiv_ _preprint arXiv:2008.01062_, 2020.


J. Wang, Z. Wu, Y. Li, H. Jiang, P. Shu, E. Shi, H. Hu, C. Ma, Y. Liu, X. Wang, Y. Yao, X. Liu, H. Zhao, Z. Liu, H. Dai, L. Zhao, B. Ge, X. Li, T. Liu, and S. Zhang. Large language models for robotics: Opportunities, challenges, and perspectives, 2024a. URL `[https://arxiv.](https://arxiv.org/abs/2401.04334)` `[org/abs/2401.04334](https://arxiv.org/abs/2401.04334)` .


L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al. A survey on large language model based autonomous agents. _Frontiers of Computer Sci-_ _ence_, 18(6):186345, 2024b.


Y. Wang, D. Xue, S. Zhang, and S. Qian. Badagent: Inserting and activating backdoor attacks in llm agents, 2024c. URL `[https://arxiv.org/](https://arxiv.org/abs/2406.03007)` `[abs/2406.03007](https://arxiv.org/abs/2406.03007)` .


A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety training fail? _Ad-_


40


Intelligent AI Delegation



_vances in Neural Information Processing Systems_, 36:80079–80110, 2023.


O. E. Williamson. Transaction-cost economics: the governance of contractual relations. _The_ _journal of Law and Economics_, 22(2):233–261, 1979.


O. E. Williamson. Transaction cost economics. _Handbook_ _of_ _industrial_ _organization_, 1:135– 182, 1989.


M. Wischnewski, N. Krämer, and E. Müller. Measuring and understanding trust calibrations for automated systems: A survey of the state-ofthe-art and future directions. In _Proceedings of_ _the_ _2023_ _CHI_ _conference_ _on_ _human_ _factors_ _in_ _computing systems_, pages 1–16, 2023.


Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al. The rise and potential of large language model based agents: A survey. _Science_ _China_ _Infor-_ _mation Sciences_, 68(2):121101, 2025.


Y. Xiao, P. P. Liang, U. Bhatt, W. Neiswanger, R. Salakhutdinov, and L.-P. Morency. Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. _arXiv_ _preprint arXiv:2210.04714_, 2022.


W. Xing, Z. Qi, Y. Qin, Y. Li, C. Chang, J. Yu, C. Lin, Z. Xie, and M. Han. Mcp-guard: A defense framework for model context protocol integrity in large language model applications. _arXiv preprint arXiv:2508.10991_, 2025.


F. Xu, Q. Hao, C. Shao, Z. Zong, Y. Li, J. Wang, Y. Zhang, J. Wang, X. Lan, J. Gong, et al. Toward large reasoning models: A survey of reinforced reasoning with large language models. _Patterns_, 6(10), 2025.


L. Xu and H. Weigand. The evolution of the contract net protocol. In _International Conference_ _on_ _Web-Age_ _Information_ _Management_, pages 257–264. Springer, 2001.


Y. Yang, Y. Wen, J. Wang, and W. Zhang. Agent exchange: Shaping the future of ai agent economics. _arXiv_ _preprint_ _arXiv:2507.03904_, 2025.



J. Yi, Y. Xie, B. Zhu, E. Kiciman, G. Sun, X. Xie, and F. Wu. Benchmarking and defending against indirect prompt injection attacks on large language models. In _Proceedings_ _of_ _the_ _31st ACM SIGKDD Conference on Knowledge Dis-_ _covery and Data Mining V.1_, page 1809–1820. ACM, July 2025. doi: 10.1145/3690624.
3709179. URL `[http://dx.doi.org/10.](http://dx.doi.org/10.1145/3690624.3709179)`
`[1145/3690624.3709179](http://dx.doi.org/10.1145/3690624.3709179)` .


H. Yu, Z. Shen, C. Leung, C. Miao, and V. R. Lesser. A survey of multi-agent trust management systems. _IEEE Access_, 1:35–50, 2013.


L. Yu, V. Do, K. Hambardzumyan, and N. Cancedda. Robust llm safeguarding via refusal feature adversarial training. _arXiv_ _preprint_ _arXiv:2409.20089_, 2024.


M. Yu, F. Meng, X. Zhou, S. Wang, J. Mao, L. Pang, T. Chen, K. Wang, X. Li, Y. Zhang, B. An, and Q. Wen. A survey on trustworthy llm agents: Threats and countermeasures, 2025. URL `[https://arxiv.org/abs/2503.09648](https://arxiv.org/abs/2503.09648)` .


Y. Yuan, W. Jiao, W. Wang, J.-t. Huang, J. Xu, T. Liang, P. He, and Z. Tu. Refuse whenever you feel unsafe: Improving safety in llms via decoupled refusal training. In _Proceedings of the_ _63rd Annual Meeting of the Association for Com-_ _putational Linguistics (Volume 1:_ _Long Papers)_, pages 3149–3167, 2025.


S. E. Yuksel, J. N. Wilson, and P. D. Gader. Twenty years of mixture of experts. _IEEE transactions_ _on neural networks and learning systems_, 23(8): 1177–1193, 2012.


F. M. Zanzotto. Human-in-the-loop artificial intelligence. _Journal of Artificial Intelligence Re-_ _search_, 64:243–252, 2019.


Q. Zhan, Z. Liang, Z. Ying, and D. Kang. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents,
2024. URL `[https://arxiv.org/abs/2403.](https://arxiv.org/abs/2403.02691)`
`[02691](https://arxiv.org/abs/2403.02691)` .


N. Zhang, J. Yan, C. Hu, Q. Sun, L. Yang, D. W. Gao, J. M. Guerrero, and Y. Li. Price-matchingbased regional energy market with hierarchical


41


Intelligent AI Delegation


reinforcement learning algorithm. _IEEE Trans-_ _actions on Industrial Informatics_, 20(9):11103– 11114, 2024.


W. Zhang, C. Cui, Y. Zhao, R. Hu, Y. Liu, Y. Zhou, and B. An. Agentorchestra: A hierarchical multi-agent framework for general-purpose task solving. _arXiv e-prints_, pages arXiv–2506, 2025a.


Z. Zhang, Q. Dai, X. Bo, C. Ma, R. Li, X. Chen, J. Zhu, Z. Dong, and J.-R. Wen. A survey on the memory mechanism of large language modelbased agents. _ACM Transactions on Information_ _Systems_, 43(6):1–47, 2025b.


K. Zhao, L. Li, K. Ding, N. Z. Gong, Y. Zhao, and Y. Dong. A survey on model extraction attacks and defenses for large language models, 2025. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2506.22521)` `[2506.22521](https://arxiv.org/abs/2506.22521)` .


W. Zhao, Y. Gao, S. A. Memon, B. Raj, and R. Singh. Hierarchical routing mixture of experts. In _2020 25th International Conference on_ _Pattern Recognition (ICPR)_, pages 7900–7906. IEEE, 2021.


L. Zhou, X. Xiong, J. Ernstberger, S. Chaliasos, Z. Wang, Y. Wang, K. Qin, R. Wattenhofer, D. Song, and A. Gervais. Sok: Decentralized finance (defi) attacks, 2023. URL `[https:](https://arxiv.org/abs/2208.13035)` `[//arxiv.org/abs/2208.13035](https://arxiv.org/abs/2208.13035)` .


Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon, et al. Mixture-ofexperts with expert choice routing. _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, 35: 7103–7114, 2022.


C. Zhu, M. Dastani, and S. Wang. A survey of multi-agent deep reinforcement learning with communication. _Autonomous Agents and Multi-_ _Agent Systems_, 38(1):4, 2024.


Z. Zou, Z. Liu, L. Zhao, and Q. Zhan. Blocka2a: Towards secure and verifiable agent-to-agent interoperability. _arXiv preprint arXiv:2508.01332_, 2025.



42


