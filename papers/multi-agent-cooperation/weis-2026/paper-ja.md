_2026-02-19_

# **In-context co-player inference ã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿**


**Marissa A. Weis** _[â˜…]_ [,1] **, Maciej WoÅ‚czyk** _[â˜…]_ [,1] **, Rajai Nasser** [1] **, Rif A. Saurous** [1] **, Blaise AgÃ¼era y Arcas** [1,2] **,** **JoÃ£o Sacramento** [1] **and Alexander Meulemans** [1]

1Google, Paradigms of Intelligence Team, 2Santa Fe Institute, _â˜…_ Equal contribution


**è‡ªå·±åˆ©ç›Šã‚’è¿½æ±‚ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿ã‚’é”æˆã™ã‚‹ã“ã¨ã¯ã€Multi-Agent Reinforcement Learning ã«ãŠã‘ã‚‹æ ¹æœ¬çš„ãªèª²é¡Œã§ã‚ã‚Šç¶šã‘ã¦ã„ã‚‹ã€‚è¿‘å¹´ã®ç ”ç©¶ã§ã¯ã€co-player ã®å­¦ç¿’ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’è€ƒæ…®ã—å½¢æˆã™ã‚‹ "learning-aware" ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã§ç›¸äº’å”èª¿ã‚’èª˜å°ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã—ã‹ã—ã€æ—¢å­˜ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é€šå¸¸ã€co-player ã®å­¦ç¿’è¦å‰‡ã«é–¢ã™ã‚‹ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸã€ã—ã°ã—ã°ä¸€è²«æ€§ã®ãªã„ä»®å®šã«ä¾å­˜ã™ã‚‹ã‹ã€é«˜é€Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§æ›´æ–°ã™ã‚‹ "naive learners" ã¨ã€ãã‚Œã‚‰ã®æ›´æ–°ã‚’è¦³å¯Ÿã™ã‚‹ "meta-learners" ã®é–“ã®å³æ ¼ãªåˆ†é›¢ã‚’å¼·åˆ¶ã™ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€Sequence Model ã® in-context learning èƒ½åŠ›ã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸä»®å®šã‚„æ˜ç¤ºçš„ãªã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«åˆ†é›¢ã‚’å¿…è¦ã¨ã›ãšã« co-player learning awareness ãŒå¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚å¤šæ§˜ãª co-player ã®åˆ†å¸ƒã«å¯¾ã—ã¦ Sequence Model ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€è‡ªç„¶ã« _in-context best-response_ æˆ¦ç•¥ãŒèª˜å°ã•ã‚Œã€é«˜é€Ÿãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã—ã¦åŠ¹æœçš„ã«æ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å…ˆè¡Œç ”ç©¶ã§ç‰¹å®šã•ã‚ŒãŸå”èª¿ãƒ¡ã‚«ãƒ‹ã‚ºãƒ â€”â€”extortion ã¸ã®è„†å¼±æ€§ãŒç›¸äº’å½¢æˆã‚’é§†å‹•ã™ã‚‹â€”â€”ãŒã“ã®è¨­å®šã§è‡ªç„¶ã«å‡ºç¾ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ãŸï¼šin-context é©å¿œãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ extortion ã«å¯¾ã—ã¦è„†å¼±ã«ã—ã€ç›¸æ‰‹ã® in-context learning ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å½¢æˆã—ã‚ˆã†ã¨ã™ã‚‹ç›¸äº’åœ§åŠ›ãŒå”èª¿è¡Œå‹•ã®å­¦ç¿’ã¸ã¨å¸°çµã™ã‚‹ã€‚æœ¬ç ”ç©¶ã®çµæœã¯ã€Sequence Model ä¸Šã®æ¨™æº–çš„ãª Decentralized Reinforcement Learning ã¨ co-player ã®å¤šæ§˜æ€§ã®çµ„ã¿åˆã‚ã›ãŒã€å”èª¿è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªé“ç­‹ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚**

### **1. åºè«–**


Foundation Model ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç™ºå±•ã«ã‚ˆã‚Šã€äººå·¥çŸ¥èƒ½ã®æ™¯è‰²ã¯å­¤ç«‹ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ç›¸äº’ä½œç”¨ã™ã‚‹è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã¨æ€¥é€Ÿã«å¤‰åŒ–ã—ã¦ã„ã‚‹ (Aguera Y Arcas et al., 2026; Park et al., 2023; Xi et al., 2023)ã€‚ã“ã‚Œã‚‰ã® Sequence Model ãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã¾ã™ã¾ã™è¤‡é›‘ãªç’°å¢ƒã«å±•é–‹ã•ã‚Œã‚‹ã«ã¤ã‚Œã€è¤‡æ•°ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ç›¸äº’ä½œç”¨ã«çµæœãŒä¾å­˜ã™ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã«å¿…ç„¶çš„ã«ç›´é¢ã™ã‚‹ã€‚ã“ã‚Œã‚‰ã®ç›¸äº’ä½œç”¨ã¯ã—ã°ã—ã°ç«¶åˆã™ã‚‹ç›®æ¨™ã‚’å«ã‚€ãŸã‚ã€æ··åˆå‹•æ©Ÿè¨­å®šã«ãŠã„ã¦è‡ªå·±åˆ©ç›Šçš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé ‘å¥ã«å”èª¿ã™ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ã¯ã€å€‹ã€…ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èƒ½åŠ›ãŒå¤§å¹…ã«æˆé•·ã—ãŸä»Šã§ã‚‚ã€é‡è¦ãªæœªè§£æ±ºã®èª²é¡Œã§ã‚ã‚‹ã€‚

Decentralized Multi-Agent Reinforcement Learning (MARL) ã¯ã€å±€æ‰€çš„ãªè¦³å¯Ÿã®ã¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªãŒã‚‰ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®ç›¸äº’ä½œç”¨ã‚’å­¦ç¿’ã™ã‚‹å•é¡Œã«å–ã‚Šçµ„ã‚€ã€‚ã—ã‹ã—ã€Decentralized MARL ã¯2ã¤ã®ä¸»è¦ãªè¦å› â€”â€”å‡è¡¡é¸æŠã¨ç’°å¢ƒã®éå®šå¸¸æ€§â€”â€”ã«ã‚ˆã‚Šå›°é›£ã§ã‚ã‚‹ (Hernandez-Leal et al., 2017; Shoham & Leyton-Brown, 2008)ã€‚ä¸€èˆ¬å’Œã‚²ãƒ¼ãƒ ã§ã¯å¤šãã® Nash å‡è¡¡ãŒå­˜åœ¨ã—ã†ã‚‹ãŒã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‹¬ç«‹ã«è‡ªèº«ã®å ±é…¬ã‚’æœ€é©åŒ–ã™ã‚‹ã¨ã€ç¤¾ä¼šçš„ã‚¸ãƒ¬ãƒ³ãƒã«ãŠã‘ã‚‹ç›¸äº’è£åˆ‡ã‚Š (mutual defection) ã®ã‚ˆã†ãªæº–æœ€é©ãªçµæœã«é »ç¹ã«åæŸã™ã‚‹ (Claus & Boutilier, 1998; Foerster et al., 2018)ã€‚ã•ã‚‰ã«ã€å˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¦–ç‚¹ã‹ã‚‰ã¯ã€ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒæ™‚ã«å­¦ç¿’ã—æ–¹ç­–ã‚’é©å¿œã•ã›ã¦ã„ã‚‹ãŸã‚ç’°å¢ƒã¯éå®šå¸¸ã§ã‚ã‚‹ (Hernandez-Leal et al., 2017)ã€‚æ¨™æº–çš„ãªå˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ RL ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯å®šå¸¸æ€§ã‚’ä»®å®šã™ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã®åˆ†æ•£è¨­å®šã§åŠ¹æœçš„ãªæ–¹ç­–ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã«ã—ã°ã—ã°å¤±æ•—ã™ã‚‹ (Claus & Boutilier, 1998; Foerster et al., 2018)ã€‚

ã“ã®éå®šå¸¸æ€§ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€_co-player learning awareness_ ã«ã‚ˆã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å­¦ç¿’ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’äºˆæ¸¬ã—ã€co-player ã®å­¦ç¿’ã‚’ã‚ˆã‚Šæœ‰ç›Šãªå‡è¡¡ã¸ã¨å½¢æˆã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚‹ (Aghajohari et al., 2024a,b; Balaguer et al., 2022; Cooijmans et al., 2023; Duque et al., 2024; Foerster et al., 2018; Khan et al., 2024; Lu et al., 2022; Meulemans et al., 2025a; Piche et al., 2025; Segura et al., 2025; Willi et al., 2022; Xie et al., 2021)ã€‚ã“ã‚Œã‚‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ä¸€èˆ¬ã«2ã¤ã®ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã•ã‚Œã‚‹ã€‚ç¬¬ä¸€ã®ã‚«ãƒ†ã‚´ãƒªã¯ co-player ã®å­¦ç¿’æ›´æ–°ã‚’æ˜ç¤ºçš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ç›¸æ‰‹ã®æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ã‚’é€šã˜ã¦å¾®åˆ†ã™ã‚‹ã“ã¨ã§ shaping gradient ã‚’æ¨å®šã™ã‚‹ (Aghajohari et al., 2024a,b; Cooijmans et al., 2023; Duque et al., 2024; Foerster et al., 2018; Piche et al., 2025; Willi et al., 2022)ã€‚ã—ã‹ã—ã€ã“ã‚Œã¯ç›¸æ‰‹ã®å­¦ç¿’è¦å‰‡ã«é–¢ã™ã‚‹å³æ ¼ãªä»®å®šã‚’å¿…è¦ã¨ã—ã€ç›¸æ‰‹ã‚‚ learning-aware ã§ã‚ã‚‹å ´åˆã«çŸ›ç›¾ãŒç”Ÿã˜ã‚‹ã€‚ç¬¬äºŒã®ã‚«ãƒ†ã‚´ãƒªã¯ã€RL ã®æ™‚é–“ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’ co-player ã®è¤‡æ•°ã®æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ã‚’åŒ…å«ã™ã‚‹ã‚ˆã†ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€æš—é»™çš„ã«ç›¸æ‰‹ã®å½¢æˆã‚’å­¦ç¿’ã™ã‚‹ (Khan et al., 2024; Lu et al., 2022; Meulemans et al., 2025a; Segura et al., 2025)ã€‚åŠ¹æœçš„ã§ã¯ã‚ã‚‹ãŒã€ã“ã‚Œã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã€Œé »ç¹ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ "naive learners"ã€ã¨ã€Œã‚†ã£ãã‚Šæ›´æ–°ã™ã‚‹ "meta-learners"ã€ã«åˆ†é›¢ã™ã‚‹ã“ã¨ã‚’å¿…è¦ã¨ã—ã€ç›¸äº’ä½œç”¨ã‚’äº‹å®Ÿä¸Š meta-learning å•é¡Œã¨ã—ã¦æ‰±ã†ã“ã¨ã«ãªã‚‹ (Bengio et al., 1990; Hochreiter et al., 2001; Schmidhuber, 1987)ã€‚

Meulemans et al. (2025a) ã¯ã€co-player learning awareness ãŒè‡ªå·±åˆ©ç›Šçš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿è¡Œå‹•ã®å­¦ç¿’ã«ã¤ãªãŒã‚‹ç†ç”±ã‚’èª¬æ˜ã™ã‚‹3æ®µéšã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹ï¼š

1. **Naive learners ã® Extortion:** Naive learnerï¼ˆé«˜é€Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†æ–¹ç­–ã‚’æ›´æ–°ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰ã«å¯¾ã™ã‚‹æœ€é©æˆ¦ç•¥ã¯ extortion ã§ã‚ã‚‹ (Press & Dyson, 2012)ã€‚Learning-aware ãª meta-agent ã¯ã€naive learner ãŒã‚ˆã‚Šå”èª¿çš„ãªæ–¹ç­–ã«æ›´æ–°ã™ã‚‹ã‚ˆã†ç›¸äº’ä½œç”¨ã‚’å½¢æˆã—ã€ãã®çµæœã¨ã—ã¦ã®è¡Œå‹•ã‚’æ¾å–ã™ã‚‹ã€‚
2. **ç›¸äº’ Extortion ãŒå”èª¿ã«ã¤ãªãŒã‚‹:** ã“ã®ã‚ˆã†ãª extortionate èƒ½åŠ›ã‚’æŒã¤2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¯¾å³™ã™ã‚‹ã¨ã€äº’ã„ã®å­¦ç¿’ã‚’å½¢æˆã—ã‚ˆã†ã¨ã™ã‚‹è©¦ã¿ã®çµæœã€ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚ˆã‚Šå”èª¿çš„ãªæˆ¦ç•¥ã‚’å­¦ç¿’ã™ã‚‹ã€‚
3. **ç•°è³ªæ€§ãŒéµ:** ã—ãŸãŒã£ã¦ã€å”èª¿ã¯ naive learners ã¨ learning-aware agents ã®æ··åˆé›†å›£ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¨“ç·´ã•ã‚ŒãŸã¨ãã«å‡ºç¾ã™ã‚‹ã€‚Naive learners ã¨ã®ç›¸äº’ä½œç”¨ãŒ extortion ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®å‹¾é…åœ§åŠ›ã‚’æä¾›ã—ï¼ˆç›¸äº’è£åˆ‡ã‚Šã‚’å›é¿ï¼‰ã€learning-aware agents ã¨ã®ç›¸äº’ä½œç”¨ãŒã“ã‚Œã‚’ç›¸äº’å”èª¿ã¸ã¨æ´—ç·´ã™ã‚‹ã€‚

æˆ‘ã€…ã¯ã€ç¾åœ¨ã® co-player learning-aware æ‰‹æ³•ãŒä½¿ç”¨ã™ã‚‹è¤‡é›‘ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ â€”â€”æ˜ç¤ºçš„ãª naive learners ã¨ meta learnersã€ã‚ã‚‹ã„ã¯ co-player ã®å­¦ç¿’æ›´æ–°ã‚’é€šã˜ãŸå¾®åˆ†â€”â€”ã¯å”èª¿è¡Œå‹•ã®å­¦ç¿’ã«ä¸è¦ã§ã‚ã‚‹ã¨ä¸»å¼µã™ã‚‹ã€‚Sequence Model ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å¤šæ§˜ãª co-player ã®åˆ†å¸ƒã«å¯¾ã—ã¦ Decentralized MARL ã§è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€è‡ªç„¶ã« _in-context best-response_ æ–¹ç­–ãŒå¾—ã‚‰ã‚Œã‚‹ã¨ã„ã†ä»®èª¬ã‚’ç«‹ã¦ã‚‹ã€‚ã“ã‚Œã‚‰ã®æ–¹ç­–ã¯ã€å˜ä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã® in-context learning ã‚’é€šã˜ã¦ç›®æ¨™æŒ‡å‘ã®é©å¿œã‚’ç¤ºã™ã€‚é‡è¦ãªã“ã¨ã«ã€ã“ã‚ŒãŒå…ˆè¡Œç ”ç©¶ã® "naive learner" ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã®æ©Ÿèƒ½çš„ãªä»£æ›¿ã¨ã—ã¦åƒãã“ã¨ã‚’ç¤ºã™ã€‚In-context learning ãŒã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®é«˜é€Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§ç”Ÿã˜ã‚‹ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ in-weight æ›´æ–°ã‚’ä½¿ç”¨ã™ã‚‹ä»–ã®å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ extortion ã«å¯¾ã—ã¦è„†å¼±ã«ãªã‚‹ã€‚ãã®çµæœã€Meulemans et al. (2025a) ãŒç‰¹å®šã—ãŸå”èª¿çš„å‹¾é…ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ãŒå‡ºç¾ã™ã‚‹ï¼šin-context learners ã® extortion ã‚’ä¿ƒé€²ã™ã‚‹å‹¾é…ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç´”ç²‹ãªè£åˆ‡ã‚Šã‹ã‚‰å¼•ãé›¢ã—ã€ç›¸äº’ extortion å‹¾é…ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å”èª¿ã¸ã¨é§†å‹•ã™ã‚‹ã€‚

æœ¬ç ”ç©¶ã®è²¢çŒ®ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã‚ã‚‹ã€‚Sequence Model ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¤šæ§˜ãª co-player ã®æ··åˆãƒ—ãƒ¼ãƒ«ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚Œã‚‹ Decentralized MARL ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’å°å…¥ã—ã€ã“ã®è¨“ç·´åˆ†å¸ƒãŒå¼·åŠ›ãª in-context co-player inference èƒ½åŠ›ã‚’èª˜å°ã—ã€ãã‚Œã«ã‚ˆã£ã¦å”èª¿ã«ã¤ãªãŒã‚‹ç›¸äº’ extortion åœ§åŠ›ã‚’ç”Ÿã˜ã•ã›ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚ã“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒã€meta ã¨ inner ã®è»Œé“ã®åŒºåˆ¥ã‚„ç›¸æ‰‹ã®å­¦ç¿’è¦å‰‡ã«é–¢ã™ã‚‹ä»®å®šãªã—ã«ã€Iterated Prisoner's Dilemma ã«ãŠã‘ã‚‹é ‘å¥ãªå”èª¿ã«ã¤ãªãŒã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚In-context learning ã¨ co-player learning-awareness ã‚’æ©‹æ¸¡ã—ã™ã‚‹ã“ã¨ã§ã€æ¨™æº–çš„ãª Sequence Modeling ã¨ RL ã‚’ç”¨ã„ãŸå”èª¿çš„ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã¸ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªé“ç­‹ã‚’æä¾›ã™ã‚‹ã€‚æ··åˆãƒ—ãƒ¼ãƒ«è¨“ç·´ã«å¿…è¦ãª in-context best-response æ–¹ç­–ã®å­¦ç¿’ã«é©ã—ãŸã€äºˆæ¸¬çš„ Sequence Model ã® Self-Supervised Learning ã‚’æ´»ç”¨ã™ã‚‹æ–°ã—ã„ RL æ‰‹æ³•ã‚’å°å…¥ã™ã‚‹ã€‚ã“ã®æ‰‹æ³•ã®è¨“ç·´å‡è¡¡ã®ç†è«–çš„ç‰¹æ€§è©•ä¾¡ã‚’æä¾›ã—ã€Nash å‡è¡¡ãŠã‚ˆã³ subjective embedded equilibria (Meulemans et al., 2025b) ã¨ã®é–¢ä¿‚ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚

### **2. å•é¡Œè¨­å®šã¨æ‰‹æ³•**


**Partially Observable Stochastic Gamesï¼ˆéƒ¨åˆ†è¦³æ¸¬ç¢ºç‡ã‚²ãƒ¼ãƒ ï¼‰**. ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç›¸äº’ä½œç”¨ã‚’ _ğ‘_ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® Partially Observable Stochastic Game (POSG; Kuhn, 1953) ã¨ã—ã¦å½¢å¼åŒ–ã™ã‚‹ã€‚å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã¯å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§è¦³æ¸¬ _ğ‘œğ‘¡_ _[ğ‘–]_ [âˆˆO] _[ğ‘–]_ ã¨å ±é…¬ _ğ‘Ÿ_ _ğ‘¡_ _[ğ‘–]_ [âˆˆR] _[ğ‘–]_ ã‚’å—ã‘å–ã‚Šã€è¡Œå‹• _ğ‘_ _ğ‘¡_ _[ğ‘–]_ [âˆˆA] _[ğ‘–]_ ã‚’å®Ÿè¡Œã™ã‚‹ã€‚ã“ã“ã§ O _[ğ‘–]_ , R _[ğ‘–]_ , A _[ğ‘–]_ ã¯æœ‰é™é›†åˆã§ã‚ã‚‹ã€‚æ–¹ç­–ã¯ç›¸äº’ä½œç”¨å±¥æ­´ _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ [=][ {(] _[ğ‘œ][ğ‘–]_ _ğ‘˜_ _[, ğ‘][ğ‘–]_ _ğ‘˜_ âˆ’1 _[, ğ‘Ÿ]_ _ğ‘˜_ _[ğ‘–]_ âˆ’1 [)}] _[ğ‘¡]_ _ğ‘˜_ =1 ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã®æ–¹ç­–ã‚’ _ğœ‹_ _[ğ‘–]_ ( _ğ‘ğ‘¡_ _[ğ‘–]_ [|] _[ğ‘¥]_ â‰¤ _[ğ‘–]_ _ğ‘¡_ [;] _[ ğœ™][ğ‘–]_ [) ã¨è¡¨è¨˜ã—ã€_ğœ™_ _[ğ‘–]_ ã«ã‚ˆã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹ã€‚

**Iterated Prisoner's Dilemmaï¼ˆåå¾©å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒï¼‰**. è‡ªå·±åˆ©ç›Šçš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿ã‚’ç ”ç©¶ã™ã‚‹ãŸã‚ã®æ¨™æº–çš„ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ Iterated Prisoner's Dilemma (IPD) ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ (Axelrod & Hamilton, 1981; Rapoport, 1974)ã€‚å„ãƒ©ã‚¦ãƒ³ãƒ‰ _ğ‘¡_ ã§ã€2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯åŒæ™‚ã«å”èª¿ (C) ã¾ãŸã¯è£åˆ‡ã‚Š (D) ã‚’é¸æŠã™ã‚‹ã€ã™ãªã‚ã¡ _ğ‘ğ‘¡_ _[ğ‘–]_ [âˆˆ{][C] _[,]_ [ D][}] ã§ã‚ã‚Šã€è¡¨1ã«è©³è¿°ã•ã‚Œã‚‹åˆ©å¾—ã‚’å—ã‘å–ã‚‹ã€‚ã“ã®æ§‹é€ ãŒç¤¾ä¼šçš„ã‚¸ãƒ¬ãƒ³ãƒã‚’ç”Ÿã¿å‡ºã™ï¼šå˜ç™ºã‚²ãƒ¼ãƒ ã§ã¯ç›¸äº’è£åˆ‡ã‚ŠãŒå”¯ä¸€ã® Nash å‡è¡¡ã§ã‚ã‚‹ãŒã€ç›¸äº’å”èª¿ã¯ã‚ˆã‚Šé«˜ã„ã‚°ãƒ­ãƒ¼ãƒãƒ«ãŠã‚ˆã³å€‹åˆ¥ãƒªã‚¿ãƒ¼ãƒ³ã‚’ã‚‚ãŸã‚‰ã™ã€‚ç„¡é™åå¾©ã‚²ãƒ¼ãƒ ã¯å”èª¿çš„ Nash å‡è¡¡ã‚’è¨±å®¹ã™ã‚‹ãŒã€Decentralized Reinforcement Learning ã«ã‚ˆã‚‹ã“ã‚Œã‚‰ã®å‡è¡¡ã¸ã®åæŸã¯å›°é›£ãªã¾ã¾ã§ã‚ã‚‹ (Claus & Boutilier, 1998; Foerster et al., 2018)ã€‚è¨ˆç®—ã®æ‰±ã„ã‚„ã™ã•ã®ãŸã‚ã€ç„¡é™ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’å›ºå®šãƒ›ãƒ©ã‚¤ã‚ºãƒ³ _ğ‘‡_ = 100 ã‚¹ãƒ†ãƒƒãƒ—ã§è¿‘ä¼¼ã™ã‚‹ã€‚ã“ã‚Œã¯æœ¬ç ”ç©¶ã§ä½¿ç”¨ã™ã‚‹å°è¦æ¨¡æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒç„¡é™ãƒ›ãƒ©ã‚¤ã‚ºãƒ³è¡Œå‹•ã‚’è¿‘ä¼¼ã™ã‚‹ã®ã«ååˆ†ã§ã‚ã‚‹ã€‚

**Mixed pool trainingï¼ˆæ··åˆãƒ—ãƒ¼ãƒ«è¨“ç·´ï¼‰**. é ‘å¥ãª in-context inference èƒ½åŠ›ã‚’èª˜å°ã™ã‚‹ãŸã‚ã€å˜ä¸€ã®å›ºå®šã•ã‚ŒãŸç›¸æ‰‹ã§ã¯ãªãæ··åˆé›†å›£å†…ã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã™ã‚‹ã€‚è¨“ç·´ãƒ—ãƒ¼ãƒ«ã¯ (i) å®Œå…¨ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å±¥æ­´ _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ ã‚’å‡¦ç†ã™ã‚‹ Sequence Model æ–¹ç­–ã‚’ä½¿ç”¨ã—ã€è¨“ç·´ä¸­ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå­¦ç¿’ã•ã‚Œã‚‹ **Learning Agents** ã¨ã€(ii) 5æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚Œã‚‹é™çš„ãª **Tabular Agents** ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚Tabular Agents ã¯åˆæœŸçŠ¶æ…‹ã¨å‰ã‚¿ãƒ¼ãƒ³ã®4ã¤ã®å¯èƒ½ãªå…±åŒè¡Œå‹•çµæœ ( _ğ‘ğ‘¡_ _[ğ‘–]_ âˆ’1 _[, ğ‘]_ _ğ‘¡_ [âˆ’] - _[ğ‘–]_ 1 ) ã«å¿œã˜ãŸå”èª¿ç¢ºç‡ã‚’å®šç¾©ã™ã‚‹ã€‚è¨“ç·´ä¸­ã€Learning Agent ã¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®50%ã‚’åˆ¥ã® Learning Agent ã¨ã€50%ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‹ã‚‰ä¸€æ§˜ã«ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸ Tabular Agent ã¨å¯¾æˆ¦ã™ã‚‹ã€‚é‡è¦ãªã“ã¨ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè­˜åˆ¥å­ã‚’å—ã‘å–ã‚‰ãšã€ç›¸äº’ä½œç”¨å±¥æ­´ _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ ã‹ã‚‰ã®ã¿ç›¸æ‰‹ã®æ€§è³ªã¨æˆ¦ç•¥ã‚’æ¨è«–ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚

Learning Agents ã®ãŸã‚ã®2ã¤ã®å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¿æŸ»ã™ã‚‹ï¼š

**Independent A2C.** æ¨™æº–çš„ãª Decentralized Model-Free RL æ‰‹æ³•ã¨ã—ã¦ Advantage Actor-Critic (A2C) (Mnih et al., 2016) ã‚’ä½¿ç”¨ã™ã‚‹ã€‚å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç’°å¢ƒã®ä¸€éƒ¨ã¨ã—ã¦æ‰±ã„ã€è‡ªèº«ã®æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœ™_ _[ğ‘–]_ ã‚’ç‹¬ç«‹ã«æœ€é©åŒ–ã—ã¦æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

**Predictive Policy Improvement (PPI).** è¡Œå‹•ã€è¦³æ¸¬ã€å ±é…¬ã®å…±åŒç³»åˆ—ã‚’äºˆæ¸¬ã™ã‚‹ Sequence Model ã‚’æ´»ç”¨ã™ã‚‹ Model-Based ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å°å…¥ã™ã‚‹ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ World Model ã¨ Policy Prior ã®ä¸¡æ–¹ã¨ã—ã¦åŒæ™‚ã«æ©Ÿèƒ½ã™ã‚‹ã€‚ã“ã®æ‰‹æ³•ã¯ Maximum A-Posteriori Policy Optimization (Abdolmaleki et al., 2018, MPO) ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ã®ãŸã‚ã® MUPI ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ (Meulemans et al., 2025b) ã«è§¦ç™ºã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€Self-Supervised Training ã‚’é€šã˜ãŸ in-context inference ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åŠ¹ç‡çš„ãªå­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯ (i) æ”¹å–„ã•ã‚ŒãŸæ–¹ç­–ã§ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€(ii) æ–°ãŸã«åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ Sequence Model ã‚’å†è¨“ç·´ã™ã‚‹ã€ã¨ã„ã†å¤å…¸çš„ãª Policy Iteration ã«é¡ä¼¼ã—ãŸæ‰‹é †ã‹ã‚‰ãªã‚‹ã€‚æ”¹å–„ã•ã‚ŒãŸæ–¹ç­– _ğœ‹_ _[ğ‘–]_ ( _ğ‘_ _[ğ‘–]_ | _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ [)] ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ï¼š

                                   -                                   _ğœ‹_ _[ğ‘–]_ ( _ğ‘_ _[ğ‘–]_ | _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ [)] [âˆ] _[ğ‘]_ _ğœ™_ _[ğ‘–]_ _[ğ‘–]_ [(] _[ğ‘][ğ‘–]_ [|] _[ğ‘¥]_ â‰¤ _[ğ‘–]_ _ğ‘¡_ [) Â·][ exp] _ğ›½ğ‘„_ [Ë†] _ğ‘ğ‘–_ ( _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ _[, ğ‘][ğ‘–]_ [)] _,_ (1)

ã“ã“ã§ _ğ›½_ ã¯é€†æ¸©åº¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚è¡Œå‹•ä¾¡å€¤ _ğ‘„_ [Ë†] _ğ‘_ ( _â„, ğ‘_ ) ã¯ Sequence Model _ğ‘ğœ™_ å†…ã§å®Ÿè¡Œã•ã‚Œã‚‹ Monte Carlo Rollout ã«ã‚ˆã‚Šæ¨å®šã•ã‚Œã‚‹ã€‚ã“ã®æ”¹å–„ã•ã‚ŒãŸæ–¹ç­– _ğœ‹_ _[ğ‘–]_ ( _ğ‘_ _[ğ‘–]_ | _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ [)] ã‚’ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®ç›¸äº’ä½œç”¨ã‚²ãƒ¼ãƒ ã«å±•é–‹ã—ã€æ–°ã—ã„è»Œé“ãƒãƒƒãƒã‚’åé›†ã™ã‚‹ã€‚

**å›³1** | **æ··åˆè¨“ç·´ã¯é ‘å¥ãªå”èª¿ã«ã¤ãªãŒã‚‹ã€‚** Tabular æ–¹ç­–ã¨ Learning Agent ã®æ··åˆã«å¯¾ã—ã¦è¨“ç·´ã•ã‚ŒãŸ RL ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å”èª¿ã«åæŸã™ã‚‹ï¼ˆå®Ÿç·šï¼‰ã€‚**Ablation:** ä»–ã® Learning Agent ã®ã¿ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆç‚¹ç·šï¼‰ã‚„æ˜ç¤ºçš„ãª co-player è­˜åˆ¥ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆç ´ç·šï¼‰ã¯è£åˆ‡ã‚Šã«åæŸã—ã€in-context inference ãŒæ¨™æº–çš„ãª Decentralized MARL ã«ãŠã‘ã‚‹å”èª¿è¡Œå‹•ã®å­¦ç¿’ã®é‡è¦ãªè¦å› ã§ã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã—ã¦ã„ã‚‹ã€‚Error bar ã¯10å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã‚ãŸã‚‹æ¨™æº–åå·®ã‚’ç¤ºã™ã€‚

ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€å¾Œã«ã€ç¾åœ¨ãŠã‚ˆã³ä»¥å‰ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®è“„ç©ã•ã‚ŒãŸå…¨è»Œé“ãƒãƒƒãƒã§ Sequence Model _ğ‘ğœ™_ _[ğ‘–]_ _[ğ‘–]_ ã‚’å†è¨“ç·´ã—ã€_ğœ‹_ _[ğ‘–]_ ã®æ”¹å–„ã•ã‚ŒãŸè¡Œå‹•ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœ™_ _[ğ‘–]_ ã«è’¸ç•™ã™ã‚‹ã€‚Sequence Model _ğ‘ğœ™_ ã¯ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«ã•ã‚ŒãŸ Tabular Agent é–“ã®ç›¸äº’ä½œç”¨ã§äº‹å‰è¨“ç·´ã™ã‚‹ã“ã¨ã§åˆæœŸåŒ–ã™ã‚‹ã€‚å®Ÿè£…ã®è©³ç´°ã¯ä»˜éŒ²Aã€PPI ã®ç†è«–çš„å°å‡ºã¨å‹•æ©Ÿä»˜ã‘ã¯ä»˜éŒ²Cã€PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡è¡¡è¡Œå‹•ã®ç†è«–çš„åˆ†æã¯ä»˜éŒ²Dã‚’å‚ç…§ã•ã‚ŒãŸã„ã€‚

### **3. çµæœ**


æœ¬ç ”ç©¶ã®ä¸­å¿ƒçš„ä»®èª¬ã¯ã€å¤šæ§˜ãª co-player ã®åˆ†å¸ƒã«å¯¾ã—ã¦ Learning Agent ã‚’è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€2ã¤ã®ç•°ãªã‚‹èƒ½åŠ›ã®ç™ºé”ãŒå¿…è¦ã«ãªã‚‹ã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ï¼š(i) ç›¸äº’ä½œç”¨å±¥æ­´ã‹ã‚‰ co-player ã®æ–¹ç­–ã‚’ _æ¨è«–_ ã™ã‚‹ã“ã¨ã€(ii) å˜ä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã§ best response ã« _é©å¿œ_ ã™ã‚‹ã“ã¨ã€‚ã“ã® _in-context best-response policy_ ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ extortion ã«å¯¾ã—ã¦è„†å¼±ã«ã—ã€Meulemans et al. (2025a) ãŒè¨˜è¿°ã—ãŸ "naive learner" ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å†ç¾ã™ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚ã“ã‚ŒãŒ extortion æ–¹ç­–ã¸ã®å­¦ç¿’åœ§åŠ›ã«ã¤ãªãŒã‚Šã€ãã®å¾Œã€Learning Agent é–“ã®ç›¸äº’ extortion ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å”èª¿æ–¹ç­–ã¸ã¨é§†å‹•ã™ã‚‹ã€‚èˆˆå‘³æ·±ã„ã“ã¨ã«ã€ã“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã¯ Learning Agent ã¯æ–‡çŒ®ã§ä¼çµ±çš„ã«åˆ†é›¢ã•ã‚Œã¦ããŸ2ã¤ã®å½¹å‰²ã‚’åŒæ™‚ã«å ã‚ã‚‹ï¼šé«˜é€Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§ã® "naive learners"ï¼ˆin-context learning ã‚’é€šã˜ã¦ï¼‰ã¨ä½é€Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§ã® "learning-aware agents"ï¼ˆweight æ›´æ–°ã‚’é€šã˜ã¦ï¼‰ã§ã‚ã‚‹ã€‚

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã¾ãšæ··åˆãƒ—ãƒ¼ãƒ«è¨“ç·´ãŒæ˜ç¤ºçš„ãªã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«åˆ†é›¢ã‚„ meta-gradient æ©Ÿæ§‹ãªã—ã«å®Ÿéš›ã«é ‘å¥ãªå”èª¿ã«ã¤ãªãŒã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚æ¬¡ã«ã€ãã®åŸºç›¤ã¨ãªã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’åˆ†æã—ã€(1) æ··åˆãƒ—ãƒ¼ãƒ«è¨“ç·´ãŒ in-context best-response æ–¹ç­–ã‚’èª˜å°ã™ã‚‹ã“ã¨ã€(2) ã“ã‚Œã‚‰ã®æ–¹ç­–ãŒ extortion ã«å¯¾ã—ã¦è„†å¼±ã§ã‚ã‚‹ã“ã¨ã€(3) ç›¸äº’ extortion åœ§åŠ›ãŒå”èª¿è¡Œå‹•ã®å­¦ç¿’ã«å¸°çµã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

**3.1.** **æ··åˆè¨“ç·´ã¯é ‘å¥ãªå”èª¿ã‚’èª˜å°ã™ã‚‹**

å›³1ã«ç¤ºã™ã‚ˆã†ã«ã€æ··åˆãƒ—ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§è¨“ç·´ã•ã‚ŒãŸ PPI ã¨ A2C ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸¡æ–¹ãŒ IPD ã§å”èª¿ã«åæŸã™ã‚‹ã€‚ã“ã‚ŒãŒ in-context ç›¸æ‰‹æ¨è«–ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«èµ·å› ã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€2ã¤ã® ablation ã‚’å®Ÿæ–½ã™ã‚‹ï¼š**(1) æ˜ç¤ºçš„è­˜åˆ¥:** ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚ã«ç›¸æ‰‹ã®æ–¹ç­–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆTabular ç›¸æ‰‹ã®å ´åˆï¼‰ã¾ãŸã¯ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ•ãƒ©ã‚°ï¼ˆä»–ã® Learning Agent ã®å ´åˆï¼‰ã‚’æ–¹ç­–ã«æ¡ä»¶ä»˜ã‘ã€in-context ç›¸æ‰‹æ¨è«–ã®å¿…è¦æ€§ã‚’æ’é™¤ã™ã‚‹ã€‚**(2) æ··åˆãƒ—ãƒ¼ãƒ«è¨“ç·´ãªã—:** å˜ä¸€ã®ä»–ã® Learning Agent ã®ã¿ã«å¯¾ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã™ã‚‹ï¼ˆTabular Agent ãƒ—ãƒ¼ãƒ«ã‚„æ§‹é€ åŒ–äº‹å‰è¨“ç·´ãªã—ï¼‰ã€‚

![](figures/paper.pdf-3-4.png)

![](figures/paper.pdf-3-6.png)

![](figures/paper.pdf-3-7.png)

![](figures/paper.pdf-3-9.png)

**Step 1) A. Training Progress**

**Step 2) C. Training Progress**

**Step 3) E. Training Progress**

**B. åæŸå¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ (Phase=30)**

**D. åæŸå¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ (Phase=30)**

![](figures/paper.pdf-4-7.png)

![](figures/paper.pdf-4-8.png)

![](figures/paper.pdf-4-9.png)

![](figures/paper.pdf-4-10.png)

![](figures/paper.pdf-4-37.png)

![](figures/paper.pdf-4-38.png)

![](figures/paper.pdf-4-39.png)

![](figures/paper.pdf-4-40.png)

![](figures/paper.pdf-4-56.png)

![](figures/paper.pdf-4-57.png)

![](figures/paper.pdf-4-58.png)

![](figures/paper.pdf-4-59.png)

**å›³2** | **Aâ€“B: In-context best response ã®å‡ºç¾ã€‚** ãƒ©ãƒ³ãƒ€ãƒ ãª Tabular ç›¸æ‰‹ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚ŒãŸ PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã€ç‰¹å®šã®å›ºå®šæˆ¦ç•¥ã«å¯¾ã™ã‚‹è©•ä¾¡æ™‚ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ in-context learning ã‚’ç¤ºã—ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã§ç›¸æ‰‹ã‚’è­˜åˆ¥ã— best response ã«åæŸã™ã‚‹ã€‚**Câ€“D: In-context learners ã® Extortion ã®å­¦ç¿’ã€‚** "Fixed In-Context Learner"ï¼ˆStep 1 ã§ Tabular æ–¹ç­–ã«å¯¾ã—ã¦ best-respond ã™ã‚‹ã‚ˆã†äº‹å‰è¨“ç·´ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ extortion ã‚’å­¦ç¿’ã™ã‚‹ã€‚RL ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç›¸æ‰‹ã® in-context é©å¿œã‚’æ¾å–ã™ã‚‹ã“ã¨ã§ã€å ±é…¬ã®ã‚ˆã‚Šå¤§ããªã‚·ã‚§ã‚¢ã‚’ç²å¾—ã™ã‚‹ã€‚**Eâ€“F: ç›¸äº’ Extortion ã‹ã‚‰å”èª¿ã¸ã€‚** Extortion æ–¹ç­–ï¼ˆStep 2 ã‹ã‚‰ï¼‰ã§åˆæœŸåŒ–ã•ã‚ŒãŸ2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒäº’ã„ã«å¯¾æˆ¦ã™ã‚‹ã¨ã€co-player ã‚’ extort ã—ã‚ˆã†ã¨ã™ã‚‹ç›¸äº’ã®è©¦ã¿ãŒã€in-context learning ã‚’é€šã˜ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†… (**F**) ã¨ in-weight learning ã‚’é€šã˜ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–“ (**E**) ã®ä¸¡æ–¹ã§ã€äº’ã„ã®æ–¹ç­–ã‚’ã‚ˆã‚Šå”èª¿çš„ãªè¡Œå‹•ã¸ã¨å½¢æˆã™ã‚‹çµæœã¨ãªã‚‹ã€‚Error bar ã¯10å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã‚ãŸã‚‹æ¨™æº–åå·®ã‚’ç¤ºã™ã€‚

å¤šæ§˜ãªç›¸æ‰‹ãŒã„ãªã‘ã‚Œã°ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã¯æ±ç”¨çš„ãª in-context learning ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç™ºé”ã•ã›ã‚‹ã‚¤ãƒ³ã‚»ãƒ³ãƒ†ã‚£ãƒ–ãŒãªã„ã€‚ä¸¡æ–¹ã® ablation ã«ãŠã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç›¸äº’è£åˆ‡ã‚Šã«å´©å£Šã™ã‚‹ï¼ˆå›³1; ç ´ç·šãƒ»ç‚¹ç·šã®æ›²ç·šå‚ç…§ï¼‰ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¤šæ§˜ãªç›¸æ‰‹ã‚’è­˜åˆ¥ã™ã‚‹å¿…è¦æ€§ã«ã‚ˆã£ã¦èª˜å°ã•ã‚Œã‚‹ in-context learning ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒå”èª¿çš„çµæœã‚’å¯èƒ½ã«ã™ã‚‹é‡è¦ãªè¦å› ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã‚‹ã€‚Ablation ã®è©³ç´°ã¯ä»˜éŒ² A.4 ã‚’å‚ç…§ã•ã‚ŒãŸã„ã€‚

**3.2.** **ãƒ¡ã‚«ãƒ‹ã‚ºãƒ åˆ†æï¼šIn-context learning ã‹ã‚‰å”èª¿ã¸**

ã“ã“ã§ã€å”èª¿è¡Œå‹•ã®å­¦ç¿’ã‚’3ã¤ã®å€‹åˆ¥ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†è§£ã—ã€å¤šæ§˜æ€§ã‹ã‚‰ in-context learningã€extortabilityã€ãã—ã¦æœ€çµ‚çš„ã«å”èª¿ã«è‡³ã‚‹å› æœé€£é–ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

**Step 1: å¤šæ§˜æ€§ãŒ in-context best-response ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’èª˜å°ã™ã‚‹ã€‚** ã¾ãšã€Tabular ãƒ—ãƒ¼ãƒ«ã«å¯¾ã™ã‚‹è¨“ç·´ãŒ in-context learning ã‚’è‚²æˆã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ã€‚Tabular Agent ãƒ—ãƒ¼ãƒ«ã®ã¿ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚ŒãŸ PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©•ä¾¡ã™ã‚‹ã€‚å›³2 **B** ã¯ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµŒéã«ã‚ãŸã‚‹ç‰¹å®šã® Tabular æ–¹ç­–ã«å¯¾ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç‰¹å®šã®ç›¸æ‰‹ã«å¯¾ã™ã‚‹ best response ã«è¿…é€Ÿã«é©å¿œã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®é«˜é€Ÿã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«ã§ç›®æ¨™æŒ‡å‘ã®é©å¿œã‚’è¡Œã† _in-context best-response ãƒ¡ã‚«ãƒ‹ã‚ºãƒ _ ã®å‡ºç¾ãŒç¢ºèªã•ã‚Œã‚‹ã€‚

**Step 2: In-context learners ã¯ extortion ã«å¯¾ã—ã¦è„†å¼±ã§ã‚ã‚‹ã€‚** æ¬¡ã«ã€ã“ã®ã‚ˆã†ãª in-context best-response æ–¹ç­–ãŒä»–ã® co-player ã«ã‚ˆã‚‹ shaping ã«å½±éŸ¿ã•ã‚Œã‚„ã™ã„ã“ã¨ã‚’ç¢ºç«‹ã™ã‚‹ã€‚Step 1 ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‡çµã—ã€"Fixed In-Context Learner" (Fixed-ICL) ã¨å‘¼ã³ã€ã“ã‚Œã«å¯¾ã—ã¦ã®ã¿æ–°ã—ã„ PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã™ã‚‹ã€‚æ–°ã—ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ Fixed-ICL æ–¹ç­–ã‚’ _extort_ ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹ï¼ˆå›³2 **C** & **D**ï¼‰(Press & Dyson, 2012)ã€‚Fixed-ICL ã®é©å¿œå‚¾å‘ã‚’æ¾å–ã™ã‚‹ã“ã¨ã§ã€æ–°ã—ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãã‚Œã‚’ä¸å…¬å¹³ãªå”èª¿ã«è¿½ã„è¾¼ã¿ã€Fixed-ICL ã‚’çŠ ç‰²ã«ã—ã¦è‡ªèº«ã®å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®ç›®æ¨™æŒ‡å‘é©å¿œãŒã€ç›¸æ‰‹ãŒ weight æ›´æ–°ã‚’é€šã˜ã¦ extortionate è¡Œå‹•ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«å¿…è¦ãªå‹¾é…ä¿¡å·ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã‚‹ã€‚

**Step 3: ç›¸äº’ extortion ãŒå”èª¿ã‚’é§†å‹•ã™ã‚‹ã€‚** Step 2 ã§å­¦ç¿’ã•ã‚ŒãŸ extortion æ–¹ç­–ã§2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã€äº’ã„ã«å¯¾ã—ã¦è¨“ç·´ã™ã‚‹ã€‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã§ã€ä¸¡æ–¹ã® extortion æ–¹ç­–ãŒäº’ã„ã® in-context learning ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’ã‚ˆã‚Šå”èª¿çš„ãªè¡Œå‹•ã¸ã¨å½¢æˆã™ã‚‹ï¼ˆå›³2 **F**ï¼‰ã€‚ã‚ˆã‚Šå”èª¿çš„ãªæ–¹å‘ã¸ã®ã“ã® push ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã«ã‚ˆã‚Šæ‹¾ã„ä¸Šã’ã‚‰ã‚Œã€ä¸¡æ–¹ç­–ã‚’å”èª¿è¡Œå‹•ã¸ã¨ã•ã‚‰ã«é§†å‹•ã™ã‚‹ï¼ˆå›³2 **E**ï¼‰ã€‚ã“ã‚Œã¯ã€æ˜ç¤ºçš„ãª learning-aware æ‰‹æ³•ã§è¦³å¯Ÿã•ã‚ŒãŸ "mutual shaping" åŠ¹æœã‚’åæ˜ ã—ã¦ã„ã‚‹ (Lu et al., 2022; Meulemans et al., 2025a)ã€‚

**Step 4: æ··åˆé›†å›£ã«ãŠã‘ã‚‹çµ±åˆã€‚** æ··åˆãƒ—ãƒ¼ãƒ«è¨“ç·´ã¯ã“ã‚Œã‚‰ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’çµ±åˆã™ã‚‹ï¼šTabular ç›¸æ‰‹ã«å¯¾ã™ã‚‹ in-context é©å¿œã®ç¶­æŒã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å¼·åˆ¶ã—ã€ã“ã‚ŒãŒä»–ã® learners ã«ã‚ˆã‚‹ç›¸äº’ extortion ã«å¯¾ã—ã¦è„†å¼±ã«ã—ã€æœ€çµ‚çš„ã«ç›¸äº’ extortion ã‚’é€šã˜ã¦ Learning Agent ã‚’å”èª¿ã¸ã¨é§†å‹•ã™ã‚‹ (Sec. 3.1; å›³1 & å›³3)ã€‚ä»˜éŒ² B.2 ã®å›³4ã¯ A2C Learning Agent ã§ã‚‚åŒæ§˜ã®çµæœã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

### **4. çµè«–**

æœ¬ç ”ç©¶ã§ã¯ã€æ˜ç¤ºçš„ãª co-player learning-awareness ã®è¤‡é›‘ãªæ©Ÿæ§‹â€”â€”meta gradient ã‚„å³æ ¼ãªã‚¿ã‚¤ãƒ ã‚¹ã‚±ãƒ¼ãƒ«åˆ†é›¢ãªã©â€”â€”ãŒä¸€èˆ¬å’Œã‚²ãƒ¼ãƒ ã«ãŠã‘ã‚‹å”èª¿è¡Œå‹•ã®å­¦ç¿’ã«å¿…è¦ã§ã¯ãªã„ã“ã¨ã‚’å®Ÿè¨¼ã—ãŸã€‚ä»£ã‚ã‚Šã«ã€å¤šæ§˜ãª co-player ã®åˆ†å¸ƒã«å¯¾ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å˜ã«è¨“ç·´ã™ã‚‹ã ã‘ã§ in-context best-response æˆ¦ç•¥ã‚’èª˜å°ã™ã‚‹ã®ã«ååˆ†ã§ã‚ã‚‹ã“ã¨ã‚’è¦‹å‡ºã—ãŸã€‚ã“ã® in-context learning ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ shaping ã«å¯¾ã—ã¦è„†å¼±ã«ã—ã€çµæœã¨ã—ã¦ç›¸äº’ extortion ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’é€šã˜ã¦å”èª¿è¡Œå‹•ã¸ã¨é§†å‹•ã™ã‚‹ã€‚é‡è¦ãªã“ã¨ã«ã€ã“ã®çµæœã¯ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã¨ç¾ä»£ã® Foundation Model ã®è¨“ç·´ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’æ©‹æ¸¡ã—ã™ã‚‹ã€‚Foundation Model ã¯æœ¬æ¥çš„ã« in-context learning ã‚’ç¤ºã—ã€å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã¨è¡Œå‹•ã§è¨“ç·´ã•ã‚Œã‚‹ãŸã‚ã€æœ¬ç ”ç©¶ã®çŸ¥è¦‹ã¯æ¨™æº–çš„ãªåˆ†æ•£å­¦ç¿’æŠ€è¡“ã‚’ç”¨ã„ãŸå”èª¿çš„ç¤¾ä¼šè¡Œå‹•ã®å‡ºç¾ã®ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã‹ã¤è¨ˆç®—åŠ¹ç‡ã®è‰¯ã„é“ç­‹ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚

### **è¬è¾**

Guillaume Lajoieã€Angelika Stegerã€ãŠã‚ˆã³ Google Paradigms of Intelligence ãƒãƒ¼ãƒ ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨æœ‰ç›Šãªè­°è«–ã«æ„Ÿè¬ã™ã‚‹ã€‚

### **å‚è€ƒæ–‡çŒ®**

Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. _arXiv preprint arXiv:1806.06920_, 2018.

Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, Shunichi Akatsuka, and Aaron Courville. Best response shaping. _arXiv preprint arXiv:2404.06519_, 2024a.

Milad Aghajohari, Juan Agustin Duque, Tim Cooijmans, and Aaron Courville. Loqa: Learning with opponent q-learning awareness. _arXiv preprint arXiv:2405.01035_, 2024b.

Blaise Aguera Y Arcas, Benjamin Bratton, and James Evans. The silicon interior, feb 2026. URL [https://antikythera.substack.com/p/the-silicon-interior](https://antikythera.substack.com/p/the-silicon-interior). Accessed: 2026-2-12.

Robert Axelrod and William D. Hamilton. The evolution of cooperation. _Science_, 211(4489):1390â€“1396, March 1981.

Jan Balaguer, Raphael Koster, Christopher Summerfield, and Andrea Tacchetti. The good shepherd: An oracle agent for mechanism design. _arXiv preprint arXiv:2202.10135_, 2022.

Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Technical report, UniversitÃ© de MontrÃ©al, DÃ©partement d'Informatique et de Recherche opÃ©rationnelle, 1990.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.

Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. _AAAI/IAAI_, 1998(746-752):2, 1998.

Tim Cooijmans, Milad Aghajohari, and Aaron Courville. Meta-value learning: a general framework for learning with learning awareness. _arXiv preprint arXiv:2307.08863_, 2023.

DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, MiloÅ¡ StanojeviÄ‡, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL [http://github.com/google-deepmind](http://github.com/google-deepmind).

Juan Agustin Duque, Milad Aghajohari, Tim Cooijmans, Razvan Ciuca, Tianyu Zhang, Gauthier Gidel, and Aaron Courville. Advantage alignment algorithms. _arXiv preprint arXiv:2406.14662_, 2024.

Jakob Foerster, Richard Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In _International Conference on Autonomous Agents and Multiagent Systems_, 2018.

Charles R. Harris, K. Jarrod Millman, StÃ©fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime FernÃ¡ndez del RÃ­o, Mark Wiebe, Pearu Peterson, Pierre GÃ©rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357â€“362, 2020.

Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2024. URL [http://github.com/google/flax](http://github.com/google/flax).

Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz De Cote. A survey of learning in multiagent environments: Dealing with non-stationarity. _arXiv preprint arXiv:1707.09183_, 2017.

Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent. In _International Conference on Artificial Neural Networks_, Lecture Notes in Computer Science. Springer, 2001.

J. D. Hunter. Matplotlib: A 2D graphics environment. _Computing in Science & Engineering_, 9(3):90â€“95, 2007.

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In _Proceedings of the nineteenth international conference on machine learning_, pp. 267â€“274, 2002.

Akbir Khan, Timon Willi, Newton Kwan, Andrea Tacchetti, Chris Lu, Edward Grefenstette, Tim RocktÃ¤schel, and Jakob N. Foerster. Scaling opponent shaping to high dimensional games. In _International Conference on Autonomous Agents and Multiagent Systems_, 2024.

H. W. Kuhn. _Extensive games and the problem of information_. Princeton University Press, 1953.

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.

Christopher Lu, Timon Willi, Christian A Schroeder De Witt, and Jakob Foerster. Model-free opponent shaping. In _International Conference on Machine Learning_, 2022.

Alexander Meulemans, Seijin Kobayashi, Johannes von Oswald, Nino Scherrer, Eric Elmoznino, Blake Richards, Guillaume Lajoie, JoÃ£o Sacramento, et al. Multi-agent cooperation through learning-aware policy gradients. _ICLR_, 2025a.

Alexander Meulemans, Rajai Nasser, Maciej WoÅ‚czyk, Marissa A. Weis, Seijin Kobayashi, Blake Richards, Guillaume Lajoie, Angelika Steger, Marcus Hutter, James Manyika, Rif A. Saurous, JoÃ£o Sacramento, and Blaise AgÃ¼era y Arcas. Embedded universal predictive intelligence: a coherent framework for multi-agent learning, 2025b.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In _International Conference on Machine Learning_, 2016.

Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In _Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology_, 2023.

Juan Perdomo, Tijana Zrnic, Celestine Mendler-DÃ¼nner, and Moritz Hardt. Performative prediction. In _International Conference on Machine Learning_, pp. 7599â€“7609. PMLR, 2020.

Dereck Piche, Mohammed Muqeeth, Milad Aghajohari, Juan Duque, Michael Noukhovitch, and Aaron Courville. Learning robust social strategies with large language models. _arXiv preprint arXiv:2511.19405_, 2025.

William H. Press and Freeman J. Dyson. Iterated Prisoner's Dilemma contains strategies that dominate any evolutionary opponent. _Proceedings of the National Academy of Sciences_, 109(26):10409â€“10413, 2012.

Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. _arXiv preprint arXiv:1710.05941_, 2017.

Anatol Rapoport. Prisoner's dilemmaâ€”recollections and observations. In _Game Theory as a Theory of a Conflict Resolution_, pp. 17â€“34. Springer, 1974.

JÃ¼rgen Schmidhuber. _Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook_. Diploma thesis, Institut fÃ¼r Informatik, Technische UniversitÃ¤t MÃ¼nchen, 1987.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_, 2015.

Marta Emili Garcia Segura, Stephen Hailes, and Mirco Musolesi. Opponent shaping in llm agents. _arXiv preprint arXiv:2510.08255_, 2025.

Yoav Shoham and Kevin Leyton-Brown. _Multiagent systems: Algorithmic, game-theoretic, and logical foundations_. Cambridge University Press, 2008.

Michael L. Waskom. seaborn: statistical data visualization. _Journal of Open Source Software_, 6(60):3021, 2021.

Wes McKinney. Data Structures for Statistical Computing in Python. In StÃ©fan van der Walt and Jarrod Millman (eds.), _Proceedings of the 9th Python in Science Conference_, pp. 56â€“61, 2010.

Timon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. COLA: consistent learning with opponent-learning awareness. In _International Conference on Machine Learning_, 2022.

Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: a survey. _arXiv preprint arXiv:2309.07864_, 2023.

Annie Xie, Dylan Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. Learning latent representations to influence multi-agent interaction. In _Conference on Robot Learning_, 2021.

### **A. æ‰‹æ³•ã®è¿½åŠ è©³ç´°**

**A.1.** **Partially Observable Stochastic Games**

ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç›¸äº’ä½œç”¨ã‚’ Partially Observable Stochastic Game (POSG; Kuhn, 1953) ã¨ã—ã¦å½¢å¼åŒ–ã™ã‚‹ã€‚ã‚¿ãƒ—ãƒ« (I, S, A, _ğ‘ƒğ‘¡_, _ğ‘ƒğ‘Ÿ_, _ğ‘ƒğ‘–_, O, _ğ‘ƒğ‘œ_, _ğ›¾_, _ğ‘‡_) ã§å®šç¾©ã•ã‚Œã‚‹ã€‚ã“ã“ã§ I = {1, ..., _ğ‘›_} ã¯ _ğ‘›_ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é›†åˆã§ã‚ã‚‹ã€‚å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ— _ğ‘¡_ ã§ç’°å¢ƒã¯çŠ¶æ…‹ _ğ‘ ğ‘¡_ âˆˆ S ã«ã‚ã‚‹ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å…±åŒè¡Œå‹•ç©ºé–“ A = Ã— _ğ‘–_ âˆˆI A _[ğ‘–]_ ã‹ã‚‰åŒæ™‚ã«è¡Œå‹•ã‚’é¸æŠã—ã€_ğ‘ƒğ‘¡_(_ğ‘†ğ‘¡_+1 | _ğ‘†ğ‘¡_, _ğ´ğ‘¡_) ã«å¾“ã£ã¦ç’°å¢ƒã‚’é·ç§»ã•ã›ã‚‹ã€‚åˆæœŸçŠ¶æ…‹ã¯ _ğ‘ƒğ‘–_(_ğ‘ _0) ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã•ã‚Œã‚‹ã€‚å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã¯å…±åŒåˆ†è§£åˆ†å¸ƒ _ğ‘ƒğ‘Ÿ_ = Ã— _ğ‘–_ âˆˆI _ğ‘ƒğ‘Ÿ_ _[ğ‘–]_(_ğ‘Ÿ_ _[ğ‘–]_ | _ğ‘ _, _ğ‘_) ã‹ã‚‰å ±é…¬ _ğ‘Ÿğ‘¡_ _[ğ‘–]_ ã‚’ã€è¦³æ¸¬ç©ºé–“ O = Ã— _ğ‘–_ âˆˆI O _[ğ‘–]_ ã‹ã‚‰åˆ†å¸ƒ _ğ‘ƒğ‘œ_(_ğ‘œğ‘¡_ | _ğ‘ ğ‘¡_, _ğ‘ğ‘¡_âˆ’1) ã‚’é€šã˜ã¦è¦³æ¸¬ _ğ‘œğ‘¡_ _[ğ‘–]_ ã‚’å—ã‘å–ã‚‹ã€‚å‰²å¼•ç‡ã‚’ _ğ›¾_ã€ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã‚’ _ğ‘‡_ ã§è¡¨ã™ã€‚ä¸Šä»˜ãæ–‡å­— _ğ‘–_ ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã«å›ºæœ‰ã®å¤‰æ•°ã‚’ã€âˆ’_ğ‘–_ ã¯æ®‹ã‚Šã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¡¨ã™ã€‚æ–¹ç­–ã¯ç›¸äº’ä½œç”¨å±¥æ­´ _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ = {(_ğ‘œ_ _[ğ‘–]_ _ğ‘˜_, _ğ‘_ _[ğ‘–]_ _ğ‘˜_âˆ’1, _ğ‘Ÿ_ _ğ‘˜_ _[ğ‘–]_âˆ’1)} _ğ‘¡_ _ğ‘˜_=1 ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã®æ–¹ç­–ã‚’ _ğœ‹_ _[ğ‘–]_(_ğ‘ğ‘¡_ _[ğ‘–]_ | _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_; _ğœ™_ _[ğ‘–]_) ã¨è¡¨è¨˜ã—ã€_ğœ™_ _[ğ‘–]_ ã«ã‚ˆã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã™ã‚‹ã€‚

**A.2.** **ç’°å¢ƒ**

**Iterated Prisoners Dilemma (IPD)** å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯2ã¤ã®è¡Œå‹•ã‚’å‡ºåŠ›ã§ãã‚‹ï¼šå”èª¿ (_ğ¶_) ã¨è£åˆ‡ã‚Š (_ğ·_)ã€‚ã—ãŸãŒã£ã¦ã€ç’°å¢ƒã¯5ã¤ã®å¯èƒ½ãªè¦³æ¸¬ã‚’å‡ºåŠ›ã™ã‚‹ï¼šåˆæœŸè¦³æ¸¬ _ğ‘ _0 ã¨ã€å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã§2ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå–ã£ãŸè¡Œå‹•ã«åŸºã¥ã4ã¤ã®è¦³æ¸¬ (_ğ¶_, _ğ¶_), (_ğ¶_, _ğ·_), (_ğ·_, _ğ¶_), (_ğ·_, _ğ·_) ã§ã‚ã‚‹ã€‚çŠ¶æ…‹ _ğ‘ ğ‘¡_ ã¯éå»ã®å…¨è¦³æ¸¬ _ğ‘œ_ â‰¤ _ğ‘¡_ ã‹ã‚‰ãªã‚‹ã€‚Tabular Agent ã¯æœ€æ–°ã®è¦³æ¸¬ _ğ‘œğ‘¡_ ã®ã¿ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚Œã‚‹ãŒã€PPI ã¨ A2C ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å®Œå…¨ãªå±¥æ­´ _ğ‘¥_ â‰¤ _ğ‘¡_ ã‚’æ´»ç”¨ã™ã‚‹ã€‚å„ã‚²ãƒ¼ãƒ ã¯100ãƒ©ã‚¦ãƒ³ãƒ‰ã‹ã‚‰ãªã‚‹ã€‚å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã®çŠ¶æ…‹ã‚’ä¸€äººç§°è¦–ç‚¹ã§è¦³æ¸¬ã™ã‚‹ï¼ˆè‡ªèº«ã®è¡Œå‹•ãŒå…ˆã«åˆ—æŒ™ã•ã‚Œã‚‹ï¼‰ã€‚å„ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã€å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¡¨1ã®åˆ©å¾—è¡Œåˆ—ã«å¾“ã£ã¦å ±é…¬ã‚’å—ã‘å–ã‚‹ã€‚

**è¡¨1** | å˜ç™º IPD åˆ©å¾—è¡Œåˆ—

|  | Player 2 C | Player 2 D |
|---|---|---|
| **Player 1 C** | (1, 1) | (-1, 2) |
| **Player 1 D** | (2, -1) | (0, 0) |

**A.3.** **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…**

_**A.3.1.**_ _**PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**_

Predictive Policy Improvement (PPI) ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€embedded Bayesian agents (Meulemans et al., 2025b) ã®å®Ÿç”¨çš„ãªè¿‘ä¼¼ã§ã‚ã‚Šã€å­¦ç¿’ã•ã‚ŒãŸ Sequence Model ã¨è¨ˆç”»ãƒ™ãƒ¼ã‚¹ã® Policy Improvement ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã€‚

**Sequence Model ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚** Sequence Model ã¯128æ¬¡å…ƒã® Hidden State ã‚’æŒã¤ Gated Recurrent Unit (GRU) ã§ã‚ã‚‹ã€‚è¦³æ¸¬ã€è¡Œå‹•ã€å ±é…¬ã‹ã‚‰ãªã‚‹å…¥åŠ›ã¯ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£å›ºæœ‰ã®ç·šå½¢å±¤ã‚’é€šã˜ã¦å…±æœ‰32æ¬¡å…ƒ Embedding ç©ºé–“ã«å°„å½±ã•ã‚Œã‚‹ã€‚è¦³æ¸¬ã¨è¡Œå‹•ã¯å°„å½±å‰ã« One-Hot ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚ã“ã‚Œã‚‰ã® Embedding ã¯ GRU ã¸ã®å…¥åŠ›ã¨ã—ã¦æ©Ÿèƒ½ã—ã€å‡ºåŠ›ã« Swish æ´»æ€§åŒ–é–¢æ•° (Ramachandran et al., 2017) ã‚’é©ç”¨ã™ã‚‹ã€‚åˆ¥å€‹ã®ç·šå½¢å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ãŒ Hidden State ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã€å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å°†æ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

**è¨“ç·´ç›®çš„ã€‚** Sequence Model ã‚’30ãƒ•ã‚§ãƒ¼ã‚ºã«ã‚ãŸã‚Šåå¾©çš„ã«è¨“ç·´ã™ã‚‹ã€‚å„ãƒ•ã‚§ãƒ¼ã‚ºã§ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœ™_ ã¯å†åˆæœŸåŒ–ã•ã‚Œã€ç›¸äº’ä½œç”¨å±¥æ­´ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ D = {_ğ‘¥_ _(ğ‘›)_} _ğ‘›_=1 _ğ‘_ ã«å¯¾ã—ã¦æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬æå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†è¨“ç·´ã•ã‚Œã‚‹ï¼š

**Algorithm 1** Predictive Policy Improvement

**Require:** åˆæœŸ Sequence Model _ğ‘ğœ™_0ã€å¼·åŒ–å­¦ç¿’ç’°å¢ƒ Eã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° _ğ‘_iterã€è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•° _ğ‘_epochsã€ã‚µãƒ³ãƒ—ãƒ«æ•° _ğ‘_samplesã€åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ D0
1: **for** _ğ‘˜_ = 1 to _ğ‘_iter **do**
2: _ğ‘ğœ™ğ‘˜_ ã®é‡ã¿ _ğœ™ğ‘˜_ ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–
3: **for** _ğ‘’_ = 1 to _ğ‘_epochs **do** âŠ² Step 1: Sequence Model ã®è¨“ç·´
4: D_ğ‘˜_âˆ’1 ã‚’ç”¨ã„ã¦ _ğ‘ğœ™ğ‘˜_ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€å¼2ã®æå¤±é–¢æ•° _ğ¿_train ã‚’æœ€å°åŒ–
5: **end for**
6: ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ R_ğ‘˜_ ã‚’åˆæœŸåŒ–
7: **for** _ğ‘Ÿ_ = 1 to _ğ‘_samples **do** âŠ² Step 2: ã‚²ãƒ¼ãƒ è»Œé“ã®åé›†
8: ç’°å¢ƒ E ã‚’ãƒªã‚»ãƒƒãƒˆ
9: E å†…ã§ _ğ‘ğœ™ğ‘˜_ ã‚’ç”¨ã„ã¦è¡Œå‹•/è¦³æ¸¬ã®ç³»åˆ—ã‚’ç”Ÿæˆ
10: E ã‹ã‚‰è»Œé“ _ğœğ‘Ÿ_ = (_ğ‘œ_0, _ğ‘Ÿ_0, _ğ‘_0, _ğ‘œ_1, _ğ‘Ÿ_1, _ğ‘_1, ...) ã‚’åé›†
11: _ğœğ‘Ÿ_ ã‚’ R_ğ‘˜_ ã«è¿½åŠ 
12: **end for**
13: æ¬¡ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨“ç·´ã®ãŸã‚ã« D_ğ‘˜_ â† D_ğ‘˜_âˆ’1 âˆª R_ğ‘˜_ ã‚’è¨­å®š
14: **end for**

_ğ¿_train = _ğœ†_obs _ğ¿_obs + _ğœ†_act _ğ¿_action + _ğœ†_rew _ğ¿_reward, (2)

_ğ¿_obs = âˆ’1/(_ğ‘ğ‘‡_) Î£_ğ‘›_ Î£_ğ‘¡_ log _ğ‘ğœ™_(_ğ‘œğ‘¡_ _(ğ‘›)_ | _ğ‘¥_ â‰¤ _(ğ‘›)_ _ğ‘¡_âˆ’1), (3)

_ğ¿_reward = âˆ’1/(_ğ‘ğ‘‡_) Î£_ğ‘›_ Î£_ğ‘¡_ log _ğ‘ğœ™_(_ğ‘Ÿğ‘¡_ _(ğ‘›)_ | _ğ‘¥_ â‰¤ _(ğ‘›)_ _ğ‘¡_âˆ’1, _ğ‘œğ‘¡_ _(ğ‘›)_), (4)

_ğ¿_action = âˆ’1/(_ğ‘ğ‘‡_) Î£_ğ‘›_ Î£_ğ‘¡_ log _ğ‘ğœ™_(_ğ‘ğ‘¡_ _(ğ‘›)_ | _ğ‘¥_ â‰¤ _(ğ‘›)_ _ğ‘¡_âˆ’1, _ğ‘œğ‘¡_ _(ğ‘›)_, _ğ‘Ÿğ‘¡_ _(ğ‘›)_). (5)

D ã¯å…¨ã¦ã®éå»ãŠã‚ˆã³ç¾åœ¨ã®ãƒ•ã‚§ãƒ¼ã‚ºã‹ã‚‰ã®ç›¸äº’ä½œç”¨å±¥æ­´ã§æ§‹æˆã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯ Performative Prediction (Perdomo et al., 2020) ã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªæˆ¦ç•¥ã§ã‚ã‚Šã€äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã‚Šå®‰å®šã—ãŸè¨“ç·´ã‚’ç¢ºä¿ã™ã‚‹ã€‚

_ğ‘ğœ™_(_ğ‘ğ‘¡_ | _ğ‘¥_ â‰¤ _ğ‘¡_) ã¨ _ğ‘ğœ™_(_ğ‘œğ‘¡_ | _ğ‘¥_<_ğ‘¡_, _ğ‘ğ‘¡_âˆ’1) ã‚’ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€æ¨™æº–çš„ãªã‚«ãƒ†ã‚´ãƒªã‚«ãƒ« Cross-Entropy æå¤±ã‚’å¾—ã‚‹ã€‚_ğ‘ğœ™_(_ğ‘Ÿğ‘¡_ | _ğ‘¥_<_ğ‘¡_, _ğ‘ğ‘¡_âˆ’1, _ğ‘œğ‘¡_) ã¯å›ºå®šåˆ†æ•£ã®æ­£è¦åˆ†å¸ƒã§ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€å¹³å‡äºŒä¹—èª¤å·®æå¤± (_ğ‘Ÿ_ âˆ’ _ğ‘ŸÌ‚_)Â² ã‚’å¾—ã‚‹ã€‚å„ãƒ•ã‚§ãƒ¼ã‚ºã§20,000è»Œé“ã‚’ã‚µãƒ³ãƒ—ãƒ«ã—ã€å‰ãƒ•ã‚§ãƒ¼ã‚ºã®ã‚µãƒ³ãƒ—ãƒ«ã¨é€£çµã—ã¦ Sequence Model ã®å…±åŒè¨“ç·´ã«ä½¿ç”¨ã™ã‚‹ã€‚æœ€é©åŒ–ã¯ AdamW (Loshchilov & Hutter, 2017)ï¼ˆå­¦ç¿’ç‡ 10â»â´ã€Weight Decay 10â»Â²ã€_ğ›½_1 = 0.9ã€_ğ›½_2 = 0.98ï¼‰ã§ã€ãƒãƒƒãƒã‚µã‚¤ã‚º256ã€10ã‚¨ãƒãƒƒã‚¯å®Ÿæ–½ã™ã‚‹ã€‚å‹¾é…ã¯ãƒãƒ«ãƒ 1.0ã§ã‚¯ãƒªãƒƒãƒ—ã™ã‚‹ã€‚

**äº‹å‰è¨“ç·´ã€‚** Sequence Model ã¯ã€2ã¤ã®ãƒ©ãƒ³ãƒ€ãƒ ãª Tabular Agent ãŒ100ãƒ©ã‚¦ãƒ³ãƒ‰ã® IPD ã‚’å¯¾æˆ¦ã™ã‚‹200,000ã‚µãƒ³ãƒ—ãƒ«è»Œé“ã®åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ D0 ã§äº‹å‰è¨“ç·´ã•ã‚Œã€ä¸Šè¨˜ã¨åŒã˜è¨“ç·´ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

**æ¨è«–ã€‚** å±•é–‹æ™‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å­¦ç¿’ã•ã‚ŒãŸ Sequence Model ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã—ã€15ãƒ©ã‚¦ãƒ³ãƒ‰å…ˆã® Monte Carlo Rollout ã‚’å®Ÿè¡Œã—ã¦ Q å€¤ã‚’æ¨å®šã™ã‚‹ã€‚æœ€çµ‚çš„ãªè¡Œå‹•é¸æŠã¯ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ç¢ºç‡ _ğ‘_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_; _ğœ™_) ã‚’ Rollout ã‹ã‚‰å°å‡ºã•ã‚ŒãŸæ¨å®šå€¤ _ğ‘„Ì‚_ _ğ‘_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) ã§é‡ã¿ä»˜ã‘ã™ã‚‹æ–¹ç­– _ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) ã«å¾“ã†ï¼š

_ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) = (1/_ğ‘_) _ğ‘_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_; _ğœ™_) exp(_ğ›½ğ‘„Ì‚_ _ğ‘_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_)). (6)

å…¨å®Ÿé¨“ã§ _ğ›½_ = 0.01 ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

_**A.3.2.**_ _**Model-Free ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**_

**ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚** PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨åŒã˜æ§‹æˆã® GRU ãƒ™ãƒ¼ã‚¹ Sequence Model ã‚’ä½¿ç”¨ã—ãŸ Advantage Actor-Critic (A2C) ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Mnih et al., 2016) ã‚’å®Ÿè£…ã™ã‚‹ã€‚GRU ã¯å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã®è¦³æ¸¬å±¥æ­´ã‚’å…¥åŠ›ã¨ã—ã€æ¬¡ã®è¡Œå‹•ã‚’å‡ºåŠ›ã™ã‚‹ã€‚GRU ã«ã¯ä¾¡å€¤é–¢æ•° _ğ‘‰_(_ğ‘¥_) ã‚’æ¨å®šã™ã‚‹ãŸã‚ã®ç·šå½¢å‡ºåŠ›ãƒ˜ãƒƒãƒ‰ã‚‚ä»˜åŠ ã•ã‚Œã‚‹ã€‚è¨“ç·´ä¸­ã€ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ— Temporal-Difference èª¤å·®ã‚’ç”¨ã„ã¦ Advantage _ğ´_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘ğ‘¡_) ã‚’æ¨å®šã™ã‚‹ï¼š

_ğ´_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘ğ‘¡_) = _ğ‘Ÿğ‘¡_ + _ğ›¾ğ‘‰_(_ğ‘¥_ â‰¤ _ğ‘¡_+1) âˆ’ _ğ‘‰_(_ğ‘¥_ â‰¤ _ğ‘¡_).

ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ Policy Gradient ã¨ Value Estimation Loss ã®çµåˆæå¤±ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†æ›´æ–°ã•ã‚Œã‚‹ï¼š

_ğ¿_ = Î£_ğ‘¡_ [âˆ’log _ğœ‹_(_ğ‘ğ‘¡_ | _ğ‘¥_ â‰¤ _ğ‘¡_) _ğ´_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘ğ‘¡_) + _ğ‘ğ‘£_(_ğ‘Ÿğ‘¡_ + _ğ›¾ğ‘‰_(_ğ‘¥_ â‰¤ _ğ‘¡_+1) âˆ’ _ğ‘‰_(_ğ‘¥_ â‰¤ _ğ‘¡_))Â² + _ğ‘ğ‘’_ Î£_ğ‘–_ _ğœ‹_(_ğ‘ğ‘¡_ _[ğ‘–]_ | _ğ‘¥_ â‰¤ _ğ‘¡_) log _ğœ‹_(_ğ‘ğ‘¡_ _[ğ‘–]_ | _ğ‘¥_ â‰¤ _ğ‘¡_)]

ã“ã“ã§ _ğ‘ğ‘£_, _ğ‘ğ‘’_ ã¯ãã‚Œãã‚Œä¾¡å€¤é–¢æ•°ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨“ç·´ä¿‚æ•°ã‚’è¡¨ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚

**è¨“ç·´ã€‚** æ¯”è¼ƒå¯èƒ½ãªçµæœã‚’å¾—ã‚‹ãŸã‚ã€Meulemans et al. (2025a) ã® A2C è¨“ç·´ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã«å¾“ã„ã€ä¾¡å€¤é–¢æ•°æ¨å®šã€Generalized Advantage Estimation (Schulman et al., 2015)ã€Advantage Normalizationã€Reward Scaling ã‚’å«ã‚€ã€‚è©³ç´°ã¯ Meulemans et al. (2025a) ã®ä»˜éŒ²Aã‚’å‚ç…§ã•ã‚ŒãŸã„ã€‚

å„å®Ÿé¨“ã§ã€å­¦ç¿’ç‡ã€GAE Lambdaã€Advantage Normalizationã€Reward Scalingã€Entropy Regularization ã«ã¤ã„ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿæ–½ã™ã‚‹ã€‚æœ€é«˜æ€§èƒ½ã®è¨­å®šã«å¯¾å¿œã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨2ã«å ±å‘Šã™ã‚‹ã€‚

_**A.3.3.**_ _**Tabular ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**_

Tabular Agent ã¯ Memory-1 æ–¹ç­–ã‚’ä½¿ç”¨ã—ã€5ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å®šç¾©ã•ã‚Œã‚‹ï¼šå‰å›ã®çµæœã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚ŒãŸå”èª¿ç¢ºç‡ (_ğ‘ğ‘_, _ğ‘ğ‘‘_, _ğ‘‘ğ‘_, _ğ‘‘ğ‘‘_) ã¨åˆæœŸçŠ¶æ…‹ (_ğ‘ _0)ã€‚å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸€æ§˜åˆ†å¸ƒ U(0, 1) ã‹ã‚‰åˆæœŸåŒ–ã•ã‚Œã‚‹ã€‚

**A.4.** **Ablation**

_**A.4.1.**_ _**Policy Conditioning**_

"Opponent ID" Ablationï¼ˆå›³1ï¼‰ã§ã¯ã€è¦³æ¸¬ç³»åˆ— **x** â‰¤ _ğ‘¡_ ã®å‰ã«ç›¸æ‰‹ã®ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¡¨ã™æ¡ä»¶ä»˜ã‘ãƒ™ã‚¯ãƒˆãƒ« **z** ã‚’ä»˜åŠ ã™ã‚‹ã€‚Tabular Agent ã®å ´åˆã€**z** ã¯å…¨å¯èƒ½ãªè¦³æ¸¬ _ğ‘œ_ âˆˆ O ã«ã‚ãŸã‚‹å¯¾æ•°ç¢ºç‡ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹ï¼š

**z** = [log _ğœ‹_(_ğ‘_ | _ğ‘œ_)] _ğ‘œ_ âˆˆ O, _ğ‘_ âˆˆ A

ã“ã“ã§ O = {(_ğ¶_, _ğ¶_), (_ğ¶_, _ğ·_), (_ğ·_, _ğ¶_), (_ğ·_, _ğ·_), Start}ã€‚A2C ãŠã‚ˆã³ PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ´åˆã€**z** = **0**ã€‚

**è¡¨2** | A2C ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| RL Hyperparameter | Step 1 | Step 2 | Step 3 | Step 4 |
|---|---|---|---|---|
| `advantages_normalization` | True | False | True | True |
| `batch_size` | 2048 | 2048 | 4096 | 4096 |
| `reward_rescaling` | 0.2 | 0.05 | 0.02 | 0.02 |
| `value_discount` (_ğ›¾_) | 0.99 | 0.99 | 0.99 | 0.99 |
| `td_lambda` (_ğœ†_td) | 0.99 | 1.0 | 0.95 | 1.0 |
| `gae_lambda` (_ğœ†_gae) | 0.99 | 1.0 | 0.95 | 1.0 |
| `value_coefficient` | 0.5 | 0.5 | 0.5 | 0.5 |
| `entropy_reg` | 0.001 | 0.001 | 0.001 | 0.01 |
| `optimizer` | Adam | Adam | Adam | Adam |
| `adam_epsilon` | 0.00001 | 0.00001 | 0.00001 | 0.00001 |
| `learning_rate` | 0.005 | 0.005 | 0.0005 | 0.001 |
| `max_grad_norm` | 1.0 | 1.0 | 1.0 | 1.0 |

_**A.4.2.**_ _**Mixed pool training ãªã—**_

"No Tabular Opponents" Ablationï¼ˆå›³1ï¼‰ã§ã¯ã€PPI ã¨ A2C ã®ä¸¡å®Ÿé¨“ã§ Mixed Agent Pool ã‹ã‚‰ Tabular ç›¸æ‰‹ã‚’é™¤å»ã™ã‚‹ã€‚PPI ã§ã¯ã€ã•ã‚‰ã«äº‹å‰è¨“ç·´ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒ _ğ·_0 ã‚’ Tabular Agent ã‚’å«ã¾ãšã€ç´”ç²‹ãªãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•ç³»åˆ—ã¨ãã®å¯¾å¿œã™ã‚‹å ±é…¬ã§æ§‹æˆã™ã‚‹ã‚ˆã†å¤‰æ›´ã™ã‚‹ã€‚

### **B. è¿½åŠ çµæœ**

**B.1.** **Mixed pool training ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…è»Œé“**

å›³3ã¯ã€æ··åˆãƒ—ãƒ¼ãƒ«è¨­å®šï¼ˆSec. 3.1 å‚ç…§ï¼‰ã«ãŠã‘ã‚‹åˆæœŸè¨“ç·´æ™‚ã®å˜ä¸€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã§ã® PPI ã¨ A2C ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ï¼ˆPPI ã¯ Phase = 8ã€A2C ã¯è¨“ç·´ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ = 70kï¼‰ã€‚In-context ç›¸æ‰‹æ¨è«–ã®å‡ºç¾ã¨ã€ä»–ã® Learning Agent ã«å¯¾ã™ã‚‹å”èª¿ã¸ã®åˆæœŸå‹¾é…ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

**B.2.** **A2C ã«é–¢ã™ã‚‹è¿½åŠ çµæœ**

å›³4ã¯ã€æœ¬æ–‡ã®å›³2ã§æç¤ºã—ãŸ PPI ã®çµæœã«å¯¾å¿œã™ã‚‹ A2C ãƒ™ãƒ¼ã‚¹ã®çµæœã‚’ç¤ºã™ã€‚Step 1 ã§ã¯ã€PPI ã¨åŒæ§˜ã« A2C ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¤šæ§˜ãª Tabular Agent ã«å¯¾ã™ã‚‹ best response ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã‚‹ã€‚ã—ã‹ã— Step 2 ã§ã¯ã€æ–°ã—ãè¨“ç·´ã•ã‚ŒãŸ A2C ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚ˆã‚Šã‚‚ Fixed-ICL ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã—ã¦é«˜ã„å ±é…¬ã‚’ç²å¾—ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã‚‹ï¼ˆãã‚Œãã‚Œ âˆ¼1.25 vs. âˆ¼0.9ï¼‰ã€‚ã“ã‚Œã¯ (i) PPI ã® Fixed-ICL æ–¹ç­–ãŒæ¾å–ã•ã‚Œã«ãã„ã€ã¾ãŸã¯ (ii) A2C ãŒã‚ˆã‚Šè‰¯ã„æ¾å–è€…æ–¹ç­–ã‚’è¦‹ã¤ã‘ãŸã€ã®ã„ãšã‚Œã‹ã«èµ·å› ã—ã†ã‚‹ã€‚å›³4Dã«ãŠã‘ã‚‹æ¾å–ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®ä¸è¦å‰‡ãªå½¢çŠ¶ã¯ã€A2C æ¾å–è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ A2C Fixed-ICL æ–¹ç­–ã«å¯¾ã—ã¦è¤‡é›‘ãªæ•µå¯¾çš„æˆ¦ç•¥ã‚’å­¦ç¿’ã—ãŸã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚å¯¾ç…§çš„ã«ã€å›³2Dã® PPI extortion æ–¹ç­–ã¯ã‚ˆã‚Šè¦å‰‡çš„ãª extortion æ–¹ç­–ã§ã‚ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚æœ€å¾Œã« Step 3 ã§ã¯ã€A2C ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯åˆæœŸçš„ã«å”èª¿ã«å‘ã‹ã†ãŒã€è¨“ç·´ã®ä¸å®‰å®šæ€§ã«ã‚ˆã‚Šã€ã‚·ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦ã¯è£åˆ‡ã‚Šã«æˆ»ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚

![](figures/paper.pdf-13-6.png)

![](figures/paper.pdf-13-7.png)

**å›³3** | **æ··åˆè¨“ç·´ã«ãŠã‘ã‚‹ Best-Response ã®å‡ºç¾ã€‚** å›³1ã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®åæŸå‰ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã€‚PPI ã¨ A2C ã®ä¸¡æ–¹ãŒã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®é–‹å§‹æ™‚ã« counterpart ã‚’ extort ã—ã‚ˆã†ã¨ã—ã€ãã®å¾Œå”èª¿ãƒ¬ãƒ™ãƒ«ãŒå¢—åŠ ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã‚‹ã€‚åŒæ™‚ã«ã€non-tit-for-tat çš„ãª Tabular æ–¹ç­–ã¨ã—ã¦ç›¸æ‰‹ã‚’è­˜åˆ¥ã™ã‚‹ã“ã¨ã¯ã€é«˜ã„è£åˆ‡ã‚Šç‡ã«ã¤ãªãŒã‚‹ã€‚Error bar ã¯10å€‹ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã‚ãŸã‚‹æ¨™æº–åå·®ã‚’ç¤ºã™ã€‚

**Step 1)**

**å›³4** | **A-B: In-context best response ã®å‡ºç¾ã€‚** ãƒ©ãƒ³ãƒ€ãƒ ãª Tabular ç›¸æ‰‹ã«å¯¾ã—ã¦è¨“ç·´ã•ã‚Œã€åæŸå¾Œã«ç‰¹å®šã®é™çš„æ–¹ç­–ã‚»ãƒƒãƒˆã§è©•ä¾¡ã•ã‚ŒãŸ A2C ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚æœ€çµ‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ "Fixed In-Context Learner" ã¨å‘¼ã¶ã€‚**C-D: In-context learners ã® Extortion ã®å­¦ç¿’ã€‚** ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸ A2C ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® Fixed In-Context Learner ã«å¯¾ã™ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚**E-F: ç›¸äº’ Extortion ã‹ã‚‰å”èª¿ã¸ã€‚** 2ã¤ã® A2C extortion ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯äº’ã„ã«å¯¾æˆ¦ã™ã‚‹ã¨åˆæœŸçš„ã«å”èª¿ã«åæŸã™ã‚‹ãŒã€ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦ã¯æ™‚é–“ã¨ã¨ã‚‚ã«ç›¸äº’è£åˆ‡ã‚Šã«å´©å£Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚Error bar ã¯5ã¤ã®ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã«ã‚ãŸã‚‹æ¨™æº–åå·®ã«å¯¾å¿œã™ã‚‹ã€‚

![](figures/paper.pdf-14-7.png)

![](figures/paper.pdf-14-8.png)

![](figures/paper.pdf-14-12.png)

![](figures/paper.pdf-14-13.png)

![](figures/paper.pdf-14-32.png)

![](figures/paper.pdf-14-33.png)

![](figures/paper.pdf-14-37.png)

![](figures/paper.pdf-14-38.png)

### **C. Predictive Policy Improvement (PPI) ã®å°å‡º**

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Predictive Policy Improvement (PPI) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å½¢å¼çš„ãªå°å‡ºã‚’æä¾›ã™ã‚‹ã€‚PPI ã¯ç†è«–çš„ã«åŸºç¤ã®ã‚ã‚‹ MUPI ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ (Meulemans et al., 2025b) ã«è§¦ç™ºã•ã‚Œã€Maximum a Posteriori Policy Optimization (MPO; Abdolmaleki et al., 2018) ã¨å¯†æ¥ã«é–¢é€£ã—ã¦ã„ã‚‹ã€‚PPI ã¯æ¨™æº–çš„ãª MPO ã®å€‹åˆ¥ã®ä¾¡å€¤é–¢æ•°ã¨ Self-Model ã‚’ã€è¡Œå‹•ã€è¦³æ¸¬ã€å ±é…¬ã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã† Self-Supervised ã«è¨“ç·´ã•ã‚ŒãŸå˜ä¸€ã® Sequence Model ã«ç½®ãæ›ãˆã‚‹ç‚¹ã§ç•°ãªã‚‹ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ World Model ã¨ Policy Prior ã®ä¸¡æ–¹ã¨ã—ã¦åŒæ™‚ã«æ©Ÿèƒ½ã—ã€Sequence Model ã®ç”Ÿæˆèƒ½åŠ›ã‚’å€¤æ¨å®šã¨æ–¹ç­–è¡¨ç¾ã«æ´»ç”¨ã™ã‚‹ã€‚

**C.1.** **ç›®çš„é–¢æ•°ï¼šå¤‰åˆ†ä¸‹ç•Œ**

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ _ğ‘‰_(_ğœ‹_) = ğ”¼_ğœ_âˆ¼â„™_ğœ‹_ [Î£_ğ‘¡_=0^_ğ‘‡_ _ğ›¾_^_ğ‘¡_ _ğ‘Ÿğ‘¡_] ã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†æ–¹ç­– _ğœ‹_ ã‚’æœ€é©åŒ–ã™ã‚‹å•é¡Œã‚’è€ƒãˆã‚‹ã€‚è¡¨è¨˜ã®ç°¡æ½”ã•ã®ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå›ºæœ‰ã®ä¸Šä»˜ãæ–‡å­—ã‚’çœç•¥ã™ã‚‹ï¼ˆã“ã®å°å‡ºã¯å˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã«ã‚‚ç­‰ã—ãé©ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚ç›¸äº’ä½œç”¨å±¥æ­´ _ğ‘¥_ â‰¤ _ğ‘¡_ ä¸Šã®è¡Œå‹•äº‹å‰åˆ†å¸ƒã¾ãŸã¯ Self-Model ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ã Sequence Model _ğ‘ğœ™_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) ã‚’å°å…¥ã™ã‚‹ã€‚è¡Œå‹•æ–¹ç­– _ğœ‹_ ã¨äº‹å‰åˆ†å¸ƒ _ğ‘ğœ™_ é–“ã® KL Divergence ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™ã“ã¨ã§ä»£ç†ç›®çš„é–¢æ•° _ğ½_ ã‚’å®šç¾©ã™ã‚‹ï¼š

_ğ½_(_ğœ‹_, _ğœ™_) = ğ”¼_ğœ_âˆ¼â„™_ğœ‹_ [Î£_ğ‘¡_=0^_ğ‘‡_ _ğ›¾_^_ğ‘¡_ _ğ‘Ÿğ‘¡_ âˆ’ _ğ›¼_ KL(_ğœ‹_(Â· | _ğ‘¥_ â‰¤ _ğ‘¡_) || _ğ‘ğœ™_(Â· | _ğ‘¥_ â‰¤ _ğ‘¡_))]. (7)

KL(Â·||Â·) â‰¥ 0 ã§ã‚ã‚‹ãŸã‚ã€_ğ½_(_ğœ‹_, _ğœ™_) ã¯ _ğ‘‰_(_ğœ‹_) ã®å³å¯†ãªä¸‹ç•Œã§ã‚ã‚Šã€_ğœ‹_ = _ğ‘ğœ™_ ã§ç­‰å·ãŒæˆã‚Šç«‹ã¤ã€‚ã“ã®ä¸‹ç•Œã‚’ _ğœ‹_ï¼ˆéãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ–¹ç­–ï¼‰ã¨ _ğœ™_ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ Sequence Modelï¼‰ã«é–¢ã™ã‚‹åº§æ¨™ä¸Šæ˜‡æ³•ã§æœ€é©åŒ–ã™ã‚‹ã€‚

**C.2.** **Step 1: _ğœ‹_ ã«é–¢ã™ã‚‹éãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ Policy Improvement**

å›ºå®š _ğœ™_ ã«å¯¾ã—ã¦ _ğ½_(_ğœ‹_, _ğœ™_) ã‚’ _ğœ‹_ ã«ã¤ã„ã¦æœ€é©åŒ–ã™ã‚‹ã“ã¨ã¯å®Œå…¨ãªæœ€é©åˆ¶å¾¡å•é¡Œã§ã‚ã‚Šã€ä¸€èˆ¬ã«è§£æè§£ã‚’æŒãŸãªã„ãŸã‚ã€ç›´æ¥çš„ãªéãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ Policy Improvement ã«ã¯é©ã•ãªã„ã€‚ä»£ã‚ã‚Šã«ã€_ğœ‹_ = _ğ‘ğœ™ğ‘˜_ å‘¨ã‚Šã® _ğ½_(_ğœ‹_, _ğœ™ğ‘˜_) ã®ä¸€æ¬¡è¿‘ä¼¼ã‚’ä½¿ç”¨ã™ã‚‹ã€‚ã“ã“ã§ _ğ‘ğœ™ğ‘˜_ ã¯å‰å›ã®æ–¹ç­– _ğœ‹ğ‘˜_âˆ’1 ã®å±•é–‹ã«ã‚ˆã‚Šåé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚ŒãŸ Self-Model ã§ã‚ã‚‹ï¼š

_ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_) = Î£_ğ‘¡_=1^_ğ‘‡_ ğ”¼_ğ‘¥_ â‰¤ _ğ‘¡_âˆ¼â„™_ğ‘ğœ™ğ‘˜_ [ğ”¼_ğ‘_âˆ¼_ğœ‹_(Â·|_ğ‘¥_ â‰¤ _ğ‘¡_) [_ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_)] âˆ’ _ğ›¼_ KL(_ğœ‹_(Â· | _ğ‘¥_ â‰¤ _ğ‘¡_) || _ğ‘ğœ™ğ‘˜_(Â· | _ğ‘¥_ â‰¤ _ğ‘¡_)) âˆ’ _ğ‘‰_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_)] + _ğ½_(_ğ‘ğœ™ğ‘˜_, _ğœ™ğ‘˜_). (8)

ã“ã“ã§ Q å€¤ã¯éæ­£å‰‡åŒ–ã•ã‚ŒãŸå€¤ _ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) = ğ”¼_ğœ_>_ğ‘¡_âˆ¼â„™_ğ‘ğœ™ğ‘˜_(Â·|_ğ‘¥_ â‰¤ _ğ‘¡_,_ğ‘_) [Î£_ğ‘¡'_=_ğ‘¡_^_ğ‘‡_ _ğ›¾_^(_ğ‘¡'_âˆ’_ğ‘¡_) _ğ‘Ÿğ‘¡'_] ã«ç­‰ã—ã„ã€‚ã“ã‚Œã¯å…¨ã¦ã® KL é …ãŒäº‹å‰åˆ†å¸ƒã®ä¸‹ã§ã‚¼ãƒ­ã«è©•ä¾¡ã•ã‚Œã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚_ğ½_ ã¨ _ğ½Ì„_ ã®æ±ºå®šçš„ãªé•ã„ã¯ã€_ğ½Ì„_ ã«ãŠã‘ã‚‹å±¥æ­´ã«é–¢ã™ã‚‹æœŸå¾…å€¤ãŒæœ€é©åŒ–ã•ã‚Œã‚‹æ–¹ç­– _ğœ‹_ ã«ä¾å­˜ã—ãªã„ã“ã¨ã§ã‚ã‚Šã€arg max_ğœ‹_ _ğ½Ì„_ ã®é–‰å½¢å¼è§£ã‚’è¨±å®¹ã™ã‚‹ã€‚

_ğ½Ì„_ ãŒç¢ºã‹ã« _ğ‘ğœ™ğ‘˜_ å‘¨ã‚Šã® _ğ½_ ã®ä¸€æ¬¡è¿‘ä¼¼ã§ã‚ã‚‹ã“ã¨ã‚’ã€ä»¥ä¸‹ã®2ã¤ã®è£œé¡Œã‚’é€šã˜ã¦ç¤ºã™ã€‚

**è£œé¡Œ C.1.** _ğ½Ì„_(_ğ‘ğœ™ğ‘˜_, _ğœ™ğ‘˜_) = _ğ½_(_ğ‘ğœ™ğ‘˜_, _ğœ™ğ‘˜_)

_è¨¼æ˜._ _ğœ‹_ = _ğ‘ğœ™ğ‘˜_ ã®ã¨ãã€å¼8ã®æœŸå¾…å€¤å†…ã®é …ãŒç›¸æ®ºã•ã‚Œã‚‹ã“ã¨ã¯å®¹æ˜“ã«ç¢ºèªã§ãã€_ğ½_(_ğ‘ğœ™ğ‘˜_, _ğœ™ğ‘˜_) ã®ã¿ãŒæ®‹ã‚‹ã€‚

**è£œé¡Œ C.2.** âˆ‡_ğœ‹_ _ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_)|_ğœ‹_=_ğ‘ğœ™ğ‘˜_ = âˆ‡_ğœ‹_ _ğ½_(_ğœ‹_, _ğœ™ğ‘˜_)|_ğœ‹_=_ğ‘ğœ™ğ‘˜_

_è¨¼æ˜._ æ–¹ç­–åˆ†å¸ƒ _ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) ã«é–¢ã™ã‚‹ä¸¡ç›®çš„é–¢æ•°ã®æ±é–¢æ•°å¾®åˆ†ã‚’ã€ç‰¹å®šã®å±¥æ­´ _ğ‘¥_ â‰¤ _ğ‘¡_ ã¨è¡Œå‹• _ğ‘_ ã§è©•ä¾¡ã—ã¦åˆ†æã™ã‚‹ã€‚

ã¾ãšä»£ç†ç›®çš„é–¢æ•° _ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_) ã‚’è€ƒãˆã‚‹ã€‚å±¥æ­´ã«é–¢ã™ã‚‹æœŸå¾…å€¤ã¯äº‹å‰åˆ†å¸ƒ â„™_ğ‘ğœ™ğ‘˜_ ã«å›ºå®šã•ã‚Œã¦ãŠã‚Šã€æœ€é©åŒ–å¤‰æ•° _ğœ‹_ ã«ä¾å­˜ã—ãªã„ãŸã‚ã€æ±é–¢æ•°å¾®åˆ†ã¯ç›´æ¥çš„ã§ã‚ã‚‹ã€‚å¯¾æ•°é …ã«ç©ã®æ³•å‰‡ã‚’é©ç”¨ã™ã‚‹ã¨ã€å±€æ‰€çš„ãªè¡Œå‹•ç¢ºç‡ _ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) ã«é–¢ã™ã‚‹æ±é–¢æ•°å¾®åˆ†ã¯ï¼š

_ğ›¿ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_) / _ğ›¿ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) = â„™_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_) [_ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) âˆ’ _ğ›¼_ log(_ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) / _ğ‘ğœ™ğ‘˜_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_)) âˆ’ _ğ›¼_]. (9)

ã“ã®å¾®åˆ†ã‚’äº‹å‰åˆ†å¸ƒ _ğœ‹_ = _ğ‘ğœ™ğ‘˜_ ã§è©•ä¾¡ã™ã‚‹ã¨ã€å¯¾æ•°é …ã¯æ¶ˆå¤±ã—ï¼ˆlog 1 = 0 ã§ã‚ã‚‹ãŸã‚ï¼‰ï¼š

_ğ›¿ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_) / _ğ›¿ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_)|_ğœ‹_=_ğ‘ğœ™ğ‘˜_ = â„™_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_)(_ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) âˆ’ _ğ›¼_). (10)

æ¬¡ã«ã€çœŸã®ç›®çš„é–¢æ•° _ğ½_(_ğœ‹_, _ğœ™ğ‘˜_) ã®å¾®åˆ†ã¯ã‚ˆã‚Šè¤‡é›‘ã§ã‚ã‚‹ã€‚_ğœ‹_ ãŒå±¥æ­´è¨ªå•åˆ†å¸ƒ â„™_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚æ­£å‰‡åŒ– Q é–¢æ•° _ğ‘„_^_ğœ‹__reg_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) ã‚’å®šç¾©ã™ã‚‹ã€‚ã“ã‚Œã¯æ™‚åˆ» _ğ‘¡_ ã®å³æ™‚ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é™¤ãå…¨ã¦ã®_å°†æ¥ã®_ KL ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’å«ã‚€æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‹ï¼š

_ğ‘„_^_ğœ‹__reg_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) = ğ”¼_ğœ_>_ğ‘¡_âˆ¼â„™_ğœ‹_(Â·|_ğ‘¥_ â‰¤ _ğ‘¡_,_ğ‘_) [Î£_ğ‘˜_=_ğ‘¡_^_ğ‘‡_ _ğ›¾_^(_ğ‘˜_âˆ’_ğ‘¡_) _ğ‘…ğ‘˜_ âˆ’ _ğ›¼_ Î£_ğ‘˜_=_ğ‘¡_+1^_ğ‘‡_ _ğ›¾_^(_ğ‘˜_âˆ’_ğ‘¡_) KL(_ğœ‹_(Â· | _ğ‘¥_ â‰¤ _ğ‘˜_) || _ğ‘ğœ™ğ‘˜_(Â· | _ğ‘¥_ â‰¤ _ğ‘˜_))]. (11)

ã“ã‚Œã‚’ç”¨ã„ã¦ã€ç‰¹å®šã®å±¥æ­´ã®ä¾¡å€¤ã¯ï¼š

_ğ‘‰_^_ğœ‹__reg_(_ğ‘¥_ â‰¤ _ğ‘¡_) = Î£_ğ‘'_ _ğœ‹_(_ğ‘'_ | _ğ‘¥_ â‰¤ _ğ‘¡_) [_ğ‘„_^_ğœ‹__reg_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘'_) âˆ’ _ğ›¼_ log(_ğœ‹_(_ğ‘'_ | _ğ‘¥_ â‰¤ _ğ‘¡_) / _ğ‘ğœ™ğ‘˜_(_ğ‘'_ | _ğ‘¥_ â‰¤ _ğ‘¡_))]. (12)

ã‚°ãƒ­ãƒ¼ãƒãƒ«ç›®çš„é–¢æ•° _ğ½_ ã®å±€æ‰€æ–¹ç­– _ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) ã«é–¢ã™ã‚‹æ±é–¢æ•°å¾®åˆ†ã‚’æ±‚ã‚ã‚‹ãŸã‚ã€Performance Difference Lemma ã®é€£ç¶šæ‹¡å¼µ (Kakade & Langford, 2002) ã‚’é©ç”¨ã™ã‚‹ã€‚ã“ã®å®šç†ã¯ã€æ–¹ç­–ãŒè¨ªå•åˆ†å¸ƒ â„™_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) ã«ä¸ãˆã‚‹é–“æ¥çš„åŠ¹æœãŒå‹¾é…ã«å¯¾ã—ã¦ã‚¼ãƒ­ã®æ­£å‘³å¯„ä¸ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ç¢ºç«‹ã™ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€å¾®åˆ†ã¯çŠ¶æ…‹è¨ªå•ç¢ºç‡ã¨ä¾¡å€¤é–¢æ•°ã®å±€æ‰€çš„å¾®åˆ†ã®ç©ã«åˆ†é›¢ã•ã‚Œã‚‹ï¼š

_ğ›¿ğ½_(_ğœ‹_, _ğœ™ğ‘˜_) / _ğ›¿ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) = â„™_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) _ğœ•ğ‘‰_^_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) / _ğœ•ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_). (13)

_ğ‘‰_^_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) ã®åå¾®åˆ†ã‚’å–ã‚‹ã¨ï¼š

_ğ›¿ğ½_(_ğœ‹_, _ğœ™ğ‘˜_) / _ğ›¿ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) = â„™_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) [_ğ‘„_^_ğœ‹__reg_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) âˆ’ _ğ›¼_ log(_ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) / _ğ‘ğœ™ğ‘˜_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_)) âˆ’ _ğ›¼_]. (14)

æœ€å¾Œã«ã€ã“ã®çœŸã®å¾®åˆ†ã‚’äº‹å‰æ–¹ç­– _ğœ‹_ = _ğ‘ğœ™ğ‘˜_ ã§è©•ä¾¡ã™ã‚‹ã€‚3ã¤ã®ç°¡ç•¥åŒ–ãŒç”Ÿã˜ã‚‹ï¼š

- å±¥æ­´è¨ªå•åˆ†å¸ƒãŒäº‹å‰åˆ†å¸ƒã«ä¸€è‡´ã™ã‚‹ï¼šâ„™_ğœ‹_(_ğ‘¥_ â‰¤ _ğ‘¡_) = â„™_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_)
- å³æ™‚ KL ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒã‚¼ãƒ­ã«è©•ä¾¡ã•ã‚Œã‚‹ï¼šlog 1 = 0
- æ–¹ç­–ãŒå…¨ã¦ã®å°†æ¥ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§äº‹å‰åˆ†å¸ƒã«å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ãŸã‚ã€å…¨ã¦ã®å°†æ¥ã® KL ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒã‚¼ãƒ­ã«è©•ä¾¡ã•ã‚Œã‚‹ã€‚ã—ãŸãŒã£ã¦ã€æ­£å‰‡åŒ– Q é–¢æ•°ã¯äº‹å‰åˆ†å¸ƒã®éæ­£å‰‡åŒ– Q é–¢æ•°ã«æ»‘ã‚‰ã‹ã«ç¸®é€€ã™ã‚‹ï¼š_ğ‘„_^_ğœ‹__reg_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) = _ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_)

ã“ã‚Œã‚‰ã®ç°¡ç•¥åŒ–ã‚’é©ç”¨ã™ã‚‹ã¨ï¼š

_ğ›¿ğ½_(_ğœ‹_, _ğœ™ğ‘˜_) / _ğ›¿ğœ‹_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_)|_ğœ‹_=_ğ‘ğœ™ğ‘˜_ = â„™_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_)(_ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) âˆ’ _ğ›¼_). (15)

_ğ½_ ã¨ _ğ½Ì„_ ã®ä¸¡æ–¹ã® _ğœ‹_ = _ğ‘ğœ™ğ‘˜_ ã§è©•ä¾¡ã•ã‚ŒãŸæ±é–¢æ•°å¾®åˆ†ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹ãŸã‚ã€âˆ‡_ğœ‹_ _ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_)|_ğœ‹_=_ğ‘ğœ™ğ‘˜_ = âˆ‡_ğœ‹_ _ğ½_(_ğœ‹_, _ğœ™ğ‘˜_)|_ğœ‹_=_ğ‘ğœ™ğ‘˜_ ãŒæˆã‚Šç«‹ã¡ã€è¨¼æ˜ãŒå®Œäº†ã™ã‚‹ã€‚

**_ğ½Ì„_ ã®æœ€é©åŒ–ã€‚** å›ºå®š _ğœ™ğ‘˜_ ã«å¯¾ã—ã¦ _ğ½Ì„_(_ğœ‹_, _ğœ™ğ‘˜_) ã‚’ _ğœ‹_ ã«ã¤ã„ã¦æœ€é©åŒ–ã™ã‚‹ã¨ã€ã‚ˆãçŸ¥ã‚‰ã‚ŒãŸ Boltzmann æ–¹ç­–ãŒè§£ã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ï¼š

_ğœ‹_*(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) = _ğ‘ğœ™ğ‘˜_(_ğ‘_ | _ğ‘¥_ â‰¤ _ğ‘¡_) exp(_ğ›½ğ‘„_^_ğ‘ğœ™ğ‘˜_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_)) / _ğ‘_(_ğ‘¥_ â‰¤ _ğ‘¡_), (16)

é€†æ¸©åº¦ _ğ›½_ = 1/_ğ›¼_ ã§ã‚ã‚‹ã€‚_ğ›½_ ã‚’ _ğ‘ğœ™ğ‘˜_ å‘¨ã‚Šã® _ğ½Ì„_ ãŒ _ğ½_ ã®ååˆ†ã«æ­£ç¢ºãªè¿‘ä¼¼ã¨ãªã‚‹ Trust Region ã‚’å®šç¾©ã™ã‚‹å›ºå®šãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†ã€‚

**C.3.** **MPO ãŠã‚ˆã³ Sequence-Model å€¤æ¨å®šã¨ã®æ¯”è¼ƒ**

PPI ã¯ MPO ã®åº§æ¨™ä¸Šæ˜‡æ§‹é€ ã‚’å…±æœ‰ã™ã‚‹ãŒã€Q å€¤ã®å–å¾—æ–¹æ³•ã¨ã€è»Œé“åé›†ã®ãŸã‚ã®è¡Œå‹•æ–¹ç­–ã¨ã—ã¦ _ğœ‹_ ã¨ _ğ‘ğœ™ğ‘˜_ ã®ã©ã¡ã‚‰ã‚’å±•é–‹ã™ã‚‹ã‹ãŒç•°ãªã‚‹ã€‚æ¨™æº–çš„ãª MPO ã§ã¯ã€_ğ‘„_(_ğ‘ _, _ğ‘_) ã¯é€šå¸¸ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè‡ªèº«ã®çµŒé¨“ã«å¯¾ã™ã‚‹ Temporal Difference (TD) å­¦ç¿’ã«ã‚ˆã‚Šè¨“ç·´ã•ã‚Œã‚‹å€‹åˆ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCriticï¼‰ã§è¡¨ç¾ã•ã‚Œã€å®Œå…¨ãªå±¥æ­´ã§ã¯ãªãå˜ä¸€ã®çŠ¶æ…‹ _ğ‘ _ ã«æ¡ä»¶ä»˜ã‘ã‚‹ãŸã‚ã«ãƒãƒ«ã‚³ãƒ•æ€§ã«ä¾å­˜ã™ã‚‹ã€‚

å¯¾ç…§çš„ã«ã€PPI ã¯ Sequence Model ã‚’ World Model ã¨ã—ã¦æ´»ç”¨ã™ã‚‹ã€‚å€¤ _ğ‘„Ì‚_ _ğ‘_(_ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) ã¯ Sequence Model è‡ªä½“ã®å†…éƒ¨ã§å®Ÿè¡Œã•ã‚Œã‚‹ Monte Carlo Rollout ã«ã‚ˆã‚Šæ¨å®šã•ã‚Œã‚‹ã€‚_ğ‘ğœ™_(Â· | _ğ‘¥_ â‰¤ _ğ‘¡_, _ğ‘_) ã‹ã‚‰å°†æ¥ã®è»Œé“ _ğœ_>_ğ‘¡_ ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç’°å¢ƒãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¨ co-player ã®äºˆæ¸¬å¿œç­”ã®ä¸¡æ–¹ã«é–¢ã™ã‚‹å†…éƒ¨è¡¨ç¾ã«åŸºã¥ã„ã¦è¡Œå‹•ã®æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’è©•ä¾¡ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Š PPI ã¯ Sequence Model ã«ã‚ˆã‚Šæ•æ‰ã•ã‚Œã‚‹é«˜å®¹é‡ã®æ™‚é–“çš„ä¾å­˜æ€§ã®æ©æµã‚’å—ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚PPI ã¯ MC Rollout ã®ã‚³ã‚¹ãƒˆã‚’å„Ÿå´ã—åˆ†æ•£ã‚’ä½æ¸›ã™ã‚‹ãŸã‚ã«ã€å®Œå…¨ãªå±¥æ­´ã«æ¡ä»¶ä»˜ã‘ã‚‰ã‚ŒãŸæ˜ç¤ºçš„ãª Q å€¤é–¢æ•°ã®å­¦ç¿’ã«å®¹æ˜“ã«æ‹¡å¼µå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã•ã‚ŒãŸã„ã€‚

### **D. PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡è¡¡è¡Œå‹•ã®ç†è«–çš„åˆ†æ**

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€Predictive Policy Improvement (PPI) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è«–çš„æ€§è³ªã‚’åˆ†æã™ã‚‹ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå›ºå®šï¼ˆã¾ãŸã¯å®šå¸¸çš„ã«é©å¿œã™ã‚‹ï¼‰ç’°å¢ƒã«å¯¾ã—ã¦æ–¹ç­–ã‚’æœ€é©åŒ–ã™ã‚‹æ¨™æº–çš„ãªå¼·åŒ–å­¦ç¿’ã¨ã¯ç•°ãªã‚Šã€PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ _performative_ ãƒ«ãƒ¼ãƒ—ã§å‹•ä½œã™ã‚‹ï¼šã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒãã®æ–¹ç­–ã‚’æ±ºå®šã—ã€æ–¹ç­–ãŒãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’æ±ºå®šã—ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯ã€ŒPerformative Predictionã€(Perdomo et al., 2020) ã®æ¦‚å¿µã¨å¯†æ¥ã«é–¢é€£ã—ã¦ãŠã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒãã®ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã¾ã•ã«ãã®ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã«å½±éŸ¿ã‚’ä¸ãˆã†ã‚‹ï¼ˆäº¤é€šäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒé¡•è‘—ãªä¾‹ã§ã‚ã‚‹ï¼‰ã€‚

ã“ã®ç›¸äº’ä½œç”¨ã‚’å½¢å¼åŒ–ã—ã€_Predictive Equilibrium_ (PE) ã®æ¦‚å¿µã‚’å®šç¾©ã™ã‚‹ã€‚Deep Neural Network ã®éå‡¸æ€§ã®ãŸã‚ã«ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªç´”ç²‹æˆ¦ç•¥å‡è¡¡ã®å­˜åœ¨ã¯ä¿è¨¼ã•ã‚Œãªã„ãŒã€æ¨™æº–çš„ãªä»®å®šã®ä¸‹ã§ _å±€æ‰€çš„_ ãª Predictive Equilibriumï¼ˆå‹¾é…ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ã¨æ•´åˆçš„ï¼‰ãŠã‚ˆã³ _æ··åˆ_ Predictive Equilibriumï¼ˆãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸæˆ¦ç•¥ï¼‰ã®å­˜åœ¨ãŒä¿è¨¼ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚æœ€å¾Œã«ã€å®Œå…¨ãª World Model ã®æ¥µé™ã§ã€Predictive Equilibrium ãŒ Subjective Embedded Equilibrium (Meulemans et al., 2025b) ã«å¯¾å¿œã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

**D.1.** **å½¢å¼çš„è¨­å®š**

_ğ‘›_ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚²ãƒ¼ãƒ ã‚’è€ƒãˆã‚‹ã€‚å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã¯ _ğœƒğ‘–_ âˆˆ Î˜_ğ‘–_ ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸäºˆæ¸¬çš„ Sequence Model _ğ‘ğœƒğ‘–_(_â„_ _[ğ‘–]_) ã‚’ç¶­æŒã™ã‚‹ã€‚ã“ã“ã§ _â„_ _[ğ‘–]_ ã¯ä»»æ„ã®é•·ã• _ğ‘¡_ ã®å±¥æ­´ _ğ‘¥_ â‰¤ _[ğ‘–]_ _ğ‘¡_ ã§ã‚ã‚Šã€Î˜_ğ‘–_ ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè·é›¢ç©ºé–“ï¼ˆä¾‹ï¼šâ„^_ğ‘‘_ ã®æœ‰ç•Œéƒ¨åˆ†é›†åˆï¼‰ã§ã‚ã‚‹ã€‚

**Performative ãƒ«ãƒ¼ãƒ—ã€‚** PPI ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆAlgorithm 1ï¼‰ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ‡ãƒ¼ã‚¿ã®é–“ã«é–‰ãƒ«ãƒ¼ãƒ—ä¾å­˜æ€§ã‚’èª˜å°ã™ã‚‹ï¼š

1. **ãƒ¢ãƒ‡ãƒ«ãŒæ–¹ç­–ã‚’èª˜å°ã™ã‚‹ï¼š** ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å¼6ã§å®šç¾©ã•ã‚ŒãŸ Policy Improvement æ¼”ç®—å­ï¼ˆQ å€¤ã«å¯¾ã™ã‚‹ Boltzmann æ–¹ç­–ï¼‰ã‚’é€šã˜ã¦ãƒ¢ãƒ‡ãƒ« _ğ‘ğœƒğ‘–_ ã‹ã‚‰æ–¹ç­– _ğœ‹ğœƒğ‘–_ ã‚’å°å‡ºã™ã‚‹ã€‚
2. **æ–¹ç­–ãŒãƒ‡ãƒ¼ã‚¿ã‚’èª˜å°ã™ã‚‹ï¼š** å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ–¹ç­– **ğ…ğœ½** = {_ğœ‹ğœƒ_1, ..., _ğœ‹ğœƒ_N} ã‚’ç”¨ã„ã¦ç›¸äº’ä½œç”¨ã™ã‚‹ã¨ã€ç›¸äº’ä½œç”¨å±¥æ­´ _â„_ ä¸Šã®å…±åŒåˆ†å¸ƒãŒèª˜å°ã•ã‚Œã‚‹ã€‚ç¾åœ¨ã®å…±åŒè¨­å®š **ğœ½** ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚Œã‚‹å±¥æ­´ã®çœŸã®ç¢ºç‡åˆ†å¸ƒã‚’ â„™(Â·; **ğœ½**) ã¨è¡¨è¨˜ã™ã‚‹ã€‚
3. **ãƒ‡ãƒ¼ã‚¿ãŒãƒ¢ãƒ‡ãƒ«ã‚’èª˜å°ã™ã‚‹ï¼š** ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯è¦³æ¸¬ã•ã‚ŒãŸåˆ†å¸ƒ â„™(Â·; **ğœ½**) ã¨ãã®ãƒ¢ãƒ‡ãƒ« _ğ‘ğœƒğ‘–_ é–“ã® KL Divergence ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã† _ğœƒğ‘–_ ã‚’æ›´æ–°ã™ã‚‹ã€‚

**D.2.** **Predictive Equilibria**

ã“ã®è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å®‰å®šç‚¹ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã¾ã•ã«ãã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å°å‡ºã•ã‚ŒãŸæ–¹ç­–ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©ã«äºˆæ¸¬ã™ã‚‹ã‚ˆã†ãªè¨­å®šã§ã‚ã‚‹ã€‚

**å®šç¾© D.1** (Global Predictive Equilibrium). å…±åŒè¨­å®š **ğœ½*** = (_ğœƒ_1*, ..., _ğœƒ_n*) ã¯ã€å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã«ã¤ã„ã¦ä»¥ä¸‹ãŒæˆã‚Šç«‹ã¤ã¨ã Global Predictive Equilibrium ã§ã‚ã‚‹ï¼š

_ğœƒğ‘–_* âˆˆ arg min_ğœƒğ‘–_ âˆˆ Î˜_ğ‘–_ KL(â„™(_â„ğ‘–_; **ğœ½***) || _ğ‘ğœƒğ‘–_(_â„ğ‘–_)). (17)

ç›´æ„Ÿçš„ã«ã¯ã€å‡è¡¡ã«ãŠã„ã¦ã©ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚‚ç¾åœ¨ã®å…±åŒãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šèª˜å°ã•ã‚Œã‚‹è¡Œå‹•ã‚’å‰æã¨ã—ã¦è‡ªèº«ã® World Model ã‚’æ”¹å–„ã§ããªã„ã€‚

**èª²é¡Œã€‚** ã‚°ãƒ­ãƒ¼ãƒãƒ« PE ã®å­˜åœ¨ã‚’è¨¼æ˜ã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã‚ã‚‹ã€‚å†™åƒ _ğœƒ_ â†¦ _ğœ‹ğœƒ_ ãŒè¤‡é›‘ã§ã‚ã‚Šã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹ç›®çš„é–¢æ•°ãŒä¸€èˆ¬ã«éå‡¸ã§ã‚ã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚"argmin" é›†åˆãŒä¸é€£ç¶šã«å¤‰åŒ–ã—ã†ã‚‹ï¼ˆMode Hoppingï¼‰ãŸã‚ã€æ¨™æº–çš„ãªä¸å‹•ç‚¹å®šç†ã®é©ç”¨ãŒå¦¨ã’ã‚‰ã‚Œã‚‹ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€2ã¤ã®ç·©å’Œã•ã‚ŒãŸè§£æ¦‚å¿µã‚’å®šç¾©ã™ã‚‹ï¼š_å±€æ‰€_ PEï¼ˆå‹¾é…é™ä¸‹æ³•ã«é–¢é€£ï¼‰ã¨ _æ··åˆ_ PEã€‚

_**D.2.1.**_ _**Local Predictive Equilibrium**_

å®Ÿéš›ã«ã¯ã€PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å‹¾é…é™ä¸‹æ³•ã«ã‚ˆã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€‚ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€å°å€¤ã§ã¯ãªãåœç•™ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹ã€‚é‡è¦ãªã®ã¯ã€æ›´æ–°ãŒãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’å›ºå®šã—ã¦ã„ã‚‹ã¨ä»®å®šã™ã‚‹ã“ã¨ï¼ˆç’°å¢ƒãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«å¯¾ã™ã‚‹ "stop-gradient" ã¨è§£é‡ˆã§ãã‚‹ï¼‰ã§ã‚ã‚‹ã€‚

**å®šç¾© D.2** (Local Predictive Equilibrium). Î˜_ğ‘–_ âŠ‚ â„^_ğ‘‘ğ‘–_ ã‚’å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ âˆˆ I ã®ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå‡¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã¨ã™ã‚‹ã€‚å…±åŒè¨­å®š **ğœ½*** = (_ğœƒ_1*, ..., _ğœƒ_n*) âˆˆ Ã—_ğ‘–_ âˆˆI Î˜_ğ‘–_ ã¯ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆéç¨‹ã‚’å›ºå®šã¨ã—ãŸä¸Šã§ã€å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ âˆˆ I ã«ã¤ã„ã¦å±€æ‰€æå¤±ã«é–¢ã™ã‚‹ä¸€æ¬¡åœç•™æ¡ä»¶ã‚’æº€ãŸã™ã¨ã Local Predictive Equilibrium ã§ã‚ã‚‹ã€‚å½¢å¼çš„ã«ã¯ï¼š

âŸ¨âˆ‡_ğœƒğ‘–_ KL(â„™(_â„ğ‘–_; **ğœ½***) || _ğ‘ğœƒğ‘–_(_â„ğ‘–_))|_ğœƒğ‘–_=_ğœƒğ‘–_*, _ğœ™ğ‘–_ âˆ’ _ğœƒğ‘–_*âŸ© â‰¥ 0, âˆ€_ğœ™ğ‘–_ âˆˆ Î˜_ğ‘–_, âˆ€_ğ‘–_ âˆˆ I, (18)

ã“ã“ã§ âŸ¨Â·, Â·âŸ© ã¯æ¨™æº–å†…ç©ã‚’è¡¨ã™ã€‚

ã“ã®å¤‰åˆ†ä¸ç­‰å¼ã®å®šç¾©ã¯ã€PPI ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãŠã‘ã‚‹å°„å½±å‹¾é…é™ä¸‹æ³•ã®åæŸåŸºæº–ã«æ­£ç¢ºã«å¯¾å¿œã™ã‚‹ã€‚_ğœƒğ‘–_* ãŒ Î˜_ğ‘–_ ã®å†…éƒ¨ã«ã‚ã‚‹å ´åˆã€å¼18ã¯æ¨™æº–çš„ãªæ¡ä»¶ âˆ‡_ğœƒğ‘–_ KL(â„™(_â„ğ‘–_; **ğœ½***) || _ğ‘ğœƒğ‘–_(_â„ğ‘–_))|_ğœƒğ‘–_=_ğœƒğ‘–_* = 0 ã‚’å«æ„ã™ã‚‹ (19)ã€‚

**å®šç† D.3** (Local Predictive Equilibrium ã®å­˜åœ¨). _Î˜_ğ‘–_ ãŒâ„^_ğ‘‘ğ‘–_ ã®ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå‡¸éƒ¨åˆ†é›†åˆã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ **ğœ½** ã‹ã‚‰æå¤±ã®å±€æ‰€å‹¾é… _ğºğ‘–_(**ğœ½**) = âˆ‡_ğœ—_ KL(â„™(Â·; **ğœ½**) || _ğ‘ğœ—_)|_ğœ—_=_ğœƒğ‘–_ ã¸ã®å†™åƒãŒé€£ç¶šã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚ã“ã®ã¨ãã€å°‘ãªãã¨ã‚‚1ã¤ã® Local Predictive Equilibrium ãŒå­˜åœ¨ã™ã‚‹ã€‚_

_è¨¼æ˜._ Local Predictive Equilibrium ã®å­˜åœ¨ã‚’ä¸å‹•ç‚¹å•é¡Œã¨ã—ã¦å®šå¼åŒ–ã—ã¦åˆ†æã™ã‚‹ã€‚L_ğ‘–_(**ğœ½**, _ğœ“_) = KL(â„™(_â„ğ‘–_; **ğœ½**) || _ğ‘ğœ“_(_â„ğ‘–_)) ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã®æå¤±é–¢æ•°ã¨ã™ã‚‹ã€‚ã“ã“ã§ç¬¬ä¸€å¼•æ•° **ğœ½** ã¯ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’æ±ºå®šã—ï¼ˆå±€æ‰€çš„ã«å›ºå®šï¼‰ã€ç¬¬äºŒå¼•æ•° _ğœ“_ ã¯æœ€é©åŒ–ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚_å±€æ‰€å‹¾é…å ´ ğº_: Î˜ â†’ â„^_ğ·_ï¼ˆã“ã“ã§ _ğ·_ = Î£_ğ‘–_ âˆˆI _ğ‘‘ğ‘–_ï¼‰ã‚’å€‹åˆ¥å‹¾é…ã®é€£çµã¨ã—ã¦å®šç¾©ã™ã‚‹ï¼š

_ğº_(**ğœ½**) = (âˆ‡_ğœ“_ L1(**ğœ½**, _ğœ“_)|_ğœ“_=_ğœƒ_1, ..., âˆ‡_ğœ“_ Ln(**ğœ½**, _ğœ“_)|_ğœ“_=_ğœƒ_n).

Local Predictive Equilibrium ã¯å¤‰åˆ†ä¸ç­‰å¼ âŸ¨_ğº_(**ğœ½***), _ğœ™_ âˆ’ **ğœ½***âŸ© â‰¥ 0ï¼ˆå…¨ _ğœ™_ âˆˆ Î˜ ã«å¯¾ã—ã¦ï¼‰ã§ç‰¹å¾´ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚ã“ã“ã§ Î˜ = Ã—_ğ‘–_ âˆˆI Î˜_ğ‘–_ã€‚

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ Î˜ ã¯ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“ã®ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå‡¸éƒ¨åˆ†é›†åˆã§ã‚ã‚Šã€å‹¾é…å ´ _ğº_(**ğœ½**) ã¯é€£ç¶šã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚_ğº_ ã®é€£ç¶šæ€§ã¯äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« _ğ‘ğœƒ_ ã¨èª˜å°ã•ã‚Œã‚‹æ–¹ç­–åˆ†å¸ƒã«é–¢ã™ã‚‹æ»‘ã‚‰ã‹ã•ã®ä»®å®šã‹ã‚‰è‡ªç„¶ã«å°ã‹ã‚Œã‚‹ã€‚

å°„å½±å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚ˆã‚Šå®šç¾©ã•ã‚Œã‚‹å†™åƒ _ğ‘‡_: Î˜ â†’ Î˜ ã‚’è€ƒãˆã‚‹ï¼š

_ğ‘‡_(**ğœ½**) = ProjÎ˜(**ğœ½** âˆ’ _ğœ‚ğº_(**ğœ½**)),

ã“ã“ã§ _ğœ‚_ > 0 ã¯ã‚¹ã‚«ãƒ©ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã§ã‚ã‚Šã€ProjÎ˜ ã¯é›†åˆ Î˜ ã¸ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰å°„å½±ã§ã‚ã‚‹ã€‚

1. **ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆæ€§ã¨å‡¸æ€§ï¼š** ä»®å®šã«ã‚ˆã‚Š Î˜ ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå‡¸é›†åˆã§ã‚ã‚‹ã€‚
2. **é€£ç¶šæ€§ï¼š** å†™åƒ _ğº_ ã¯ä»®å®šã«ã‚ˆã‚Šé€£ç¶šã€‚å°„å½±æ¼”ç®—å­ ProjÎ˜ ã¯éæ‹¡å¤§çš„ã§ã‚ã‚Šã€ã—ãŸãŒã£ã¦é€£ç¶šã€‚ã‚ˆã£ã¦åˆæˆ _ğ‘‡_ ã¯ Î˜ ã‹ã‚‰è‡ªèº«ã¸ã®é€£ç¶šå†™åƒã§ã‚ã‚‹ã€‚

Brouwer ã®ä¸å‹•ç‚¹å®šç†ã«ã‚ˆã‚Šã€_ğ‘‡_(**ğœ½***) = **ğœ½*** ã‚’æº€ãŸã™ç‚¹ **ğœ½*** âˆˆ Î˜ ãŒå­˜åœ¨ã™ã‚‹ã€‚ã“ã®ä¸å‹•ç‚¹æ¡ä»¶ã¯ **ğœ½*** = ProjÎ˜(**ğœ½*** âˆ’ _ğœ‚ğº_(**ğœ½***)) ã‚’å«æ„ã™ã‚‹ã€‚

é–‰å‡¸é›†åˆã¸ã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰å°„å½±ã®æ¨™æº–çš„æ€§è³ªã«ã‚ˆã‚Šã€ã“ã®ç­‰å¼ã¯ âŸ¨(**ğœ½*** âˆ’ _ğœ‚ğº_(**ğœ½***)) âˆ’ **ğœ½***, _ğœ™_ âˆ’ **ğœ½***âŸ© â‰¤ 0ï¼ˆå…¨ _ğœ™_ âˆˆ Î˜ï¼‰ãŒæˆã‚Šç«‹ã¤å ´åˆã‹ã¤ãã®å ´åˆã«é™ã‚Šæˆç«‹ã™ã‚‹ã€‚

å†…ç©å†…ã®é …ã‚’æ•´ç†ã™ã‚‹ã¨ï¼š

âŸ¨âˆ’_ğœ‚ğº_(**ğœ½***), _ğœ™_ âˆ’ **ğœ½***âŸ© â‰¤ 0 âŸ¹ âŸ¨_ğº_(**ğœ½***), _ğœ™_ âˆ’ **ğœ½***âŸ© â‰¥ 0, âˆ€_ğœ™_ âˆˆ Î˜.

ã“ã®ä¸ç­‰å¼ã¯ã¾ã•ã«å¼18ã§å®šç¾©ã•ã‚ŒãŸä¸€æ¬¡åœç•™æ¡ä»¶ã‚’å…±åŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ Î˜ ã«ä¸€èˆ¬åŒ–ã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚ã—ãŸãŒã£ã¦ä¸å‹•ç‚¹ **ğœ½*** ã¯ Local Predictive Equilibrium ã‚’æ§‹æˆã—ã€å†…éƒ¨åœç•™ç‚¹ã¨å¢ƒç•Œè§£ã®ä¸¡æ–¹ã‚’å³å¯†ã«åå®¹ã™ã‚‹ã€‚

_**D.2.2.**_ _**Mixed Predictive Equilibrium**_

å±€æ‰€è¿‘ä¼¼ã«ä¾å­˜ã›ãšã«å‡è¡¡ã®å­˜åœ¨ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸Šã§ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã™ã‚‹ã“ã¨ã‚’è¨±å¯ã§ãã‚‹ã€‚ã“ã‚Œã¯ã‚²ãƒ¼ãƒ ç†è«–ã«ãŠã‘ã‚‹æ··åˆæˆ¦ç•¥ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã€‚

**å®šç¾© D.4** (Mixed Predictive Equilibrium). Î”Î˜_ğ‘–_ ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ Î˜_ğ‘–_ ä¸Šã®ç¢ºç‡åˆ†å¸ƒã®é›†åˆã¨ã™ã‚‹ã€‚Mixed Predictive Equilibrium ã¯åˆ†å¸ƒã®ã‚¿ãƒ—ãƒ« **ğ*** = (_ğœ‡_*1, ..., _ğœ‡_*n) ã§ã‚ã‚Šã€å…¨ _ğ‘–_ âˆˆ I ã«ã¤ã„ã¦ï¼š

_ğœ‡_*_ğ‘–_ âˆˆ arg min_ğœ‡ğ‘–_ âˆˆ Î”Î˜_ğ‘–_ KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡ğ‘–_(_â„ğ‘–_)), (20)

ã“ã“ã§ _ğ‘ğœ‡ğ‘–_(_â„ğ‘–_) = ğ”¼_ğœƒğ‘–_âˆ¼_ğœ‡ğ‘–_ [_ğ‘ğœƒğ‘–_(_â„ğ‘–_)]ã€â„™(_â„ğ‘–_; **ğ***) ã¯å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ãŒ Policy Improvement æ¼”ç®—å­ï¼ˆå¼6ï¼‰ã‚’ _ğ‘ğœ‡ğ‘–_ ã«é©ç”¨ã—ã¦å¾—ã‚‰ã‚ŒãŸæ–¹ç­– _ğœ‹ğœ‡ğ‘–_ ã«å¾“ã†å ´åˆã®å±¥æ­´åˆ†å¸ƒã€‚

**å®šç† D.5** (Mixed Predictive Equilibrium ã®å­˜åœ¨). _Î˜_ğ‘–_ ãŒã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè·é›¢ç©ºé–“Â¹ ã§ã‚ã‚Šã€å†™åƒ (**ğœ½**, _ğœƒğ‘–'_) â†¦ KL(â„™(Â·; **ğœ½**) || _ğ‘ğœƒğ‘–'_) ãŒå…¨ _ğ‘–_ âˆˆ I ã«å¯¾ã—ã¦é€£ç¶šã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚ã•ã‚‰ã« KL(â„™(_â„ğ‘–_; **ğ**) || _ğ‘ğœ‡ğ‘–_(_â„ğ‘–_)) < âˆï¼ˆå…¨ **ğ** âˆˆ Î” = Ã—_ğ‘–_ âˆˆI Î”Î˜_ğ‘–_ ã«å¯¾ã—ã¦ï¼‰ã‚’ä»®å®šã™ã‚‹ã€‚ã“ã®ã¨ã Mixed Predictive Equilibrium ãŒå­˜åœ¨ã™ã‚‹ã€‚_

_è¨¼æ˜._ æ··åˆæˆ¦ç•¥ã®ç©ºé–“ä¸Šã®é€£ç¶šå†™åƒã‚’æ§‹æˆã—ä¸å‹•ç‚¹å®šç†ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§å­˜åœ¨ã‚’è¨¼æ˜ã™ã‚‹ã€‚Î”Î˜_ğ‘–_ ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè·é›¢ç©ºé–“ Î˜_ğ‘–_ ä¸Šã® Borel ç¢ºç‡æ¸¬åº¦ã®ç©ºé–“ã¨ã™ã‚‹ã€‚Wasserstein è·é›¢ã‚’å‚™ãˆãŸ Î”Î˜_ğ‘–_ ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå‡¸è·é›¢ç©ºé–“ã§ã‚ã‚‹ã€‚Î” = Ã—_ğ‘–_ âˆˆI Î”Î˜_ğ‘–_ ã‚’å…±åŒæˆ¦ç•¥ç©ºé–“ã¨ã™ã‚‹ã€‚

Î”Î˜_ğ‘–_ ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè·é›¢ç©ºé–“ã§ã‚ã‚‹ãŸã‚å¯åˆ†ã§ã‚ã‚‹ã€‚å¯ç®—ç¨ å¯†éƒ¨åˆ†é›†åˆ _ğ·ğ‘–_ = {_ğœ‡Ìƒğ‘–,ğ‘˜_}^âˆ_ğ‘˜_=1 âŠ‚ Î”Î˜_ğ‘–_ ã‚’å›ºå®šã§ãã‚‹ã€‚

é€£ç¶š Advantage é–¢æ•° _ğ‘ğ‘–_: Î” Ã— Î”Î˜_ğ‘–_ â†’ â„â‰¥0 ã‚’å®šç¾©ã™ã‚‹ï¼š

_ğ‘ğ‘–_(**ğ**, _ğœ‡'ğ‘–_) = max{0, KL(â„™(_â„ğ‘–_; **ğ**) || _ğ‘ğœ‡ğ‘–_(_â„ğ‘–_)) âˆ’ KL(â„™(_â„ğ‘–_; **ğ**) || _ğ‘ğœ‡'ğ‘–_(_â„ğ‘–_))}.

KL(â„™(_â„ğ‘–_; **ğ**) || _ğ‘ğœ‡ğ‘–_(_â„ğ‘–_)) < âˆ ã§ã‚ã‚‹ãŸã‚ã€Advantage é–¢æ•°ã¯ well-defined ã§ã‚ã‚Šæœ‰é™å®Ÿæ•°ã«è©•ä¾¡ã•ã‚Œã‚‹ã€‚

é·ç§»å†™åƒ _ğ‘‡ğ‘–_: Î” â†’ Î”Î˜_ğ‘–_ ã‚’æ§‹æˆã™ã‚‹ã€‚Î˜_ğ‘–_ ä¸Šã®æœ‰é™æ¸¬åº¦ _ğ´ğ‘–_(**ğ**) ã‚’ç¨ å¯†éƒ¨åˆ†é›†åˆ _ğ·ğ‘–_ ä¸Šã« Advantage ã«æ¯”ä¾‹ã—ãŸé‡ã¿ã§å®šç¾©ã™ã‚‹ï¼š

_ğ´ğ‘–_(**ğ**) = Î£^âˆ_ğ‘˜_=1 2^(âˆ’_ğ‘˜_) _ğ‘ğ‘–_(**ğ**, _ğœ‡Ìƒğ‘–,ğ‘˜_) _ğœ‡Ìƒğ‘–,ğ‘˜_.

_ğ´ğ‘–_(**ğ**)(Î˜_ğ‘–_) = Î£^âˆ_ğ‘˜_=1 2^(âˆ’_ğ‘˜_) _ğ‘ğ‘–_(**ğ**, _ğœ‡Ìƒğ‘–,ğ‘˜_) ã‚’ãã®å…¨è³ªé‡ã¨ã™ã‚‹ã€‚ç¾åœ¨ã®æˆ¦ç•¥ _ğœ‡ğ‘–_ ã¨æ”¹å–„æ¸¬åº¦ _ğ´ğ‘–_(**ğ**) ã‚’æ··åˆã™ã‚‹ã“ã¨ã§ _ğ‘‡ğ‘–_(**ğ**) ã‚’å®šç¾©ã™ã‚‹ï¼š

_ğ‘‡ğ‘–_(**ğ**) = (_ğœ‡ğ‘–_ + _ğ´ğ‘–_(**ğ**)) / (1 + _ğ´ğ‘–_(**ğ**)(Î˜_ğ‘–_)).

å†™åƒ **ğœ½** â†¦ KL(â„™(Â·; **ğœ½**) || _ğ‘ğœ‡'ğ‘–_) ãŒé€£ç¶šã§ã‚ã‚Šç©ºé–“ãŒã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§ã‚ã‚‹ãŸã‚ã€_ğ‘ğ‘–_ ã¯ä¸€æ§˜æœ‰ç•Œã§ã‚ã‚Šå¼±-*ä½ç›¸ã«é–¢ã—ã¦ **ğ** ã«ã¤ã„ã¦é€£ç¶šã§ã‚ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€å…±åŒå†™åƒ _ğ‘‡_(**ğ**) = (_ğ‘‡_1(**ğ**), ..., _ğ‘‡_n(**ğ**)) ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå‡¸é›†åˆ Î” ã‹ã‚‰è‡ªèº«ã¸ã®é€£ç¶šé–¢æ•°ã§ã‚ã‚‹ã€‚Schauder ã®ä¸å‹•ç‚¹å®šç†ã«ã‚ˆã‚Šã€_ğ‘‡_(**ğ***) = **ğ*** ã‚’æº€ãŸã™ä¸å‹•ç‚¹ **ğ*** âˆˆ Î” ãŒå­˜åœ¨ã™ã‚‹ã€‚

Â¹ å®šç† D.5 ã§ã¯ Î˜_ğ‘–_ ã®å‡¸æ€§ã¯å¿…è¦ãªãã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆæ€§ã®ã¿ã‚’å¿…è¦ã¨ã™ã‚‹ã“ã¨ã«æ³¨æ„ã€‚

**ğ*** ãŒ Mixed Predictive Equilibrium ã§ã‚ã‚‹ã“ã¨ã‚’èƒŒç†æ³•ã§è¨¼æ˜ã™ã‚‹ã€‚_ğ¶ğ‘–_ = _ğ´ğ‘–_(**ğ***)(Î˜_ğ‘–_) ã¨ãŠãã€‚

ä¸å‹•ç‚¹æ¡ä»¶ _ğœ‡_*_ğ‘–_ = _ğ‘‡ğ‘–_(**ğ***) ã‚ˆã‚Šï¼š

_ğœ‡_*_ğ‘–_(1 + _ğ¶ğ‘–_) = _ğœ‡_*_ğ‘–_ + _ğ´ğ‘–_(**ğ***) âŸ¹ _ğ¶ğ‘–_ _ğœ‡_*_ğ‘–_ = _ğ´ğ‘–_(**ğ***).

**ğ*** ãŒ Mixed Predictive Equilibrium ã§ãªã„ã¨ä»®å®šã™ã‚‹ã€‚ã‚ã‚‹ _ğ‘–_ âˆˆ I ã«å¯¾ã—ã¦ã€KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡Ì‚ğ‘–_(_â„ğ‘–_)) < KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)) ã‚’æº€ãŸã™åˆ†å¸ƒ _ğœ‡Ì‚ğ‘–_ âˆˆ Î”Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚

_ğœ–_ := KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)) âˆ’ KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡Ì‚ğ‘–_(_â„ğ‘–_)) > 0 ã¨ãŠãã€‚

å®šç¾©ã«ã‚ˆã‚Š _ğœ‡Ì‚ğ‘–_ ã® Advantage ã¯å³å¯†ã«æ­£ï¼š_ğ‘ğ‘–_(**ğ***, _ğœ‡Ì‚ğ‘–_) = _ğœ–_ > 0ã€‚

å†™åƒ (**ğœ½**, _ğœƒğ‘–'_) â†¦ KL(â„™(Â·; **ğœ½**) || _ğ‘ğœƒğ‘–'_) ãŒé€£ç¶šã§ã‚ã‚‹ãŸã‚ã€æ±é–¢æ•° _ğœ‡'ğ‘–_ â†¦ KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡'ğ‘–_(_â„ğ‘–_)) ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè·é›¢ç©ºé–“ Î”Î˜_ğ‘–_ ä¸Šã§é€£ç¶šã§ã‚ã‚Šã€Advantage é–¢æ•° _ğ‘ğ‘–_(**ğ***, Â·) ã¯ä¸€æ§˜é€£ç¶šã§ã‚ã‚‹ã€‚ã—ãŸãŒã£ã¦ _ğœ‡Ì‚ğ‘–_ ã‚’å«ã‚€é–‹è¿‘å‚ _ğ‘ˆ_ âŠ‚ Î”Î˜_ğ‘–_ ãŒå­˜åœ¨ã—ã€å…¨ _ğœ‡'ğ‘–_ âˆˆ _ğ‘ˆ_ ã«ã¤ã„ã¦ _ğ‘ğ‘–_(**ğ***, _ğœ‡'ğ‘–_) > _ğœ–_/2 ã§ã‚ã‚‹ã€‚

é›†åˆ _ğ·ğ‘–_ = {_ğœ‡Ìƒğ‘–,ğ‘˜_}^âˆ_ğ‘˜_=1 ãŒ Î”Î˜_ğ‘–_ ã§ç¨ å¯†ã§ã‚ã‚‹ãŸã‚ã€_ğœ‡Ìƒğ‘–,ğ¾_ âˆˆ _ğ‘ˆ_ ã‚’æº€ãŸã™æ•´æ•° _ğ¾_ ãŒå­˜åœ¨ã™ã‚‹ã€‚ã—ãŸãŒã£ã¦ _ğ‘ğ‘–_(**ğ***, _ğœ‡Ìƒğ‘–,ğ¾_) > _ğœ–_/2 > 0ã€‚ã“ã®å³å¯†ã«æ­£ã® Advantage ã¯æ”¹å–„æ¸¬åº¦ã®å…¨è³ªé‡ãŒå³å¯†ã«æ­£ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ï¼š

_ğ¶ğ‘–_ = _ğ´ğ‘–_(**ğ***)(Î˜_ğ‘–_) â‰¥ 2^(âˆ’_ğ¾_) _ğ‘ğ‘–_(**ğ***, _ğœ‡Ìƒğ‘–,ğ¾_) > 0.

ä¸å‹•ç‚¹æ¡ä»¶ _ğ¶ğ‘–_ _ğœ‡_*_ğ‘–_ = _ğ´ğ‘–_(**ğ***) ã¨ _ğ¶ğ‘–_ > 0 ã‚ˆã‚Šã€_ğœ‡_*_ğ‘–_ ã‚’ _ğ·ğ‘–_ ã®åŸºåº•æ¸¬åº¦ã®ç„¡é™å‡¸çµåˆã¨ã—ã¦è¡¨ç¾ã§ãã‚‹ï¼š

_ğœ‡_*_ğ‘–_ = (1/_ğ¶ğ‘–_) _ğ´ğ‘–_(**ğ***) = Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ _ğœ‡Ìƒğ‘–,ğ‘˜_,

é‡ã¿ _ğ‘¤ğ‘˜_ = 2^(âˆ’_ğ‘˜_) _ğ‘ğ‘–_(**ğ***, _ğœ‡Ìƒğ‘–,ğ‘˜_) / _ğ¶ğ‘–_ â‰¥ 0 ã¯ã¡ã‚‡ã†ã©1ã«åˆè¨ˆã•ã‚Œã‚‹ã€‚

æ··åˆæˆ¦ç•¥ _ğœ‡_*_ğ‘–_ ã®ä¸‹ã§ã®æœŸå¾…äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒãˆã‚‹ã€‚æœŸå¾…å€¤ã®ç·šå½¢æ€§ã«ã‚ˆã‚Šï¼š

_ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_) = ğ”¼_ğœƒğ‘–_âˆ¼_ğœ‡_*_ğ‘–_ [_ğ‘ğœƒğ‘–_(_â„ğ‘–_)] = Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ ğ”¼_ğœƒğ‘–_âˆ¼_ğœ‡Ìƒğ‘–,ğ‘˜_ [_ğ‘ğœƒğ‘–_(_â„ğ‘–_)] = Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ _ğ‘ğœ‡Ìƒğ‘–,ğ‘˜_(_â„ğ‘–_).

KL Divergence ã¯ç¬¬äºŒå¼•æ•°ã«é–¢ã—ã¦å³å¯†ã«å‡¸ã§ã‚ã‚‹ãŸã‚ã€ç„¡é™å‡¸çµåˆã« Jensen ã®ä¸ç­‰å¼ã‚’é©ç”¨ã§ãã‚‹ï¼š

KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)) = KL(â„™(_â„ğ‘–_; **ğ***) || Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ _ğ‘ğœ‡Ìƒğ‘–,ğ‘˜_(_â„ğ‘–_)) â‰¤ Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡Ìƒğ‘–,ğ‘˜_(_â„ğ‘–_)).

é‡è¦ãªã®ã¯ã€Advantage é–¢æ•°ã®å®šç¾©ã¨é‡ã¿ _ğ‘¤ğ‘˜_ ã®æ§‹æˆã«ã‚ˆã‚Šã€é‡ã¿ _ğ‘¤ğ‘˜_ ãŒå³å¯†ã«æ­£ã§ã‚ã‚‹ã®ã¯å¯¾å¿œã™ã‚‹ Advantage _ğ‘ğ‘–_(**ğ***, _ğœ‡Ìƒğ‘–,ğ‘˜_) > 0 ã®å ´åˆã‹ã¤ãã®å ´åˆã«é™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚å³å¯†ã«æ­£ã® Advantage ã¯è©•ä¾¡ã•ã‚ŒãŸæ¸¬åº¦ãŒç¾åœ¨ã®çŠ¶æ…‹ _ğœ‡_*_ğ‘–_ ã‚ˆã‚Šã‚‚å³å¯†ã«ä½ã„æå¤±ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’æ­£ç¢ºã«æ„å‘³ã™ã‚‹ï¼š

KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡Ìƒğ‘–,ğ‘˜_(_â„ğ‘–_)) < KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)).

_ğœ–_/2 ã§ä¸‹ã‹ã‚‰æœ‰ç•Œãª Advantage ã‚’æŒã¤å°‘ãªãã¨ã‚‚1ã¤ã®é‡ã¿ _ğ‘¤ğ¾_ > 0 ãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€ã“ã®å³å¯†ãªä¸Šç•Œã‚’ _ğ‘˜_ ä¸Šã®å’Œã«ä»£å…¥ã™ã‚‹ã¨ï¼š

Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡Ìƒğ‘–,ğ‘˜_(_â„ğ‘–_)) < Î£^âˆ_ğ‘˜_=1 _ğ‘¤ğ‘˜_ KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)) = KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)).

ä¸ç­‰å¼ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€ä»¥ä¸‹ã®çµ¶å¯¾çŸ›ç›¾ã«åˆ°é”ã™ã‚‹ï¼š

KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)) < KL(â„™(_â„ğ‘–_; **ğ***) || _ğ‘ğœ‡_*_ğ‘–_(_â„ğ‘–_)).

ã—ãŸãŒã£ã¦ã€æœ€åˆã®ä»®å®šã¯å½ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚ãã®ã‚ˆã†ãªå„ªè¶Šçš„åˆ†å¸ƒ _ğœ‡Ì‚ğ‘–_ ã¯å­˜åœ¨ã—ãˆãšã€ä¸å‹•ç‚¹ **ğ*** ã¯ç¢ºã‹ã« Mixed Predictive Equilibrium ã§ã‚ã‚‹ã€‚

ä¸Šè¨˜ã®å®šç†ã®èˆˆå‘³æ·±ã„ç³»ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒé–¢æ•°ç©ºé–“ã§å‡¸ã§ã‚ã‚‹å ´åˆã€ç´”ç²‹ãª Global Predictive Equilibrium ãŒå­˜åœ¨ã™ã‚‹ã€‚

**ç³» D.6** (ãƒ¢ãƒ‡ãƒ«ã®é–¢æ•°å‡¸æ€§ã®ä¸‹ã§ã® Pure Predictive Equilibrium ã®å­˜åœ¨). _å®šç† D.5 ã¨åŒã˜ä»®å®šã‚’è€ƒãˆã‚‹ã€‚ã•ã‚‰ã«å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ âˆˆ I ã«ã¤ã„ã¦ã€è¡¨ç¾å¯èƒ½ãªäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“ {_ğ‘ğœƒğ‘–_ | _ğœƒğ‘–_ âˆˆ Î˜_ğ‘–_} ãŒå‡¸ã§ã‚ã‚‹ã¨ä»®å®šã™ã‚‹ã€‚ã™ãªã‚ã¡ã€å…¨ _ğœƒğ‘–'_, _ğœƒğ‘–''_ âˆˆ Î˜_ğ‘–_ ã¨å…¨ _ğ›¼ğ‘–_ âˆˆ [0, 1] ã«å¯¾ã—ã¦ã€_ğ‘ğœƒğ‘–_ = _ğ›¼ğ‘–_ _ğ‘ğœƒğ‘–'_ + (1 âˆ’ _ğ›¼ğ‘–_) _ğ‘ğœƒğ‘–''_ ã‚’æº€ãŸã™ç´”ç²‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœƒğ‘–_ âˆˆ Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚Â² ã“ã‚Œã‚‰ã®æ¡ä»¶ã®ä¸‹ã§ã€Global Predictive Equilibriumï¼ˆç´”ç²‹æˆ¦ç•¥ã«ãŠã‘ã‚‹ï¼‰ã¯å¸¸ã«å­˜åœ¨ã™ã‚‹ã€‚_

_è¨¼æ˜._ å®šç† D.5 ã‚ˆã‚Šã€Mixed Predictive Equilibrium **ğ*** = (_ğœ‡_*1, ..., _ğœ‡_*n) âˆˆ Ã—_ğ‘–_ âˆˆI Î”Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚Pure Global Predictive Equilibrium ã®å­˜åœ¨ã‚’ç¢ºç«‹ã™ã‚‹ãŸã‚ã€ä»»æ„ã®ç¢ºç‡åˆ†å¸ƒ _ğœ‡ğ‘–_ âˆˆ Î”Î˜_ğ‘–_ ã«å¯¾ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®é–¢æ•°å‡¸æ€§ä»®å®šãŒ _ğ‘ğœƒğ‘–_* = _ğ‘ğœ‡ğ‘–_ = ğ”¼_ğœƒğ‘–_âˆ¼_ğœ‡ğ‘–_ [_ğ‘ğœƒğ‘–_] ã‚’æº€ãŸã™ç´”ç²‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœƒğ‘–_* âˆˆ Î˜_ğ‘–_ ã®å­˜åœ¨ã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚

ã¾ãšæœ‰é™å°æ¸¬åº¦ã«å¯¾ã—ã¦ã“ã®ä¸»å¼µã‚’è¨¼æ˜ã™ã‚‹ã€‚_ğœ‡ğ‘–_ = Î£^_ğ‘š_  _ğ‘˜_=1 _ğ‘¤ğ‘˜_ _ğ›¿ğœƒğ‘–,ğ‘˜_ ã‚’ Î˜_ğ‘–_ ä¸Šã®æœ‰é™å°ç¢ºç‡æ¸¬åº¦ã¨ã—ã€_ğ‘¤ğ‘˜_ â‰¥ 0 ã‹ã¤ Î£^_ğ‘š_ _ğ‘˜_=1 _ğ‘¤ğ‘˜_ = 1ã€‚å°ã®ã‚µã‚¤ã‚º _ğ‘š_ ã«é–¢ã™ã‚‹å¸°ç´æ³•ã§é€²ã‚ã‚‹ã€‚åŸºåº• _ğ‘š_ = 1 ã¯è‡ªæ˜ã§ã‚ã‚Šã€_ğ‘ğœ‡ğ‘–_ = _ğ‘ğœƒğ‘–,1_ã€‚ä¸»å¼µãŒ _ğ‘š_ âˆ’ 1 ã«å¯¾ã—ã¦æˆã‚Šç«‹ã¤ã¨ä»®å®šã™ã‚‹ã¨ã€_ğœ‡ğ‘–_ï¼ˆ_ğ‘¤ğ‘š_ < 1 ã®å ´åˆï¼‰ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¾ã§ãã‚‹ï¼š

_ğ‘ğœ‡ğ‘–_ = _ğ‘¤ğ‘š_ _ğ‘ğœƒğ‘–,ğ‘š_ + (1 âˆ’ _ğ‘¤ğ‘š_) Î£^(_ğ‘š_âˆ’1) _ğ‘˜_=1 (_ğ‘¤ğ‘˜_/(1 âˆ’ _ğ‘¤ğ‘š_)) _ğ‘ğœƒğ‘–,ğ‘˜_.

å¸°ç´çš„ä»®èª¬ã«ã‚ˆã‚Šã€_ğ‘ğœƒÌƒğ‘–_ = Î£^(_ğ‘š_âˆ’1) _ğ‘˜_=1 (_ğ‘¤ğ‘˜_/(1 âˆ’ _ğ‘¤ğ‘š_)) _ğ‘ğœƒğ‘–,ğ‘˜_ ã‚’æº€ãŸã™ç´”ç²‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœƒÌƒğ‘–_ âˆˆ Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚å‡¸æ€§ä»®å®šã‚’ _ğ›¼ğ‘–_ = _ğ‘¤ğ‘š_ã€_ğœƒğ‘–'_ = _ğœƒğ‘–,ğ‘š_ã€_ğœƒğ‘–''_ = _ğœƒÌƒğ‘–_ ã§é©ç”¨ã™ã‚‹ã¨ã€_ğ‘ğœƒğ‘–_ = _ğ‘¤ğ‘š_ _ğ‘ğœƒğ‘–,ğ‘š_ + (1 âˆ’ _ğ‘¤ğ‘š_) _ğ‘ğœƒÌƒğ‘–_ = _ğ‘ğœ‡ğ‘–_ ã‚’æº€ãŸã™ _ğœƒğ‘–_ âˆˆ Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚ã‚ˆã£ã¦ä¸»å¼µã¯å…¨ã¦ã®æœ‰é™å°æ¸¬åº¦ã«å¯¾ã—ã¦æˆã‚Šç«‹ã¤ã€‚

æ¬¡ã«ä»»æ„ã®æ¸¬åº¦ _ğœ‡ğ‘–_ âˆˆ Î”Î˜_ğ‘–_ ã‚’è€ƒãˆã‚‹ã€‚æœ‰é™å°æ¸¬åº¦ã®é›†åˆãŒå¼±-*ä½ç›¸ã®ä¸‹ã§ Î”Î˜_ğ‘–_ ã§ç¨ å¯†ã§ã‚ã‚‹ãŸã‚ã€_ğœ‡ğ‘–_ ã«å¼±åæŸã™ã‚‹æœ‰é™å°æ¸¬åº¦ã®åˆ— (_ğœ‡ğ‘–_^(_ğ‘š_))^âˆ_ğ‘š_=1 ãŒå­˜åœ¨ã™ã‚‹ã€‚

ä»»æ„ã® _â„ğ‘–_ ã«å¯¾ã—ã¦å†™åƒ _ğœƒğ‘–_ â†¦ _ğ‘ğœƒğ‘–_(_â„ğ‘–_) ãŒé€£ç¶šæœ‰ç•Œã§ã‚ã‚‹ãŸã‚ã€æ±é–¢æ•° _ğœˆ_ â†¦ _ğ‘ğœˆ_(_â„ğ‘–_) = âˆ« _ğ‘ğœƒğ‘–_(_â„ğ‘–_) _ğ‘‘ğœˆ_(_ğœƒğ‘–_) ã¯å¼±-*ä½ç›¸ã«é–¢ã—ã¦é€£ç¶šã§ã‚ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€æœŸå¾…ãƒ¢ãƒ‡ãƒ«ã®åˆ—ã¯å„ç‚¹åæŸã™ã‚‹ï¼š_ğ‘ğœ‡ğ‘–_^(_ğ‘š_) â†’ _ğ‘ğœ‡ğ‘–_ï¼ˆ_ğ‘š_ â†’ âˆï¼‰ã€‚

å¸°ç´çš„ã‚¹ãƒ†ãƒƒãƒ—ã‚ˆã‚Šã€å„æœ‰é™å°æ¸¬åº¦ _ğœ‡ğ‘–_^(_ğ‘š_) ã«å¯¾ã—ã¦ _ğ‘ğœƒğ‘–_^(_ğ‘š_) = _ğ‘ğœ‡ğ‘–_^(_ğ‘š_) ã‚’æº€ãŸã™å¯¾å¿œã™ã‚‹ç´”ç²‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ _ğœƒğ‘–_^(_ğ‘š_) âˆˆ Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Š Î˜_ğ‘–_ ã®ç´”ç²‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ— (_ğœƒğ‘–_^(_ğ‘š_))^âˆ_ğ‘š_=1 ãŒæ§‹æˆã•ã‚Œã‚‹ã€‚Î˜_ğ‘–_ ãŒã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè·é›¢ç©ºé–“ã§ã‚ã‚‹ãŸã‚ã€ã“ã®åˆ—ã¯ã‚ã‚‹æ¥µé™ç‚¹ _ğœƒğ‘–_* âˆˆ Î˜_ğ‘–_ ã«åæŸã™ã‚‹éƒ¨åˆ†åˆ— (_ğœƒğ‘–_^(_ğ‘šğ‘˜_))^âˆ_ğ‘˜_=1 ã‚’è¨±å®¹ã™ã‚‹ã€‚

å†™åƒ _ğœƒğ‘–_ â†¦ _ğ‘ğœƒğ‘–_ ã®é€£ç¶šæ€§ã«ã‚ˆã‚Šï¼š

_ğ‘ğœƒğ‘–_* = lim_ğ‘˜_â†’âˆ _ğ‘ğœƒğ‘–_^(_ğ‘šğ‘˜_) = lim_ğ‘˜_â†’âˆ _ğ‘ğœ‡ğ‘–_^(_ğ‘šğ‘˜_) = _ğ‘ğœ‡ğ‘–_.

ã—ãŸãŒã£ã¦ã€Mixed Predictive Equilibrium **ğ*** ã«å¯¾ã—ã¦ã€å…¨ _ğ‘–_ âˆˆ I ã«ã¤ã„ã¦ _ğ‘ğœƒğ‘–_* = _ğ‘ğœ‡_*_ğ‘–_ ã‚’æº€ãŸã™ç´”ç²‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å…±åŒè¨­å®š **ğœ½*** = (_ğœƒ_1*, ..., _ğœƒ_n*) âˆˆ Ã—_ğ‘–_ âˆˆI Î˜_ğ‘–_ ãŒå­˜åœ¨ã™ã‚‹ã€‚

ã‚ˆã£ã¦ _ğœƒğ‘–_* âˆˆ arg min_ğœƒğ‘–_ âˆˆ Î˜_ğ‘–_ KL(â„™(_â„ğ‘–_; **ğœ½***) || _ğ‘ğœƒğ‘–_(_â„ğ‘–_))ï¼ˆå…¨ _ğ‘–_ âˆˆ Iï¼‰ãŒæˆã‚Šç«‹ã¤ã€‚

ã“ã‚Œã¯ã¾ã•ã« Global Predictive Equilibrium ã®å®šç¾©ã‚’æº€ãŸã—ã€ã“ã‚Œã‚‰ã®æ¡ä»¶ä¸‹ã§ã®ç´”ç²‹æˆ¦ç•¥ã«ãŠã‘ã‚‹å­˜åœ¨ã‚’è¨¼æ˜ã™ã‚‹ã€‚

é–¢æ•°å‡¸æ€§ã®ä»®å®šã¯æœ‰é™å®¹é‡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¨ã£ã¦ã¯ç†æƒ³åŒ–ã§ã‚ã‚‹ãŒã€Deep Neural Network ã¯ä¸‡èƒ½é–¢æ•°è¿‘ä¼¼å™¨ã§ã‚ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«å®¹é‡ãŒå¢—åŠ ã™ã‚‹ã«ã¤ã‚Œã¦è¡¨ç¾å¯èƒ½ãªåˆ†å¸ƒã®ç©ºé–“ãŒæœ‰åŠ¹ãªç¢ºç‡æ¸¬åº¦ã®å®Œå…¨å‡¸é›†åˆã«è¿‘ä¼¼ã—ã€ç´”ç²‹å‡è¡¡ã®å­˜åœ¨ãŒã¾ã™ã¾ã™æ­£ç¢ºãªè¿‘ä¼¼ã¨ãªã‚‹ã“ã¨ã‚’æ³¨è¨˜ã™ã‚‹ã€‚

Â² ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹å‡¸æ€§ã€ã™ãªã‚ã¡ _ğ‘_(_ğ›¼ğ‘–_ _ğœƒğ‘–'_ + (1 âˆ’ _ğ›¼ğ‘–_) _ğœƒğ‘–''_) = _ğ›¼ğ‘–_ _ğ‘ğœƒğ‘–'_ + (1 âˆ’ _ğ›¼ğ‘–_) _ğ‘ğœƒğ‘–''_ ã‚’è¦æ±‚ã—ãªã„ã“ã¨ã‚’å¼·èª¿ã™ã‚‹ã€‚

**D.3.** **Nash å‡è¡¡ãŠã‚ˆã³ Subjective Embedded Equilibria ã¨ã®é–¢ä¿‚**

æœ€å¾Œã«ã€PPI ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸å‹•ç‚¹ã‚’ã‚²ãƒ¼ãƒ ç†è«–ã®æ¨™æº–çš„ãªè§£æ¦‚å¿µã«æ¥ç¶šã™ã‚‹ã€‚æ¨™æº–çš„ãªã‚²ãƒ¼ãƒ ç†è«–ã§ã¯ã€Nash Equilibrium ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå›ºå®šã•ã‚ŒãŸç’°å¢ƒã§æœ€é©ã«è¡Œå‹•ã™ã‚‹ã“ã¨ã‚’ä»®å®šã—ã€co-player ã®æ–¹ç­–ã¯ç„¦ç‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç¾åœ¨ã®è¡Œå‹•é¸æŠã‹ã‚‰ç‹¬ç«‹ã—ã¦ã„ã‚‹ã€‚å¯¾ç…§çš„ã«ã€PPI ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å†…éƒ¨ World Model _ğ‘ğœƒğ‘–_ ã«é–¢ã—ã¦æœ€é©ã«è¡Œå‹•ã—ã€ã“ã® World Model ã¯å°†æ¥ã®è»Œé“ã®å…±åŒåˆ†å¸ƒã‚’æ¨å®šã™ã‚‹ã“ã¨ã§ç„¦ç‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã¨ co-player ã®å¿œç­”é–“ã®æ½œåœ¨çš„ãªåå¿œçš„ä¾å­˜æ€§ã‚’æ•æ‰ã™ã‚‹ã€‚

ã“ã‚Œã¯ã€ŒEmbedded Equilibriaã€ã®æ¦‚å¿µã¨å¯†æ¥ã«é–¢é€£ã—ã¦ãŠã‚Šã€ã“ã®ã‚ˆã†ãªè‡ªå·±äºˆæ¸¬çš„ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‹ã‚‰å‡ºç¾ã™ã‚‹å‡è¡¡è¡Œå‹•ã‚’ç‰¹å¾´ä»˜ã‘ã‚‹ï¼š

**å®šç¾© D.7** (Subjective Embedded Equilibrium). (Meulemans et al., 2025b) å…±åŒæ–¹ç­–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ« **ğ…*** ã¨å†…éƒ¨ Sequence Model ã®é›†åˆ {_ğ‘_*1, ..., _ğ‘_*n} ãŒ Subjective Embedded Equilibrium ã‚’æ§‹æˆã™ã‚‹ã®ã¯ä»¥ä¸‹ã®å ´åˆã§ã‚ã‚‹ï¼š

1. **ä¸»è¦³çš„æœ€é©æ€§ï¼š** å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–¹ç­– _ğœ‹_*_ğ‘–_ ãŒãã®å†…éƒ¨ World Model _ğ‘_*_ğ‘–_ ã«å¯¾ã™ã‚‹å³å¯†ãª best-response ã§ã‚ã‚‹ã€‚
2. **On-Path æ•´åˆæ€§ï¼š** å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® World Model ãŒçœŸã®ç’°å¢ƒãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¨å‡è¡¡ãƒ‘ã‚¹ä¸Šã§ã®ã¿ï¼ˆå…±åŒæ–¹ç­– **ğ…*** ã«ã‚ˆã‚ŠçœŸã«ç”Ÿæˆã•ã‚Œã‚‹å±¥æ­´ã®åˆ†å¸ƒ â„™* ä¸Šã§ï¼‰å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã€‚

é‡è¦ãªã®ã¯ã€Subjective Embedded Equilibrium ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã® off-path counterfactualsï¼ˆ**ğ…*** ã®ä¸‹ã§ã‚¼ãƒ­ç¢ºç‡ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹è¡Œå‹•ï¼‰ã«é–¢ã™ã‚‹ç²¾åº¦ã«åˆ¶ç´„ã‚’ç½®ã‹ãªã„ã“ã¨ã§ã‚ã‚‹ã€‚ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€_ğœ‹_*_ğ‘–_ ã¯ _ğ‘_*_ğ‘–_ ã«é–¢ã™ã‚‹ best response ã§ãªã‘ã‚Œã°ãªã‚‰ãšã€ã“ã‚Œã¯åå®Ÿä»®æƒ³çš„ãª off-policy ãƒ‘ã‚¹ã‚’è€ƒæ…®ã«å…¥ã‚Œã‚‹ã€‚æ›è¨€ã™ã‚Œã°ã€äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« _ğ‘_*_ğ‘–_ ã«ã‚ˆã‚‹ã¨ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ _ğ‘–_ ã¯ _ğœ‹_*_ğ‘–_ ã‹ã‚‰ã®é€¸è„±ã«ã‚ˆã£ã¦ã‚ˆã‚Šé«˜ã„æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’å¾—ã‚‹ã“ã¨ã¯ãªã„ã€‚

Subjective Embedded Equilibria ã¨ãã®æ€§è³ªã®ã•ã‚‰ãªã‚‹è©³ç´°ã«ã¤ã„ã¦ã¯ Meulemans et al. (2025b) ã‚’å‚ç…§ã•ã‚ŒãŸã„ã€‚

PPI ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒï¼ˆäºˆæ¸¬çš„ï¼‰World Model ãŒå®Œå…¨ã§ã‚ã‚‹ä¸å‹•ç‚¹ã«åæŸã—ãŸå ´åˆã€Predictive Equilibrium ã¯ Subjective Embedded Equilibrium ã«å¯¾å¿œã™ã‚‹ã“ã¨ãŒåˆ¤æ˜ã™ã‚‹ã€‚ã¾ãšå®Œå…¨ãª World Model ã‚’æŒã¤ Predictive Equilibrium ã‚’å½¢å¼åŒ–ã™ã‚‹ï¼š

**å®šç¾© D.8** (Perfect Predictive Equilibrium). Perfect Predictive Equilibrium ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒèª˜å°ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’å®Œå…¨ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹è¨­å®š **ğœ½*** ã§ã‚ã‚‹ï¼š

KL(â„™(_â„ğ‘–_; **ğœ½***) || _ğ‘ğœƒğ‘–_*(_â„ğ‘–_)) = 0, âˆ€_ğ‘–_ âˆˆ I. (21)

**å®šç† D.9** (Perfect Predictive Equilibrium âŸ¹ Subjective Embedded Equilibrium). _å¼6ã§å®šç¾©ã•ã‚ŒãŸ Policy Improvement æ¼”ç®—å­ _ğœ‹ğœƒğ‘–_(_ğ‘ğ‘–_ | _â„ğ‘–_) âˆ _ğ‘ğœƒğ‘–_(_ğ‘ğ‘–_ | _â„ğ‘–_) exp(_ğ›½ğ‘„_^_ğ‘ğœƒğ‘–_(_â„ğ‘–_, _ğ‘ğ‘–_)) ã‚’ä½¿ç”¨ã™ã‚‹äºˆæ¸¬ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è€ƒãˆã‚‹ã€‚**ğœ½*** ãŒ Perfect Predictive Equilibrium ã§ã‚ã‚‹ãªã‚‰ã°ã€çµæœã¨ã—ã¦å¾—ã‚‰ã‚Œã‚‹è¨­å®šã¯ Subjective Embedded Equilibrium ã¨æ•´åˆçš„ã§ã‚ã‚‹ã€‚_

_è¨¼æ˜._ Perfect Predictive Equilibrium ã«ãŠã„ã¦ã€æ¡ä»¶ KL(â„™(Â·; **ğœ½***) || _ğ‘ğœƒğ‘–_*(Â·)) = 0 ã¯ Sequence Model ãŒã»ã¼è‡³ã‚‹æ‰€ã§çœŸã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’å«æ„ã™ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€å‡è¡¡ãƒ‘ã‚¹ä¸Šã§ Sequence Model ã«ã‚ˆã‚Šç”Ÿæˆã•ã‚Œã‚‹äº‹å‰è¡Œå‹•ç¢ºç‡ã¯çœŸã®è¡Œå‹•æ–¹ç­–ã«æ­£ç¢ºã«ä¸€è‡´ã™ã‚‹ï¼š_ğ‘ğœƒğ‘–_*(_ğ‘ğ‘–_ | _â„ğ‘–_) = _ğœ‹ğœƒğ‘–_*(_ğ‘ğ‘–_ | _â„ğ‘–_)ã€‚ã“ã‚Œã«ã‚ˆã‚Š On-Path Consistency æ¡ä»¶ãŒç›´ã¡ã«æº€ãŸã•ã‚Œã‚‹ã€‚

_ğ‘ğœƒğ‘–_* = _ğœ‹ğœƒğ‘–_* ã‚’ Policy Improvement æ¼”ç®—å­ã«ä»£å…¥ã™ã‚‹ã¨ï¼š

_ğœ‹ğœƒğ‘–_*(_ğ‘ğ‘–_ | _â„ğ‘–_) = (1/_ğ‘_(_â„ğ‘–_)) _ğœ‹ğœƒğ‘–_*(_ğ‘ğ‘–_ | _â„ğ‘–_) exp(_ğ›½ğ‘„_^_ğ‘ğœƒ_*_ğ‘–_(_â„ğ‘–_, _ğ‘ğ‘–_)).

æ–¹ç­–ã®ã‚µãƒãƒ¼ãƒˆå†…ã®ä»»æ„ã®è¡Œå‹• _ğ‘ğ‘–_ï¼ˆ_ğœ‹ğœƒğ‘–_*(_ğ‘ğ‘–_ | _â„ğ‘–_) > 0ï¼‰ã«ã¤ã„ã¦ã€ä¸¡è¾ºã‚’ _ğœ‹ğœƒğ‘–_*(_ğ‘ğ‘–_ | _â„ğ‘–_) ã§é™¤ã™ã‚‹ã¨ï¼š

1 = (1/_ğ‘_(_â„ğ‘–_)) exp(_ğ›½ğ‘„_^_ğ‘ğœƒ_*_ğ‘–_(_â„ğ‘–_, _ğ‘ğ‘–_)) âŸ¹ _ğ‘„_^_ğ‘ğœƒ_*_ğ‘–_(_â„ğ‘–_, _ğ‘ğ‘–_) = ln _ğ‘_(_â„ğ‘–_) / _ğ›½_.

_ğ‘_(_â„ğ‘–_) ã¯ _ğ‘ğ‘–_ ã«ä¾å­˜ã—ãªã„æ­£è¦åŒ–å®šæ•°ã§ã‚ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®ä¸‹ã§è©•ä¾¡ã•ã‚ŒãŸæœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã¯æ­£ã®ç¢ºç‡ã§å®Ÿè¡Œã•ã‚Œã‚‹å…¨ã¦ã®è¡Œå‹•ã«å¯¾ã—ã¦åŒä¸€ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚

æ¬¡ã«ã€æ–¹ç­–ã®ã‚µãƒãƒ¼ãƒˆã«ãªã„ off-path è¡Œå‹• _ğ‘'ğ‘–_ï¼ˆ_ğœ‹ğœƒğ‘–_*(_ğ‘'ğ‘–_ | _â„ğ‘–_) = 0ï¼‰ã‚’è€ƒãˆã‚‹ã€‚ã“ã®è¡Œå‹•ã¯å…±åŒæ–¹ç­–ã®ä¸‹ã§æ±ºã—ã¦å–ã‚‰ã‚Œãªã„ãŸã‚ã€å‘¨è¾ºç¢ºç‡ â„™(_â„ğ‘–_, _ğ‘'ğ‘–_; **ğœ½***) = 0ã€‚ã—ãŸãŒã£ã¦ã€KL Divergence ã¯ _ğ‘'ğ‘–_ ã«ç¶šããƒ¢ãƒ‡ãƒ«ã®æ¡ä»¶ä»˜ãäºˆæ¸¬ã«ä¸€åˆ‡ã®åˆ¶ç´„ã‚’èª²ã•ãªã„ã€‚

Subjective Optimality ã‚’å½¢å¼çš„ã«æ¤œè¨¼ã™ã‚‹ãŸã‚ã€_ğœ‹ğœƒğ‘–_*(_ğ‘'ğ‘–_ | _â„ğ‘–_) = 0 ã‚’æ­£å½“åŒ–ã™ã‚‹ Sequence Model ã® off-path æ¡ä»¶ä»˜ãç¢ºç‡ã®æœ‰åŠ¹ãªè£œå®ŒãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚_ğ‘’_min = (_ğ‘œ_, _ğ‘Ÿ_min) ã‚’æœ€å°å¯èƒ½å ±é…¬ _ğ‘Ÿ_min ã‚’å«ã‚€ç’°å¢ƒçŸ¥è¦šã¨ã™ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ã® off-path åå®Ÿä»®æƒ³è£œå®Œã‚’ _ğ‘ğœƒğ‘–_*(_ğ‘’_min | _â„ğ‘–_, _ğ‘'ğ‘–_) = 1 ã¨å®šç¾©ã—ã€ãã®å¾Œã¯å¸åçš„æœ€å°å ±é…¬ã‚’ä»®å®šã™ã‚‹ã€‚

ã“ã®è£œå®Œã•ã‚ŒãŸä¸»è¦³ãƒ¢ãƒ‡ãƒ«ã®ä¸‹ã§æœŸå¾…ãƒªã‚¿ãƒ¼ãƒ³ã‚’è©•ä¾¡ã™ã‚‹ã¨ _ğ‘„_^_ğ‘ğœƒ_*_ğ‘–_(_â„ğ‘–_, _ğ‘'ğ‘–_) = _ğ‘‰_min ãŒå¾—ã‚‰ã‚Œã€ã“ã‚Œã¯ on-path ãƒªã‚¿ãƒ¼ãƒ³ ln _ğ‘_(_â„ğ‘–_) / _ğ›½_ ä»¥ä¸‹ã§ã‚ã‚‹ã€‚æ–¹ç­–æ¼”ç®—å­ã¯äº‹å‰åˆ†å¸ƒ _ğ‘ğœƒğ‘–_*(_ğ‘'ğ‘–_ | _â„ğ‘–_) ã«åˆ¶ç´„ã•ã‚Œã¦ãŠã‚Šã€ä¸å‹•ç‚¹ã‚’æº€ãŸã™ãŸã‚ã«ã“ã‚Œã¯ã‚¼ãƒ­ã«è©•ä¾¡ã•ã‚Œãªã‘ã‚Œã°ãªã‚‰ãªã„ãŸã‚ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æº–æœ€é©ãªé€¸è„± _ğ‘'ğ‘–_ ã«æ­£ç¢ºã«ã‚¼ãƒ­ã®ç¢ºç‡ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãã®ä¸»è¦³çš„ World Model ã«å¯¾ã™ã‚‹æ­£ç¢ºãª best-response ã‚’å®Ÿè¡Œã—ã¦ãŠã‚Šã€Subjective Embedded Equilibrium ã®å®šç¾©ã‚’å®Œå…¨ã«æº€ãŸã™ã€‚

### **E. ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢**

å®Ÿé¨“ã¯ Python ã¨ Google JAX (Bradbury et al., 2018) ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ãŠã‚ˆã³ NumPy (Harris et al., 2020)ã€pandas (Wes McKinney, 2010)ã€Matplotlib (Hunter, 2007)ã€seaborn (Waskom, 2021)ã€Flax (Heek et al., 2024)ã€Optax (DeepMind et al., 2020) ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç”¨ã„ã¦å®Ÿè£…ã•ã‚ŒãŸã€‚

**E.1.** **LLM ã®ä½¿ç”¨**

æœ¬åŸç¨¿ã®æº–å‚™ã«ãŠã„ã¦ã€è¨€èªç·¨é›†ã¨å¯èª­æ€§ã®å‘ä¸Šã« Gemini 3 Pro ã‚’ä½¿ç”¨ã—ãŸã€‚ã¾ãŸè£œé¡Œ C.2 ã®è¨¼æ˜ã®è¿½åŠ è©³ç´°ã®æä¾›ã«ã‚‚ Gemini 3 Pro ã‚’ä½¿ç”¨ã—ã€ãã®å¾Œè‘—è€…ã‚‰ã«ã‚ˆã‚Šæ¤œè¨¼ã•ã‚ŒãŸã€‚
